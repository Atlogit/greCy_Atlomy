{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: grc\n",
      "Training pipeline: transformer, trainable_lemmatizer\n",
      "3521 training docs\n",
      "627 evaluation docs\n",
      "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 766797 total word(s) in the data (91726 unique)\u001b[0m\n",
      "\u001b[38;5;3m⚠ 14 misaligned tokens in the training data\u001b[0m\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "============================ Trainable Lemmatizer ============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 38837 lemmatizer trees generated from training data\u001b[0m\n",
      "\u001b[38;5;4mℹ 13746 lemmatizer trees generated from dev data\u001b[0m\n",
      "\u001b[38;5;4mℹ 4387 lemmatizer trees (31.9% of dev trees) were found exclusively in\n",
      "the dev data.\u001b[0m\n",
      "\u001b[38;5;3m⚠ 1 training docs with 0 or 1 unique lemmas.\u001b[0m\n",
      "\u001b[38;5;2m✔ All training docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;2m✔ All dev docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4mℹ 81 training docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4mℹ 14 dev docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 5 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data configs/lemmatizer_trf.cfg --paths.train corpus/train/lemma_train --paths.dev corpus/dev/lemma_dev/ --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: grc\n",
      "Training pipeline: transformer, trainable_lemmatizer\n",
      "3076 training docs\n",
      "627 evaluation docs\n",
      "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 424100 total word(s) in the data (67136 unique)\u001b[0m\n",
      "\u001b[38;5;3m⚠ 14 misaligned tokens in the training data\u001b[0m\n",
      "10 most common words: ',' (24151), '.' (16237), 'καὶ' (11861), 'δὲ' (10087), '·'\n",
      "(9068), 'δʼ' (5213), 'τε' (3825), 'τὴν' (3540), 'τῶν' (3489), 'μὲν' (3418)\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "============================ Trainable Lemmatizer ============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 29126 lemmatizer trees generated from training data\u001b[0m\n",
      "\u001b[38;5;4mℹ 13746 lemmatizer trees generated from dev data\u001b[0m\n",
      "\u001b[38;5;4mℹ 6016 lemmatizer trees (43.8% of dev trees) were found exclusively in\n",
      "the dev data.\u001b[0m\n",
      "\u001b[38;5;3m⚠ 1 training docs with 0 or 1 unique lemmas.\u001b[0m\n",
      "\u001b[38;5;2m✔ All training docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;2m✔ All dev docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4mℹ 81 training docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4mℹ 14 dev docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 5 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data configs/lemmatizer_trf.cfg --paths.train corpus/train/lemma_train/Merged_lemma_train_dataset.spacy --paths.dev corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy --nlp.lang=grc --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-24 18:35:01,264] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'nlp.lang']\n",
      "\u001b[38;5;4mℹ Saving to output directory: training/lemmatizer_mostfiles\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-03-24 18:35:01,528] [INFO] Set up nlp object from config\n",
      "[2023-03-24 18:35:02,389] [DEBUG] Loading corpus from path: corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy\n",
      "[2023-03-24 18:35:02,390] [DEBUG] Loading corpus from path: corpus/train/lemma_train/Merged_lemma_train_dataset.spacy\n",
      "[2023-03-24 18:35:02,390] [INFO] Pipeline: ['transformer', 'trainable_lemmatizer']\n",
      "[2023-03-24 18:35:02,393] [INFO] Created vocabulary\n",
      "[2023-03-24 18:35:02,394] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-03-24 18:35:33,488] [INFO] Initialized pipeline components: ['transformer', 'trainable_lemmatizer']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2023-03-24 18:35:33,495] [DEBUG] Loading corpus from path: corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy\n",
      "[2023-03-24 18:35:33,496] [DEBUG] Loading corpus from path: corpus/train/lemma_train/Merged_lemma_train_dataset.spacy\n",
      "[2023-03-24 18:35:33,524] [DEBUG] Removed existing output directory: training/lemmatizer_mostfiles/model-best\n",
      "[2023-03-24 18:35:33,553] [DEBUG] Removed existing output directory: training/lemmatizer_mostfiles/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'trainable_lemmatizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matlomy\u001b[0m (\u001b[33matlomy-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/Projects/Atlomy/git/greCy_ATLOMY/wandb/run-20230324_183535-9pb0ycl5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-rain-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/9pb0ycl5\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS TRAIN...  LEMMA_ACC  SCORE \n",
      "---  ------  -------------  -------------  ---------  ------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./corpus)... Done. 0.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "  0       0           0.00         611.98      27.28    0.27\n",
      "  0     200         785.70      335479.67      27.28    0.27\n",
      "  1     400        3131.59      309447.37      27.89    0.28\n",
      "  2     600        2396.80      266858.59      47.62    0.48\n",
      "  3     800        2968.13      203982.71      64.38    0.64\n",
      "  3    1000        2989.96      163783.77      70.94    0.71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      "  4    1200        2979.45      140548.53      75.30    0.75\n",
      "  5    1400        2941.29      125736.81      78.08    0.78\n",
      "  6    1600        2887.92      113730.11      79.87    0.80\n",
      "  7    1800        2838.53      105272.75      81.15    0.81\n",
      "  7    2000        2714.63       96646.33      82.18    0.82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      "  8    2200        2569.28       90265.15      83.27    0.83\n",
      "  9    2400        2479.26       83534.61      83.97    0.84\n",
      " 10    2600        2460.38       81399.65      84.80    0.85\n",
      " 11    2800        2239.54       74437.65      85.19    0.85\n",
      " 11    3000        2254.12       72783.90      85.66    0.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.8s\n",
      " 12    3200        2086.64       67580.11      86.14    0.86\n",
      " 13    3400        2005.26       64034.13      86.36    0.86\n",
      " 14    3600        1900.41       60855.87      86.87    0.87\n",
      " 15    3800        1823.96       57600.11      87.13    0.87\n",
      " 15    4000        1792.33       55737.57      87.38    0.87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.8s\n",
      " 16    4200        1626.62       51289.12      87.54    0.88\n",
      " 17    4400        1611.43       49497.09      87.90    0.88\n",
      " 18    4600        1575.40       47886.65      87.85    0.88\n",
      " 19    4800        1521.01       45716.86      88.24    0.88\n",
      " 19    5000        1417.75       43179.03      88.26    0.88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 20    5200        1339.51       40572.88      88.59    0.89\n",
      " 21    5400        1340.72       39432.48      88.30    0.88\n",
      " 22    5600        1292.30       37628.82      88.66    0.89\n",
      " 23    5800        1241.39       35662.49      88.89    0.89\n",
      " 23    6000        1228.62       35203.51      88.81    0.89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 24    6200        1131.99       32439.27      89.07    0.89\n",
      " 25    6400        1147.16       31562.58      89.20    0.89\n",
      " 26    6600        1058.11       29490.86      89.34    0.89\n",
      " 27    6800        1063.60       28889.46      89.49    0.89\n",
      " 27    7000        1011.20       27285.23      89.47    0.89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 28    7200         932.31       25100.01      89.61    0.90\n",
      " 29    7400         931.73       24729.11      89.67    0.90\n",
      " 30    7600         894.06       23112.60      89.78    0.90\n",
      " 31    7800         883.21       22382.49      89.73    0.90\n",
      " 31    8000         847.82       21445.57      89.81    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 32    8200         754.46       19131.40      89.90    0.90\n",
      " 33    8400         786.49       18982.00      89.93    0.90\n",
      " 34    8600         771.82       18152.33      89.98    0.90\n",
      " 35    8800         660.15       15952.14      89.99    0.90\n",
      " 35    9000         719.96       16241.44      89.96    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 36    9200         639.47       14397.99      89.96    0.90\n",
      " 37    9400         596.99       13240.29      90.00    0.90\n",
      " 38    9600         597.18       12865.63      89.99    0.90\n",
      " 39    9800         567.99       11764.96      90.01    0.90\n",
      " 39   10000         590.30       11585.60      90.10    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 40   10200         510.61       10010.97      90.06    0.90\n",
      " 41   10400         514.35        9700.62      90.11    0.90\n",
      " 42   10600         479.16        8718.84      90.10    0.90\n",
      " 43   10800         479.16        8578.90      90.10    0.90\n",
      " 43   11000         440.52        7633.05      90.13    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 44   11200         394.82        6732.12      90.07    0.90\n",
      " 45   11400         419.29        6801.84      90.13    0.90\n",
      " 46   11600         398.12        6144.57      90.12    0.90\n",
      " 47   11800         353.92        5594.47      90.09    0.90\n",
      " 47   12000         369.49        5446.80      90.16    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 48   12200         338.93        4862.72      90.14    0.90\n",
      " 49   12400         341.88        4865.09      90.13    0.90\n",
      " 50   12600         280.04        4105.30      90.16    0.90\n",
      " 51   12800         302.26        4060.06      90.14    0.90\n",
      " 51   13000         270.57        3643.09      90.17    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 52   13200         250.55        3360.69      90.18    0.90\n",
      " 53   13400         224.95        3017.44      90.14    0.90\n",
      " 54   13600         220.97        2919.56      90.16    0.90\n",
      " 55   13800         236.93        2907.73      90.17    0.90\n",
      " 55   14000         243.43        2837.16      90.12    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 56   14200         198.17        2422.59      90.15    0.90\n",
      " 57   14400         199.26        2387.28      90.20    0.90\n",
      " 58   14600         181.00        2119.16      90.17    0.90\n",
      " 59   14800         176.29        2107.98      90.17    0.90\n",
      " 59   15000         187.84        2085.31      90.17    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 60   15200         155.13        1762.95      90.13    0.90\n",
      " 61   15400         160.87        1806.35      90.16    0.90\n",
      " 62   15600         152.11        1703.90      90.18    0.90\n",
      " 63   15800         150.20        1633.72      90.17    0.90\n",
      " 63   16000         132.57        1509.60      90.16    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 64   16200         124.51        1352.16      90.17    0.90\n",
      " 65   16400         142.03        1443.05      90.17    0.90\n",
      " 66   16600         117.70        1261.16      90.19    0.90\n",
      " 67   16800         115.58        1225.54      90.18    0.90\n",
      " 67   17000         116.13        1213.09      90.19    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 68   17200         108.77        1139.58      90.22    0.90\n",
      " 69   17400          97.47        1040.67      90.18    0.90\n",
      " 70   17600         104.29        1139.34      90.19    0.90\n",
      " 71   17800          93.25         937.00      90.20    0.90\n",
      " 71   18000          85.87         943.54      90.22    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 72   18200          94.53         978.55      90.21    0.90\n",
      " 73   18400          85.19         882.88      90.24    0.90\n",
      " 74   18600          88.11         930.85      90.20    0.90\n",
      " 75   18800          89.31         883.24      90.23    0.90\n",
      " 75   19000          86.97         874.20      90.21    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.8s\n",
      " 76   19200          83.79         844.86      90.22    0.90\n",
      " 77   19400          79.59         795.40      90.22    0.90\n",
      " 78   19600          75.48         819.06      90.22    0.90\n",
      " 79   19800          71.39         761.54      90.22    0.90\n",
      " 79   20000          81.61         825.37      90.22    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 lemma_acc ▁▁▆▇▇▇▇█████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_trainable_lemmatizer ▁█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_transformer ▁███▇▇▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     score ▁▁▆▇▇▇▇█████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     speed ▅▁▂▃▆▅▆▆▆▆▄▇▇█▇▃▇▇▇▇▇▇██▆▇▇▇▇▇█▇▇▇▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 token_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_f ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_p ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_r ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 lemma_acc 0.90222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_trainable_lemmatizer 825.36667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_transformer 81.61107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     score 0.90222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     speed 17238.50699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_f 0.98861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_p 0.98487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_r 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgenial-rain-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/9pb0ycl5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 386 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230324_183535-9pb0ycl5/logs\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "training/lemmatizer_mostfiles/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train configs/lemmatizer_trf.cfg --output training/lemmatizer_mostfiles --gpu-id 0 --paths.train corpus/train/lemma_train/Merged_lemma_train_dataset.spacy --paths.dev corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy --nlp.lang=grc --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "LEMMA   92.30 \n",
      "SPEED   4265  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark accuracy /root/Projects/Atlomy/git/greCy_ATLOMY/training/lemmatizer_mostfiles/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/grc_proiel-ud-test.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "training/lemmatizer_mostfiles/assembled\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-03-24 22:38:15,854] [INFO] Set up nlp object from config\n",
      "[2023-03-24 22:38:16,589] [INFO] Pipeline: ['transformer', 'morphologizer', 'tagger', 'parser', 'trainable_lemmatizer', 'attribute_ruler', 'sentencizer']\n",
      "[2023-03-24 22:38:16,595] [INFO] Created vocabulary\n",
      "[2023-03-24 22:38:16,595] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-03-24 22:38:28,008] [INFO] Initialized pipeline components: ['transformer', 'morphologizer', 'tagger', 'parser', 'attribute_ruler', 'sentencizer']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'morphologizer', 'tagger', 'parser',\n",
      "'trainable_lemmatizer', 'attribute_ruler', 'sentencizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Frozen components: ['trainable_lemmatizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matlomy\u001b[0m (\u001b[33matlomy-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/logging/__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "UnicodeEncodeError: 'ascii' codec can't encode character '\\xb7' in position 1275: ordinal not in range(128)\n",
      "Call stack:\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/cli/_util.py\", line 74, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/cli/train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/cli/train.py\", line 75, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/training/loop.py\", line 105, in train\n",
      "    log_step, finalize_logger = train_logger(nlp, stdout, stderr)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy_loggers/wandb.py\", line 157, in setup_logger\n",
      "    run = wandb.init(\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1144, in init\n",
      "    run = wi.init()\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 552, in init\n",
      "    logger.info(\n",
      "Message: \"wandb.init called with sweep_config: {}\\nconfig: {'components': {'attribute_ruler': {'factory': 'attribute_ruler', 'scorer': {'@scorers': 'spacy.attribute_ruler_scorer.v1'}, 'validate': False}, 'morphologizer': {'factory': 'morphologizer', 'extend': False, 'overwrite': True, 'scorer': {'@scorers': 'spacy.morphologizer_scorer.v1'}, 'model': {'@architectures': 'spacy.Tagger.v2', 'no': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy-transformers.TransformerListener.v1', 'grad_factor': 1.0, 'pooling': {'@layers': 'reduce_mean.v1'}, 'upstream': 'transformer'}}}, 'parser': {'factory': 'parser', 'learn_tokens': False, 'min_action_freq': 30, 'moves': None, 'scorer': {'@scorers': 'spacy.parser_scorer.v1'}, 'update_with_oracle_cut_size': 100, 'model': {'@architectures': 'spacy.TransitionBasedParser.v2', 'state_type': 'parser', 'extra_state_tokens': False, 'hidden_width': 64, 'maxout_pieces': 2, 'use_upper': False, 'no': None, 'tok2vec': {'@architectures': 'spacy-transformers.TransformerListener.v1', 'grad_factor': 1.0, 'pooling': {'@layers': 'reduce_mean.v1'}, 'upstream': 'transformer'}}}, 'sentencizer': {'factory': 'sentencizer', 'overwrite': False, 'punct_chars': ['.', ';', '·'], 'scorer': {'@scorers': 'spacy.senter_scorer.v1'}}, 'tagger': {'factory': 'tagger', 'neg_prefix': '!', 'overwrite': False, 'scorer': {'@scorers': 'spacy.tagger_scorer.v1'}, 'model': {'@architectures': 'spacy.Tagger.v2', 'no': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy-transformers.TransformerListener.v1', 'grad_factor': 1.0, 'pooling': {'@layers': 'reduce_mean.v1'}, 'upstream': 'transformer'}}}, 'trainable_lemmatizer': {'factory': 'trainable_lemmatizer', 'backoff': 'orth', 'min_tree_freq': 1, 'overwrite': False, 'scorer': {'@scorers': 'spacy.lemmatizer_scorer.v1'}, 'top_k': 6, 'model': {'@architectures': 'spacy.Tagger.v2', 'no': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy-transformers.Tok2VecTransformer.v3', 'name': 'Jacobo/aristoBERTo', 'mixed_precision': False, 'pooling': {'@layers': 'reduce_mean.v1'}, 'grad_factor': 1.0, 'get_spans': {'@span_getters': 'spacy-transformers.strided_spans.v1', 'window': 128, 'stride': 96}, 'tokenizer_config': {'use_fast': True}}}}, 'transformer': {'factory': 'transformer', 'max_batch_items': 4096, 'set_extra_annotations': {'@annotation_setters': 'spacy-transformers.null_annotation_setter.v1'}, 'model': {'@architectures': 'spacy-transformers.TransformerModel.v3', 'name': 'Jacobo/aristoBERTo', 'mixed_precision': False, 'get_spans': {'@span_getters': 'spacy-transformers.strided_spans.v1', 'window': 128, 'stride': 96}, 'tokenizer_config': {'use_fast': True}}}}, 'corpora': {'dev': {'@readers': 'spacy.Corpus.v1', 'max_length': 0, 'gold_preproc': False, 'limit': 0, 'augmenter': None}, 'train': {'@readers': 'spacy.Corpus.v1', 'max_length': 0, 'gold_preproc': False, 'limit': 0, 'augmenter': None}}, 'initialize': {'vectors': None, 'init_tok2vec': None, 'vocab_data': None, 'lookups': None, 'before_init': None, 'after_init': None, 'components': {'attribute_ruler': {'patterns': {'@readers': 'srsly.read_json.v1', 'path': 'data/augments/attribute_ruler_patterns.json'}}, 'parser': {'labels': {'@readers': 'spacy.read_labels.v1', 'path': 'data/labels/parser.json', 'require': False}}, 'tagger': {'labels': {'@readers': 'spacy.read_labels.v1', 'path': 'data/labels/tagger.json', 'require': False}}}}, 'nlp': {'lang': 'grc', 'pipeline': ['transformer', 'morphologizer', 'tagger', 'parser', 'trainable_lemmatizer', 'attribute_ruler', 'sentencizer'], 'batch_size': 128, 'disabled': ['trainable_lemmatizer'], 'before_creation': None, 'after_creation': None, 'after_pipeline_creation': None, 'tokenizer': {'@tokenizers': 'spacy.Tokenizer.v1'}}, 'paths': {'vectors': None, 'init_tok2vec': None}, 'system': {'gpu_allocator': 'pytorch', 'seed': 0}, 'training': {'accumulate_gradient': 3, 'dev_corpus': 'corpora.dev', 'train_corpus': 'corpora.train', 'seed': 0, 'gpu_allocator': 'pytorch', 'dropout': 0.1, 'patience': 5000, 'max_epochs': 0, 'max_steps': 20000, 'eval_frequency': 200, 'frozen_components': ['trainable_lemmatizer'], 'annotating_components': [], 'before_to_disk': None, 'before_update': None, 'batcher': {'@batchers': 'spacy.batch_by_padded.v1', 'discard_oversize': True, 'size': 2000, 'buffer': 256, 'get_length': None}, 'logger': {'@loggers': 'spacy.WandbLogger.v3', 'project_name': 'Lemmatizer (Ancient Greek)', 'remove_config_values': ['paths.train', 'paths.dev', 'corpora.train.path', 'corpora.dev.path'], 'log_dataset_dir': './corpus', 'model_log_interval': 1000, 'entity': None, 'run_name': 'greCy_mostfiles_full_pipeline_pytorch2_0.2'}, 'optimizer': {'@optimizers': 'Adam.v1', 'beta1': 0.9, 'beta2': 0.999, 'l2_is_weight_decay': True, 'l2': 0.01, 'grad_clip': 1.0, 'use_averages': True, 'eps': 1e-08, 'learn_rate': {'@schedules': 'warmup_linear.v1', 'warmup_steps': 250, 'total_steps': 20000, 'initial_rate': 5e-05}}, 'score_weights': {'pos_acc': 0.06, 'morph_acc': 0.06, 'morph_per_feat': None, 'tag_acc': 0.12, 'dep_uas': 0.06, 'dep_las': 0.06, 'dep_las_per_type': None, 'sents_p': None, 'sents_r': None, 'sents_f': 0.01, 'lemma_acc': 0.63}}}\"\n",
      "Arguments: ()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/Projects/Atlomy/git/greCy_ATLOMY/wandb/run-20230324_223829-oco0a1pd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgood-serenity-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/oco0a1pd\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS MORPH...  LOSS TAGGER  LOSS PARSER  POS_ACC  MORPH_ACC  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  LEMMA_ACC  SCORE \n",
      "---  ------  -------------  -------------  -----------  -----------  -------  ---------  -------  -------  -------  -------  ---------  ------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./corpus)... Done. 0.2s\n",
      "  0       0         766.75         428.58       410.35       787.48     4.53       2.28     5.11     7.46     7.46     0.14      92.20    0.60\n",
      "  1     200      542178.94      355306.31    320113.55    415712.30    43.61      27.42    54.70    46.97    31.43    11.53      92.20    0.74\n",
      "  3     400      292994.76      302493.78    109981.13    251310.83    93.78      46.86    95.50    69.02    59.95    42.19      92.20    0.86\n",
      "  5     600      246842.02      174491.84     17436.99    198574.38    96.84      77.26    97.74    74.77    67.76    50.83      92.20    0.89\n",
      "  7     800      214566.99       87736.13      8158.59    166454.00    97.60      87.01    98.05    78.60    72.38    59.38      92.20    0.91\n",
      "  9    1000      189238.34       57317.92      5228.10    143611.61    97.88      89.72    98.22    80.36    74.74    64.58      92.20    0.91\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 2.1s\n",
      " 11    1200      168711.72       43326.52      3711.72    127073.00    98.18      91.24    98.27    81.20    76.24    60.44      92.20    0.91\n",
      " 13    1400      153416.39       36131.99      2863.55    114584.41    98.15      91.85    98.24    81.94    77.12    66.99      92.20    0.91\n",
      " 15    1600      136258.07       29993.31      2110.94    100863.52    98.16      92.40    98.37    82.76    78.11    65.70      92.20    0.92\n",
      " 17    1800      124426.19       26158.38      1777.72     92941.02    98.22      92.81    98.35    83.39    78.80    69.05      92.20    0.92\n",
      " 19    2000      110306.39       22516.33      1343.00     82765.95    98.23      93.17    98.35    83.69    79.40    68.25      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.6s\n",
      " 21    2200      100165.39       19640.17      1081.03     75449.58    98.20      93.25    98.32    84.04    79.83    69.53      92.20    0.92\n",
      " 23    2400       94073.19       17138.90       948.16     70132.60    98.21      93.41    98.33    84.07    80.00    68.38      92.20    0.92\n",
      " 25    2600       85840.48       15354.38       783.25     64698.83    98.33      93.53    98.41    84.27    80.30    70.44      92.20    0.92\n",
      " 27    2800       80529.72       13163.55       685.37     60824.20    98.27      93.58    98.43    84.15    80.07    70.56      92.20    0.92\n",
      " 28    3000       73140.68       11562.50       594.46     55825.58    98.26      93.66    98.43    84.13    80.29    69.57      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.3s\n",
      " 30    3200       70834.00       10098.06       482.00     53799.06    98.25      93.90    98.40    84.13    80.41    68.08      92.20    0.92\n",
      " 32    3400       63210.01        8704.03       502.63     48817.55    98.34      93.85    98.45    84.30    80.71    69.36      92.20    0.92\n",
      " 34    3600       62345.78        7779.15       408.13     47924.59    98.27      93.93    98.36    84.35    80.64    70.96      92.20    0.92\n",
      " 36    3800       62168.89        6636.00       351.55     46447.20    98.31      93.91    98.43    84.62    80.80    69.85      92.20    0.92\n",
      " 38    4000       51469.50        5730.68       322.11     41577.98    98.34      94.03    98.43    84.77    80.95    70.91      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      " 40    4200       48019.40        5165.03       286.10     39788.38    98.29      93.81    98.36    84.41    80.71    71.31      92.20    0.92\n",
      " 42    4400       44606.33        4396.22       299.75     37716.42    98.32      94.04    98.41    84.82    81.17    70.31      92.20    0.92\n",
      " 44    4600       43628.47        3810.78       239.08     37469.19    98.36      93.96    98.48    84.85    81.15    71.26      92.20    0.92\n",
      " 46    4800       40818.94        3263.46       216.93     35443.08    98.37      94.07    98.40    85.00    81.42    71.14      92.20    0.92\n",
      " 48    5000       40042.72        3018.53       261.87     34276.22    98.29      94.04    98.43    84.78    81.23    70.35      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      " 50    5200       36778.06        2561.62       159.30     33317.08    98.25      93.93    98.37    84.54    80.96    71.18      92.20    0.92\n",
      " 52    5400       34748.03        2268.40       161.52     32108.29    98.30      93.99    98.45    84.69    81.18    69.82      92.20    0.92\n",
      " 54    5600       32562.66        1980.55       138.99     31267.17    98.34      94.04    98.41    85.01    81.38    70.24      92.20    0.92\n",
      " 56    5800       31986.01        1765.25       155.15     30367.12    98.33      94.08    98.43    85.15    81.64    72.06      92.20    0.92\n",
      " 58    6000       31708.88        1526.98       154.48     30075.53    98.35      94.13    98.41    84.68    81.15    70.79      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.5s\n",
      " 60    6200       29142.48        1318.83       126.04     29170.60    98.33      94.28    98.41    84.75    81.23    71.10      92.20    0.92\n",
      " 62    6400       29033.94        1177.53       139.53     29037.55    98.36      94.10    98.43    85.09    81.64    72.10      92.20    0.92\n",
      " 64    6600       27420.84        1059.83       129.05     28144.00    98.31      94.04    98.43    85.20    81.70    70.96      92.20    0.92\n",
      " 66    6800       26898.10         919.69       114.87     27617.66    98.26      93.91    98.34    85.09    81.73    70.47      92.20    0.92\n",
      " 68    7000       25439.77         851.54        97.46     27293.56    98.34      94.15    98.40    85.14    81.57    70.46      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.4s\n",
      " 70    7200       24229.67         739.36        87.77     26890.37    98.34      94.15    98.43    85.13    81.61    70.74      92.20    0.92\n",
      " 71    7400       24146.20         713.25       127.24     26755.65    98.32      94.07    98.39    85.36    81.86    71.63      92.20    0.92\n",
      " 73    7600       24025.59         678.18        92.50     26999.47    98.33      93.95    98.41    85.17    81.72    70.87      92.20    0.92\n",
      " 75    7800       22631.46         570.75        78.36     26467.89    98.30      94.03    98.40    85.37    81.78    71.90      92.20    0.92\n",
      " 77    8000       22464.73         581.24        75.72     26423.18    98.33      93.99    98.43    85.20    81.71    71.35      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.6s\n",
      " 79    8200       21154.44         477.55        78.09     25483.14    98.32      93.89    98.43    85.09    81.63    71.12      92.20    0.92\n",
      " 81    8400       20794.14         479.59        80.51     25327.79    98.37      94.13    98.40    85.20    81.65    71.11      92.20    0.92\n",
      " 83    8600       19960.25         435.68        69.88     25150.12    98.37      93.94    98.40    84.90    81.40    70.75      92.20    0.92\n",
      " 85    8800       20333.37         430.08        76.31     25469.86    98.38      94.00    98.46    85.02    81.61    71.42      92.20    0.92\n",
      " 87    9000       19979.39         374.49        59.67     25193.06    98.37      94.04    98.45    85.34    81.92    71.74      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      " 89    9200       19186.29         367.52        59.61     24580.58    98.37      94.15    98.43    85.47    81.97    71.04      92.20    0.92\n",
      " 91    9400       18495.04         331.99        51.83     24975.71    98.41      94.06    98.48    85.23    81.80    71.81      92.20    0.92\n",
      " 93    9600       18091.83         315.00        72.66     24520.19    98.37      93.88    98.44    85.49    81.85    71.16      92.20    0.92\n",
      " 95    9800       18031.73         290.72        59.20     24518.61    98.38      93.92    98.48    85.45    81.93    72.00      92.20    0.92\n",
      " 97   10000       17655.37         279.53        51.38     24233.77    98.37      94.02    98.46    85.53    81.97    71.59      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 0.9s\n",
      " 99   10200       17263.77         262.76        39.84     24371.86    98.37      93.89    98.47    85.47    82.02    71.69      92.20    0.92\n",
      "101   10400       16992.15         244.23        38.16     24205.11    98.43      93.91    98.52    85.13    81.72    71.61      92.20    0.92\n",
      "103   10600       16505.01         257.34        58.71     23363.14    98.46      93.99    98.56    85.28    81.80    71.41      92.20    0.92\n",
      "105   10800       16312.29         210.53        30.79     24087.30    98.41      94.04    98.45    85.36    81.90    72.43      92.20    0.92\n",
      "107   11000       16122.31         205.18        29.54     23974.94    98.39      93.90    98.44    85.37    81.89    71.80      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 0.9s\n",
      "108   11200       15361.41         191.08        36.48     23631.91    98.39      93.95    98.48    85.46    82.07    72.48      92.20    0.92\n",
      "110   11400       15427.93         195.26        35.69     23811.38    98.40      94.11    98.45    85.51    82.08    72.25      92.20    0.92\n",
      "112   11600       15281.70         177.03        31.73     23840.92    98.44      94.10    98.50    85.71    82.24    72.11      92.20    0.92\n",
      "114   11800       15098.48         147.73        40.87     23464.94    98.40      94.15    98.47    85.64    82.22    72.89      92.20    0.92\n",
      "116   12000       14696.03         153.36        44.89     23204.80    98.44      93.96    98.51    85.38    82.00    71.85      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      "118   12200       14849.60         146.61        25.15     23631.27    98.45      94.12    98.49    85.80    82.43    73.62      92.20    0.92\n",
      "120   12400       14289.57         155.06        28.81     23404.40    98.43      93.89    98.50    85.65    82.22    72.30      92.20    0.92\n",
      "122   12600       13864.79         130.98        29.05     23061.04    98.43      94.07    98.52    85.52    82.09    73.17      92.20    0.92\n",
      "124   12800       13528.43         104.62        25.37     23155.34    98.47      94.12    98.56    85.69    82.32    72.32      92.20    0.92\n",
      "126   13000       13411.33         132.40        30.79     23318.83    98.42      94.02    98.49    85.68    82.24    72.06      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      "128   13200       13438.95          95.43        17.13     23411.82    98.43      94.06    98.55    85.41    82.05    72.48      92.20    0.92\n",
      "130   13400       13273.93         114.05        20.82     23090.52    98.41      94.02    98.55    85.61    82.24    72.62      92.20    0.92\n",
      "132   13600       12661.58         111.70        16.19     22822.93    98.45      94.06    98.51    85.48    82.08    71.67      92.20    0.92\n",
      "134   13800       12358.25         105.94        20.77     22541.72    98.44      94.07    98.53    85.49    82.11    72.13      92.20    0.92\n",
      "136   14000       12668.44          97.03        15.25     23024.86    98.43      94.04    98.51    85.53    82.12    71.15      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.2s\n",
      "138   14200       12493.08          79.39        18.68     22655.87    98.39      93.98    98.45    85.54    82.16    71.16      92.20    0.92\n",
      "140   14400       12259.27         105.06        16.85     22834.81    98.45      93.97    98.50    85.75    82.31    72.75      92.20    0.92\n",
      "142   14600       12145.74          83.66        21.45     22855.40    98.43      94.06    98.49    85.69    82.24    72.38      92.20    0.92\n",
      "144   14800       11963.96          92.52        11.60     22809.51    98.41      94.04    98.45    85.65    82.25    72.09      92.20    0.92\n",
      "145   15000       11850.36          65.19        19.87     22729.92    98.39      94.00    98.45    85.62    82.19    71.89      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.0s\n",
      "147   15200       11921.50          78.62        21.60     22518.23    98.45      94.07    98.50    85.68    82.28    72.20      92.20    0.92\n",
      "149   15400       11540.69          65.48        20.82     22568.14    98.39      93.92    98.48    85.61    82.19    72.83      92.20    0.92\n",
      "151   15600       11723.51          74.24        17.93     22803.69    98.43      93.96    98.51    85.64    82.18    72.96      92.20    0.92\n",
      "153   15800       11380.61          64.37         9.56     22500.97    98.42      94.00    98.50    85.58    82.25    72.67      92.20    0.92\n",
      "155   16000       11048.91          54.25         9.56     22323.90    98.41      93.96    98.51    85.47    82.12    72.45      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.7s\n",
      "157   16200       11129.87          56.47        13.77     22845.78    98.41      94.05    98.48    85.45    82.13    72.33      92.20    0.92\n",
      "159   16400       11065.60          54.66        13.91     22573.81    98.39      94.06    98.48    85.58    82.17    72.88      92.20    0.92\n",
      "161   16600       11052.11          55.25        22.73     22415.48    98.40      94.12    98.45    85.54    82.22    72.00      92.20    0.92\n",
      "163   16800       11108.62          53.52         6.13     22705.11    98.43      94.07    98.50    85.46    82.14    72.40      92.20    0.92\n",
      "165   17000       10502.80          55.73        13.88     22007.97    98.41      94.08    98.48    85.67    82.25    72.10      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 2.4s\n",
      "167   17200       10451.96          57.42        10.24     22215.94    98.41      94.03    98.48    85.53    82.09    72.00      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 16416.046 MB of 16416.062 MB uploaded (8470.896 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_las ▁▆▇▇████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_uas ▁▇▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lemma_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_morphologizer ▁█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_parser ▁█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_tagger ▁█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_transformer ▁█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          morph_acc ▁▄▇█████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_f ▁▅██████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_p ▁▅██████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_r ▁▆██████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            pos_acc ▁███████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              score ▁▇██████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_f ▁▅▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_p ▁▅▆▆▇▇██████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_r ▁▆▇▇▇███████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              speed ▅▇▇▇▇▃▇▇██▁█▄▇▇▇▇▇▇███▇█▇▇▇█▇▇▇▇▇▇█▆▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            tag_acc ▁███████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_f ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_p ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_r ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_las 0.82091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_uas 0.85526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lemma_acc 0.92199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_morphologizer 57.4178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_parser 22215.9397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_tagger 10.23548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_transformer 10451.9632\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          morph_acc 0.9403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_f 0.98198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_p 0.98205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_r 0.98191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            pos_acc 0.9841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              score 0.92226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_f 0.72004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_p 0.70384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_r 0.737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              speed 8289.53275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            tag_acc 0.98476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_f 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_p 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_r 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgood-serenity-26\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/oco0a1pd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 374 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230324_223829-oco0a1pd/logs\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "training/lemmatizer_mostfiles/assembled/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train configs/transformer.cfg --output training/lemmatizer_mostfiles/assembled --gpu-id 0 --paths.train corpus/train/grc_proiel-ud-train.spacy --paths.dev corpus/dev/grc_proiel-ud-dev.spacy --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      98.25 \n",
      "POS      98.16 \n",
      "MORPH    93.98 \n",
      "LEMMA    92.30 \n",
      "UAS      84.93 \n",
      "LAS      81.33 \n",
      "SENT P   69.30 \n",
      "SENT R   69.44 \n",
      "SENT F   69.37 \n",
      "SPEED    3917  \n",
      "\n",
      "\u001b[1m\n",
      "============================== MORPH (per feat) ==============================\u001b[0m\n",
      "\n",
      "                P        R        F\n",
      "Case        98.88    98.80    98.84\n",
      "Gender      94.03    94.39    94.21\n",
      "Number      99.56    99.44    99.50\n",
      "Aspect      98.46    98.39    98.42\n",
      "Mood        99.01    98.72    98.86\n",
      "Person      98.77    98.02    98.40\n",
      "Tense       98.54    98.43    98.48\n",
      "VerbForm    99.63    99.49    99.56\n",
      "Voice       97.84    97.74    97.79\n",
      "PronType    98.52    98.03    98.27\n",
      "Degree      95.70    96.20    95.95\n",
      "Definite    99.33    99.39    99.36\n",
      "Reflex     100.00   100.00   100.00\n",
      "Polarity    99.54    98.64    99.09\n",
      "Poss       100.00    92.31    96.00\n",
      "\n",
      "\u001b[1m\n",
      "=============================== LAS (per type) ===============================\u001b[0m\n",
      "\n",
      "                  P       R       F\n",
      "iobj          77.13   78.72   77.92\n",
      "root          86.08   86.25   86.16\n",
      "nsubj         81.24   80.61   80.92\n",
      "advmod        76.11   71.67   73.82\n",
      "advcl         72.93   75.95   74.41\n",
      "ccomp         68.71   62.35   65.37\n",
      "discourse     84.50   84.73   84.62\n",
      "obj           86.46   87.32   86.89\n",
      "det           94.67   94.34   94.51\n",
      "nmod          78.04   76.83   77.43\n",
      "cop           81.86   83.02   82.44\n",
      "appos         55.65   39.02   45.88\n",
      "case          96.17   97.16   96.66\n",
      "acl           60.42   53.05   56.49\n",
      "mark          90.50   92.59   91.53\n",
      "obl           77.85   79.90   78.86\n",
      "nsubj:pass    73.08   80.00   76.38\n",
      "xcomp         69.23   71.25   70.23\n",
      "cc            73.16   72.87   73.02\n",
      "conj          67.37   66.49   66.93\n",
      "dislocated    30.00   10.71   15.79\n",
      "amod          81.13   79.63   80.37\n",
      "parataxis      0.00    0.00    0.00\n",
      "dep            0.00    0.00    0.00\n",
      "nummod        88.57   88.57   88.57\n",
      "fixed         85.71   75.00   80.00\n",
      "obl:agent     86.67   59.09   70.27\n",
      "orphan        25.00   20.00   22.22\n",
      "csubj:pass    75.00   42.86   54.55\n",
      "vocative      83.93   77.05   80.34\n",
      "aux            0.00    0.00    0.00\n",
      "aux:pass       0.00    0.00    0.00\n",
      "flat:name    100.00   85.71   92.31\n",
      "\n",
      "\u001b[38;5;2m✔ Generated 200 parses as HTML\u001b[0m\n",
      "training/lemmatizer_mostfiles/assembled/model-best\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark accuracy training/lemmatizer_mostfiles/assembled/model-best corpus/test/grc_proiel-ud-test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path training/lemmatizer_mostfiles/assembled/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      -     \n",
      "POS      -     \n",
      "MORPH    -     \n",
      "LEMMA    -     \n",
      "UAS      -     \n",
      "LAS      -     \n",
      "SENT P   -     \n",
      "SENT R   -     \n",
      "SENT F   -     \n",
      "SPEED    802   \n",
      "\n",
      "\u001b[38;5;2m✔ Generated 200 parses as HTML\u001b[0m\n",
      "/root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "φλέψ = \"φλέψ\"\n",
    "#normalize texwor\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "φλέψ = normalize_text(φλέψ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttest = normalize_text(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι. ἐπειδὴ γὰρ τὴν τῶν ὀστῶν κατασκευὴν ἐν αὐτοῖς ἐπιτηδειοτάτην ὀργάνοις ἀντιληπτικοῖς ἡ φύσις ἐποίησεν, ἦν δ’ ἀμήχανον αὐτοῖς τοῖς ὀστοῖς οὕτω γεώδεσί τε καὶ λιθώδεσιν οὖσι μεταδοῦναι τῆς καθ’ ὁρμὴν κινήσεως, ἐξεῦρεν, ὅτῳ τρόπῳ δι’ ἑτέρων αὐτὰ κινήσει. τῶν οὖν κατὰ τὸν πῆχυν μυῶν ἀποφύσασα τένοντας εὐθὺ τῶν δακτύλων ἤγαγεν. ἃ γὰρ οἱ παλαιοὶ καλοῦσι νεῦρα, ταυτὶ τὰ προφανῆ, τὰ κινοῦντα τοὺς δακτύλους, οἱ τένοντές εἰσιν·\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι.\n",
      "ἐπειδὴ γὰρ τὴν τῶν ὀστῶν κατασκευὴν ἐν αὐτοῖς ἐπιτηδειοτάτην ὀργάνοις ἀντιληπτικοῖς ἡ φύσις ἐποίησεν, ἦν δ’ ἀμήχανον αὐτοῖς τοῖς ὀστοῖς οὕτω γεώδεσί τε καὶ λιθώδεσιν οὖσι μεταδοῦναι τῆς καθ’ ὁρμὴν κινήσεως, ἐξεῦρεν\n",
      ", ὅτῳ τρόπῳ δι’ ἑτέρων αὐτὰ κινήσει\n",
      ".\n",
      "τῶν οὖν κατὰ τὸν πῆχυν μυῶν ἀποφύσασα τένοντας εὐθὺ τῶν δακτύλων ἤγαγεν\n",
      ".\n",
      "ἃ γὰρ οἱ παλαιοὶ καλοῦσι νεῦρα, ταυτὶ τὰ προφανῆ, τὰ κινοῦντα τοὺς δακτύλους, οἱ τένοντές εἰσιν·\n",
      "Ταῦτ ADJ obj  Ταῦτ\n",
      "’ ADV discourse  punc1\n",
      "οὖν ADV discourse  οὖν\n",
      "εἴς ADP case  εἴς\n",
      "τε CCONJ cc  τε\n",
      "τὸν DET det  τὸν\n",
      "παρόντα VERB obj  παρόντα\n",
      "καὶ CCONJ cc  καί\n",
      "τὸν DET det  τὸν\n",
      "ἑξῆς ADV amod  ἑξῆς\n",
      "ἅπαντα ADJ amod  ἅπας\n",
      "λόγον NOUN conj  λόγος\n",
      "οἷον ADV advmod  οἷος\n",
      "ὑποθέσεις NOUN obj  ὑποθέσεις\n",
      "τινὰς DET det  τινὰς\n",
      "τῶν DET det  ὁ\n",
      "ἀποδείξεων NOUN nmod  ἀποδείξις\n",
      "λαμβάνοντες VERB advcl  λαμβάνω\n",
      "ἐν ADP case  ἐν\n",
      "ἑκάστῳ ADJ obl  ἑκάστῳ\n",
      "τῶν DET det  ὁ\n",
      "ὀργάνων NOUN nmod  ὀργάνων\n",
      "τὴν DET det  τὴν\n",
      "ἐξ ADP case  ἐκ\n",
      "αὐτῶν PRON nmod  αὐτός\n",
      "ὠφέλειαν NOUN obj  ὠφέλεια\n",
      "ἐροῦμεν VERB ROOT  ἐροῦμεν\n",
      "ἀπὸ ADP case  ἀπὸ\n",
      "τῶν DET det  ὁ\n",
      "δακτύλων NOUN obl  δακτῦλον\n",
      "αὖθις ADV advmod  αὖθις\n",
      "ἀρξάμενοι VERB advcl  ἀρξάμενοι\n",
      ". PUNCT advmod  .\n",
      "ἐπειδὴ SCONJ mark  ἐπειδὴς\n",
      "γὰρ ADV discourse  γὰρ\n",
      "τὴν DET det  τὴν\n",
      "τῶν DET det  ὁ\n",
      "ὀστῶν NOUN nmod  ὀσ́τος\n",
      "κατασκευὴν NOUN obj  κατασκευή\n",
      "ἐν ADP case  ἐν\n",
      "αὐτοῖς PRON obl  αὐτός\n",
      "ἐπιτηδειοτάτην ADJ amod  ἐπιτηδειοτάτην\n",
      "ὀργάνοις NOUN xcomp  ὀργᾶνον\n",
      "ἀντιληπτικοῖς ADJ amod  ἀντιληπτικός\n",
      "ἡ DET det  ὁ\n",
      "φύσις NOUN nsubj  φύσις\n",
      "ἐποίησεν VERB advcl  ἐποίησεν\n",
      ", PUNCT dep  ,\n",
      "ἦν AUX cop  ἦν\n",
      "δ’ ADV discourse  ὁ μέν...ὁ δέ\n",
      "ἀμήχανον ADJ parataxis  ἀμήχανος\n",
      "αὐτοῖς DET det  αὐτός\n",
      "τοῖς DET det  ὁ\n",
      "ὀστοῖς NOUN iobj  ὀστοῖς\n",
      "οὕτω ADV advmod  οὕτως\n",
      "γεώδεσί ADJ advcl  γεώδεσί\n",
      "τε CCONJ cc  τε\n",
      "καὶ CCONJ cc  καί\n",
      "λιθώδεσιν ADJ conj  λιθώδης\n",
      "οὖσι AUX cop  οὖς\n",
      "μεταδοῦναι VERB ccomp  μεταδοῦναι\n",
      "τῆς DET det  ὁ\n",
      "καθ’ ADP case  καθά\n",
      "ὁρμὴν NOUN nmod  ὁρμή\n",
      "κινήσεως NOUN iobj  κινήσεως\n",
      ", PUNCT advmod  ,\n",
      "ἐξεῦρεν VERB ROOT  ἐξεῦρεν\n",
      ", PUNCT advmod  ,\n",
      "ὅτῳ PRON det  ὅτῳ\n",
      "τρόπῳ NOUN obl  τρόπῳ\n",
      "δι’ ADP case  διά\n",
      "ἑτέρων ADJ advmod  ἑτέρων\n",
      "αὐτὰ PRON obj  αὐτὰ\n",
      "κινήσει VERB ROOT  κινήω\n",
      ". PUNCT ROOT  .\n",
      "τῶν DET det  ὁ\n",
      "οὖν ADV discourse  οὖν\n",
      "κατὰ ADP case  κατὰ\n",
      "τὸν DET det  τὸν\n",
      "πῆχυν NOUN nmod  πῆχυς\n",
      "μυῶν NOUN obj  μ́υος\n",
      "ἀποφύσασα VERB advcl  ἀποφύω\n",
      "τένοντας NOUN obj  τένων\n",
      "εὐθὺ ADV advmod  εὐθύς\n",
      "τῶν DET det  ὁ\n",
      "δακτύλων NOUN iobj  δακτῦλον\n",
      "ἤγαγεν VERB ROOT  ἤγαγεν\n",
      ". PUNCT ROOT  .\n",
      "ἃ PRON dep  ἃ\n",
      "γὰρ ADV discourse  γὰρ\n",
      "οἱ DET det  οἱ\n",
      "παλαιοὶ ADJ nsubj  παλαιοὶ\n",
      "καλοῦσι VERB acl  καλέω\n",
      "νεῦρα NOUN dep  νεῦρον\n",
      ", PUNCT dep  ,\n",
      "ταυτὶ DET det  ταυτὶ\n",
      "τὰ DET det  τὰ\n",
      "προφανῆ ADJ conj  προφανής\n",
      ", PUNCT dep  ,\n",
      "τὰ DET det  τὰ\n",
      "κινοῦντα VERB conj  κινοῦντα\n",
      "τοὺς DET det  τοὺς\n",
      "δακτύλους NOUN obj  δάκτυλος\n",
      ", PUNCT dep  ,\n",
      "οἱ DET det  οἱ\n",
      "τένοντές NOUN dep  τένοντές\n",
      "εἰσιν AUX cop  εἰμί\n",
      "· PUNCT ROOT  ·\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texttest)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc:\n",
    "     print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "Warming up for 3 epochs...\n",
      "100%|████████████████████████████████████████| 315/315 [00:04<00:00, 72.84doc/s]\n",
      "\n",
      "Benchmarking 50 batches...\n",
      "100%|██████████████████████████████████████| 6400/6400 [02:47<00:00, 38.32doc/s]\n",
      "\n",
      "Outliers: 20.0%, extreme outliers: 0.0%\n",
      "Mean: 6767.7 words/s (95% CI: -701.5 +659.6)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark speed /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/grc_proiel-ud-test.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "#normalize texwor\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"καὶ γὰρ ἀναπνεῖ καὶ ἐκπνεῖ ταύτῃ καὶ ὁ πταρμὸς διὰ ταύτης γίνεται πνεύματος ἀθρόου ἔξοδος σημεῖον οἰωνιστικὸν καὶ ἱερὸν μόνον τῶν πνευμάτων\"\n",
    "text =(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι.\")\n",
    "text_n = normalize_text(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"training/transformer_mostfiles/assembled/model-best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ταῦτ Ταῦτ ADJ obj\n",
      "’ punc1 ADV discourse\n",
      "οὖν οὖν ADV discourse\n",
      "εἴς εἴς ADP case\n",
      "τε τε CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "παρόντα πάρειμι VERB dep\n",
      "καὶ καί CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "ἑξῆς ἑξῆς ADV amod\n",
      "ἅπαντα ἅπας ADJ amod\n",
      "λόγον λόγος NOUN conj\n",
      "οἷον οἷος ADV advmod\n",
      "ὑποθέσεις ὑποθέσεις NOUN obj\n",
      "τινὰς τινὰς DET det\n",
      "τῶν ὁ DET det\n",
      "ἀποδείξεων ἀποδείξις NOUN nmod\n",
      "λαμβάνοντες λαμβάνω VERB advcl\n",
      "ἐν ἐν ADP case\n",
      "ἑκάστῳ ἕκαστος ADJ obl\n",
      "τῶν ὁ DET det\n",
      "ὀργάνων ὀργάνον NOUN nmod\n",
      "τὴν τὴν DET det\n",
      "ἐξ ἐκ ADP case\n",
      "αὐτῶν αὐτός PRON nmod\n",
      "ὠφέλειαν ὠφέλεια NOUN obj\n",
      "ἐροῦμεν ἐροῦμεν VERB ROOT\n",
      "ἀπὸ ἀπὸ ADP case\n",
      "τῶν ὁ DET det\n",
      "δακτύλων δάκτυλος NOUN obl\n",
      "αὖθις αὖθις ADV advmod\n",
      "ἀρξάμενοι ἄρχω VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "#test non nfkd\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ταῦτ Ταῦτ ADJ obj\n",
      "’ punc1 ADV discourse\n",
      "οὖν οὖν ADV discourse\n",
      "εἴς εἴς ADP case\n",
      "τε τε CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "παρόντα παρόντα VERB dep\n",
      "καὶ καί CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "ἑξῆς ἑξῆς ADV amod\n",
      "ἅπαντα ἅπας ADJ amod\n",
      "λόγον λόγος NOUN conj\n",
      "οἷον οἷος ADV advmod\n",
      "ὑποθέσεις ὑποθέσεις NOUN obj\n",
      "τινὰς τινὰς DET det\n",
      "τῶν ὁ DET det\n",
      "ἀποδείξεων ἀποδείξις NOUN nmod\n",
      "λαμβάνοντες λαμβάνω VERB advcl\n",
      "ἐν ἐν ADP case\n",
      "ἑκάστῳ ἑκάστῳ ADJ obl\n",
      "τῶν ὁ DET det\n",
      "ὀργάνων ὀργάνον NOUN nmod\n",
      "τὴν τὴν DET det\n",
      "ἐξ ἐκ ADP case\n",
      "αὐτῶν αὐτός PRON nmod\n",
      "ὠφέλειαν ὠφέλεια NOUN obj\n",
      "ἐροῦμεν ἐροῦμεν VERB ROOT\n",
      "ἀπὸ ἀπὸ ADP case\n",
      "τῶν ὁ DET det\n",
      "δακτύλων δακτύλων NOUN obl\n",
      "αὖθις αὖθις ADV advmod\n",
      "ἀρξάμενοι ἀρξάμενοι VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "#test nfkd\n",
    "doc = nlp(text_n)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ταῦτ Ταῦτ ADJ obj\n",
      "’ punc1 ADV discourse\n",
      "οὖν οὖν ADV discourse\n",
      "εἴς εἴς ADP case\n",
      "τε τε CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "παρόντα πάρειμι VERB dep\n",
      "καὶ καί CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "ἑξῆς ἑξῆς ADV amod\n",
      "ἅπαντα ἅπας ADJ amod\n",
      "λόγον λόγος NOUN conj\n",
      "οἷον οἷος ADV advmod\n",
      "ὑποθέσεις ὑποθέσεις NOUN obj\n",
      "τινὰς τινὰς DET det\n",
      "τῶν ὁ DET det\n",
      "ἀποδείξεων ἀποδείξις NOUN nmod\n",
      "λαμβάνοντες λαμβάνω VERB advcl\n",
      "ἐν ἐν ADP case\n",
      "ἑκάστῳ ἕκαστος ADJ obl\n",
      "τῶν ὁ DET det\n",
      "ὀργάνων ὀργάνον NOUN nmod\n",
      "τὴν τὴν DET det\n",
      "ἐξ ἐκ ADP case\n",
      "αὐτῶν αὐτός PRON nmod\n",
      "ὠφέλειαν ὠφέλεια NOUN obj\n",
      "ἐροῦμεν ἐροῦμεν VERB ROOT\n",
      "ἀπὸ ἀπὸ ADP case\n",
      "τῶν ὁ DET det\n",
      "δακτύλων δάκτυλος NOUN obl\n",
      "αὖθις αὖθις ADV advmod\n",
      "ἀρξάμενοι ἄρχω VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ταῦτ Ταῦτ ADJ obj\n",
      "’ punc1 ADV discourse\n",
      "οὖν οὖν ADV discourse\n",
      "εἴς εἴς ADP case\n",
      "τε τε CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "παρόντα πάρειμι VERB obj\n",
      "καὶ καί CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "ἑξῆς ἑξῆς ADV amod\n",
      "ἅπαντα ἅπας ADJ amod\n",
      "λόγον λόγος NOUN conj\n",
      "οἷον οἷος ADV advmod\n",
      "ὑποθέσεις ὑπόθεσις NOUN obj\n",
      "τινὰς τίς DET det\n",
      "τῶν ὁ DET det\n",
      "ἀποδείξεων ἀποδείξις NOUN nmod\n",
      "λαμβάνοντες λαμβάνω VERB advcl\n",
      "ἐν ἐν ADP case\n",
      "ἑκάστῳ ἕκαστος ADJ obl\n",
      "τῶν ὁ DET det\n",
      "ὀργάνων ὄργανον NOUN nmod\n",
      "τὴν τὴν DET det\n",
      "ἐξ ἐκ ADP case\n",
      "αὐτῶν αὐτός PRON nmod\n",
      "ὠφέλειαν ὠφέλεια NOUN obj\n",
      "ἐροῦμεν λέγω VERB ROOT\n",
      "ἀπὸ ἀπὸ ADP case\n",
      "τῶν ὁ DET det\n",
      "δακτύλων δάκτυλος NOUN obl\n",
      "αὖθις αὖθις ADV advmod\n",
      "ἀρξάμενοι ἄρχω VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"training/transformer/assembled/model-best/\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ταῦτ Ταῦτ ADJ obj\n",
      "’ ’ ADV discourse\n",
      "οὖν οὖν ADV discourse\n",
      "εἴς εἴς ADP case\n",
      "τε τε CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "παρόντα παρόντα VERB obj\n",
      "καὶ καί CCONJ cc\n",
      "τὸν τὸν DET det\n",
      "ἑξῆς ἑξῆς ADV amod\n",
      "ἅπαντα ἅπαντα ADJ amod\n",
      "λόγον λόγον NOUN conj\n",
      "οἷον οἷον ADV advmod\n",
      "ὑποθέσεις ὑποθέσεις NOUN obj\n",
      "τινὰς τινὰς DET det\n",
      "τῶν τῶν DET det\n",
      "ἀποδείξεων ἀποδείξεον NOUN nmod\n",
      "λαμβάνοντες λαμβάνοντες VERB advcl\n",
      "ἐν ἐν ADP case\n",
      "ἑκάστῳ ἑκάστῳ ADJ obl\n",
      "τῶν τῶν DET det\n",
      "ὀργάνων ὀργάνον NOUN nmod\n",
      "τὴν τὴν DET det\n",
      "ἐξ ἐξ ADP case\n",
      "αὐτῶν αὐτῶν PRON nmod\n",
      "ὠφέλειαν ὠφέλειαν NOUN obj\n",
      "ἐροῦμεν ἐροῦμεν VERB ROOT\n",
      "ἀπὸ ἀπὸ ADP case\n",
      "τῶν τός DET det\n",
      "δακτύλων δακτύλ NOUN obl\n",
      "αὖθις αὖθις ADV advmod\n",
      "ἀρξάμενοι ἀρξάμενοι VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"training/NER/model-best/\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
