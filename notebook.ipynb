{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m‚úî Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m‚úî Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: grc\n",
      "Training pipeline: transformer, trainable_lemmatizer\n",
      "3521 training docs\n",
      "627 evaluation docs\n",
      "\u001b[38;5;2m‚úî No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 766797 total word(s) in the data (91726 unique)\u001b[0m\n",
      "\u001b[38;5;3m‚ö† 14 misaligned tokens in the training data\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "============================ Trainable Lemmatizer ============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 38837 lemmatizer trees generated from training data\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 13746 lemmatizer trees generated from dev data\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 4387 lemmatizer trees (31.9% of dev trees) were found exclusively in\n",
      "the dev data.\u001b[0m\n",
      "\u001b[38;5;3m‚ö† 1 training docs with 0 or 1 unique lemmas.\u001b[0m\n",
      "\u001b[38;5;2m‚úî All training docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;2m‚úî All dev docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 81 training docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 14 dev docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m‚úî 5 checks passed\u001b[0m\n",
      "\u001b[38;5;3m‚ö† 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data configs/lemmatizer_trf.cfg --paths.train corpus/train/lemma_train --paths.dev corpus/dev/lemma_dev/ --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m‚úî Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m‚úî Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: grc\n",
      "Training pipeline: transformer, trainable_lemmatizer\n",
      "3076 training docs\n",
      "627 evaluation docs\n",
      "\u001b[38;5;2m‚úî No overlap between training and evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 424100 total word(s) in the data (67136 unique)\u001b[0m\n",
      "\u001b[38;5;3m‚ö† 14 misaligned tokens in the training data\u001b[0m\n",
      "10 most common words: ',' (24151), '.' (16237), 'Œ∫Œ±·Ω∂' (11861), 'Œ¥·Ω≤' (10087), '¬∑'\n",
      "(9068), 'Œ¥ º' (5213), 'œÑŒµ' (3825), 'œÑ·Ω¥ŒΩ' (3540), 'œÑ·ø∂ŒΩ' (3489), 'Œº·Ω≤ŒΩ' (3418)\n",
      "\u001b[38;5;4m‚Ñπ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "============================ Trainable Lemmatizer ============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 29126 lemmatizer trees generated from training data\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 13746 lemmatizer trees generated from dev data\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 6016 lemmatizer trees (43.8% of dev trees) were found exclusively in\n",
      "the dev data.\u001b[0m\n",
      "\u001b[38;5;3m‚ö† 1 training docs with 0 or 1 unique lemmas.\u001b[0m\n",
      "\u001b[38;5;2m‚úî All training docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;2m‚úî All dev docs have lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 81 training docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ 14 dev docs with partial lemma annotations.\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m‚úî 5 checks passed\u001b[0m\n",
      "\u001b[38;5;3m‚ö† 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data configs/lemmatizer_trf.cfg --paths.train corpus/train/lemma_train/Merged_lemma_train_dataset.spacy --paths.dev corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy --nlp.lang=grc --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-24 18:35:01,264] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'nlp.lang']\n",
      "\u001b[38;5;4m‚Ñπ Saving to output directory: training/lemmatizer_mostfiles\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-03-24 18:35:01,528] [INFO] Set up nlp object from config\n",
      "[2023-03-24 18:35:02,389] [DEBUG] Loading corpus from path: corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy\n",
      "[2023-03-24 18:35:02,390] [DEBUG] Loading corpus from path: corpus/train/lemma_train/Merged_lemma_train_dataset.spacy\n",
      "[2023-03-24 18:35:02,390] [INFO] Pipeline: ['transformer', 'trainable_lemmatizer']\n",
      "[2023-03-24 18:35:02,393] [INFO] Created vocabulary\n",
      "[2023-03-24 18:35:02,394] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-03-24 18:35:33,488] [INFO] Initialized pipeline components: ['transformer', 'trainable_lemmatizer']\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2023-03-24 18:35:33,495] [DEBUG] Loading corpus from path: corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy\n",
      "[2023-03-24 18:35:33,496] [DEBUG] Loading corpus from path: corpus/train/lemma_train/Merged_lemma_train_dataset.spacy\n",
      "[2023-03-24 18:35:33,524] [DEBUG] Removed existing output directory: training/lemmatizer_mostfiles/model-best\n",
      "[2023-03-24 18:35:33,553] [DEBUG] Removed existing output directory: training/lemmatizer_mostfiles/model-last\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['transformer', 'trainable_lemmatizer']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.0\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matlomy\u001b[0m (\u001b[33matlomy-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/Projects/Atlomy/git/greCy_ATLOMY/wandb/run-20230324_183535-9pb0ycl5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-rain-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/9pb0ycl5\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS TRAIN...  LEMMA_ACC  SCORE \n",
      "---  ------  -------------  -------------  ---------  ------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./corpus)... Done. 0.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (TransientError), entering retry loop.\n",
      "  0       0           0.00         611.98      27.28    0.27\n",
      "  0     200         785.70      335479.67      27.28    0.27\n",
      "  1     400        3131.59      309447.37      27.89    0.28\n",
      "  2     600        2396.80      266858.59      47.62    0.48\n",
      "  3     800        2968.13      203982.71      64.38    0.64\n",
      "  3    1000        2989.96      163783.77      70.94    0.71\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      "  4    1200        2979.45      140548.53      75.30    0.75\n",
      "  5    1400        2941.29      125736.81      78.08    0.78\n",
      "  6    1600        2887.92      113730.11      79.87    0.80\n",
      "  7    1800        2838.53      105272.75      81.15    0.81\n",
      "  7    2000        2714.63       96646.33      82.18    0.82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      "  8    2200        2569.28       90265.15      83.27    0.83\n",
      "  9    2400        2479.26       83534.61      83.97    0.84\n",
      " 10    2600        2460.38       81399.65      84.80    0.85\n",
      " 11    2800        2239.54       74437.65      85.19    0.85\n",
      " 11    3000        2254.12       72783.90      85.66    0.86\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.8s\n",
      " 12    3200        2086.64       67580.11      86.14    0.86\n",
      " 13    3400        2005.26       64034.13      86.36    0.86\n",
      " 14    3600        1900.41       60855.87      86.87    0.87\n",
      " 15    3800        1823.96       57600.11      87.13    0.87\n",
      " 15    4000        1792.33       55737.57      87.38    0.87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.8s\n",
      " 16    4200        1626.62       51289.12      87.54    0.88\n",
      " 17    4400        1611.43       49497.09      87.90    0.88\n",
      " 18    4600        1575.40       47886.65      87.85    0.88\n",
      " 19    4800        1521.01       45716.86      88.24    0.88\n",
      " 19    5000        1417.75       43179.03      88.26    0.88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 20    5200        1339.51       40572.88      88.59    0.89\n",
      " 21    5400        1340.72       39432.48      88.30    0.88\n",
      " 22    5600        1292.30       37628.82      88.66    0.89\n",
      " 23    5800        1241.39       35662.49      88.89    0.89\n",
      " 23    6000        1228.62       35203.51      88.81    0.89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 24    6200        1131.99       32439.27      89.07    0.89\n",
      " 25    6400        1147.16       31562.58      89.20    0.89\n",
      " 26    6600        1058.11       29490.86      89.34    0.89\n",
      " 27    6800        1063.60       28889.46      89.49    0.89\n",
      " 27    7000        1011.20       27285.23      89.47    0.89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 28    7200         932.31       25100.01      89.61    0.90\n",
      " 29    7400         931.73       24729.11      89.67    0.90\n",
      " 30    7600         894.06       23112.60      89.78    0.90\n",
      " 31    7800         883.21       22382.49      89.73    0.90\n",
      " 31    8000         847.82       21445.57      89.81    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 32    8200         754.46       19131.40      89.90    0.90\n",
      " 33    8400         786.49       18982.00      89.93    0.90\n",
      " 34    8600         771.82       18152.33      89.98    0.90\n",
      " 35    8800         660.15       15952.14      89.99    0.90\n",
      " 35    9000         719.96       16241.44      89.96    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 36    9200         639.47       14397.99      89.96    0.90\n",
      " 37    9400         596.99       13240.29      90.00    0.90\n",
      " 38    9600         597.18       12865.63      89.99    0.90\n",
      " 39    9800         567.99       11764.96      90.01    0.90\n",
      " 39   10000         590.30       11585.60      90.10    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 40   10200         510.61       10010.97      90.06    0.90\n",
      " 41   10400         514.35        9700.62      90.11    0.90\n",
      " 42   10600         479.16        8718.84      90.10    0.90\n",
      " 43   10800         479.16        8578.90      90.10    0.90\n",
      " 43   11000         440.52        7633.05      90.13    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 44   11200         394.82        6732.12      90.07    0.90\n",
      " 45   11400         419.29        6801.84      90.13    0.90\n",
      " 46   11600         398.12        6144.57      90.12    0.90\n",
      " 47   11800         353.92        5594.47      90.09    0.90\n",
      " 47   12000         369.49        5446.80      90.16    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 48   12200         338.93        4862.72      90.14    0.90\n",
      " 49   12400         341.88        4865.09      90.13    0.90\n",
      " 50   12600         280.04        4105.30      90.16    0.90\n",
      " 51   12800         302.26        4060.06      90.14    0.90\n",
      " 51   13000         270.57        3643.09      90.17    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 52   13200         250.55        3360.69      90.18    0.90\n",
      " 53   13400         224.95        3017.44      90.14    0.90\n",
      " 54   13600         220.97        2919.56      90.16    0.90\n",
      " 55   13800         236.93        2907.73      90.17    0.90\n",
      " 55   14000         243.43        2837.16      90.12    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 56   14200         198.17        2422.59      90.15    0.90\n",
      " 57   14400         199.26        2387.28      90.20    0.90\n",
      " 58   14600         181.00        2119.16      90.17    0.90\n",
      " 59   14800         176.29        2107.98      90.17    0.90\n",
      " 59   15000         187.84        2085.31      90.17    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 60   15200         155.13        1762.95      90.13    0.90\n",
      " 61   15400         160.87        1806.35      90.16    0.90\n",
      " 62   15600         152.11        1703.90      90.18    0.90\n",
      " 63   15800         150.20        1633.72      90.17    0.90\n",
      " 63   16000         132.57        1509.60      90.16    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 64   16200         124.51        1352.16      90.17    0.90\n",
      " 65   16400         142.03        1443.05      90.17    0.90\n",
      " 66   16600         117.70        1261.16      90.19    0.90\n",
      " 67   16800         115.58        1225.54      90.18    0.90\n",
      " 67   17000         116.13        1213.09      90.19    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 68   17200         108.77        1139.58      90.22    0.90\n",
      " 69   17400          97.47        1040.67      90.18    0.90\n",
      " 70   17600         104.29        1139.34      90.19    0.90\n",
      " 71   17800          93.25         937.00      90.20    0.90\n",
      " 71   18000          85.87         943.54      90.22    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      " 72   18200          94.53         978.55      90.21    0.90\n",
      " 73   18400          85.19         882.88      90.24    0.90\n",
      " 74   18600          88.11         930.85      90.20    0.90\n",
      " 75   18800          89.31         883.24      90.23    0.90\n",
      " 75   19000          86.97         874.20      90.21    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.8s\n",
      " 76   19200          83.79         844.86      90.22    0.90\n",
      " 77   19400          79.59         795.40      90.22    0.90\n",
      " 78   19600          75.48         819.06      90.22    0.90\n",
      " 79   19800          71.39         761.54      90.22    0.90\n",
      " 79   20000          81.61         825.37      90.22    0.90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/model-last)... Done. 0.7s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 lemma_acc ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_trainable_lemmatizer ‚ñÅ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_transformer ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     score ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     speed ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñà‚ñá‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 token_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_f ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_p ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_r ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 lemma_acc 0.90222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_trainable_lemmatizer 825.36667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          loss_transformer 81.61107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     score 0.90222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     speed 17238.50699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_f 0.98861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_p 0.98487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   token_r 0.99238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mgenial-rain-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/9pb0ycl5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 386 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230324_183535-9pb0ycl5/logs\u001b[0m\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "training/lemmatizer_mostfiles/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train configs/lemmatizer_trf.cfg --output training/lemmatizer_mostfiles --gpu-id 0 --paths.train corpus/train/lemma_train/Merged_lemma_train_dataset.spacy --paths.dev corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy --nlp.lang=grc --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "LEMMA   92.30 \n",
      "SPEED   4265  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark accuracy /root/Projects/Atlomy/git/greCy_ATLOMY/training/lemmatizer_mostfiles/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/grc_proiel-ud-test.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Saving to output directory:\n",
      "training/lemmatizer_mostfiles/assembled\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-03-24 22:38:15,854] [INFO] Set up nlp object from config\n",
      "[2023-03-24 22:38:16,589] [INFO] Pipeline: ['transformer', 'morphologizer', 'tagger', 'parser', 'trainable_lemmatizer', 'attribute_ruler', 'sentencizer']\n",
      "[2023-03-24 22:38:16,595] [INFO] Created vocabulary\n",
      "[2023-03-24 22:38:16,595] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-03-24 22:38:28,008] [INFO] Initialized pipeline components: ['transformer', 'morphologizer', 'tagger', 'parser', 'attribute_ruler', 'sentencizer']\n",
      "\u001b[38;5;2m‚úî Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Pipeline: ['transformer', 'morphologizer', 'tagger', 'parser',\n",
      "'trainable_lemmatizer', 'attribute_ruler', 'sentencizer']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Frozen components: ['trainable_lemmatizer']\u001b[0m\n",
      "\u001b[38;5;4m‚Ñπ Initial learn rate: 0.0\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matlomy\u001b[0m (\u001b[33matlomy-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/logging/__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "UnicodeEncodeError: 'ascii' codec can't encode character '\\xb7' in position 1275: ordinal not in range(128)\n",
      "Call stack:\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/cli/_util.py\", line 74, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/cli/train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/cli/train.py\", line 75, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy/training/loop.py\", line 105, in train\n",
      "    log_step, finalize_logger = train_logger(nlp, stdout, stderr)\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/spacy_loggers/wandb.py\", line 157, in setup_logger\n",
      "    run = wandb.init(\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1144, in init\n",
      "    run = wi.init()\n",
      "  File \"/root/anaconda3/envs/nlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 552, in init\n",
      "    logger.info(\n",
      "Message: \"wandb.init called with sweep_config: {}\\nconfig: {'components': {'attribute_ruler': {'factory': 'attribute_ruler', 'scorer': {'@scorers': 'spacy.attribute_ruler_scorer.v1'}, 'validate': False}, 'morphologizer': {'factory': 'morphologizer', 'extend': False, 'overwrite': True, 'scorer': {'@scorers': 'spacy.morphologizer_scorer.v1'}, 'model': {'@architectures': 'spacy.Tagger.v2', 'no': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy-transformers.TransformerListener.v1', 'grad_factor': 1.0, 'pooling': {'@layers': 'reduce_mean.v1'}, 'upstream': 'transformer'}}}, 'parser': {'factory': 'parser', 'learn_tokens': False, 'min_action_freq': 30, 'moves': None, 'scorer': {'@scorers': 'spacy.parser_scorer.v1'}, 'update_with_oracle_cut_size': 100, 'model': {'@architectures': 'spacy.TransitionBasedParser.v2', 'state_type': 'parser', 'extra_state_tokens': False, 'hidden_width': 64, 'maxout_pieces': 2, 'use_upper': False, 'no': None, 'tok2vec': {'@architectures': 'spacy-transformers.TransformerListener.v1', 'grad_factor': 1.0, 'pooling': {'@layers': 'reduce_mean.v1'}, 'upstream': 'transformer'}}}, 'sentencizer': {'factory': 'sentencizer', 'overwrite': False, 'punct_chars': ['.', ';', '¬∑'], 'scorer': {'@scorers': 'spacy.senter_scorer.v1'}}, 'tagger': {'factory': 'tagger', 'neg_prefix': '!', 'overwrite': False, 'scorer': {'@scorers': 'spacy.tagger_scorer.v1'}, 'model': {'@architectures': 'spacy.Tagger.v2', 'no': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy-transformers.TransformerListener.v1', 'grad_factor': 1.0, 'pooling': {'@layers': 'reduce_mean.v1'}, 'upstream': 'transformer'}}}, 'trainable_lemmatizer': {'factory': 'trainable_lemmatizer', 'backoff': 'orth', 'min_tree_freq': 1, 'overwrite': False, 'scorer': {'@scorers': 'spacy.lemmatizer_scorer.v1'}, 'top_k': 6, 'model': {'@architectures': 'spacy.Tagger.v2', 'no': None, 'normalize': False, 'tok2vec': {'@architectures': 'spacy-transformers.Tok2VecTransformer.v3', 'name': 'Jacobo/aristoBERTo', 'mixed_precision': False, 'pooling': {'@layers': 'reduce_mean.v1'}, 'grad_factor': 1.0, 'get_spans': {'@span_getters': 'spacy-transformers.strided_spans.v1', 'window': 128, 'stride': 96}, 'tokenizer_config': {'use_fast': True}}}}, 'transformer': {'factory': 'transformer', 'max_batch_items': 4096, 'set_extra_annotations': {'@annotation_setters': 'spacy-transformers.null_annotation_setter.v1'}, 'model': {'@architectures': 'spacy-transformers.TransformerModel.v3', 'name': 'Jacobo/aristoBERTo', 'mixed_precision': False, 'get_spans': {'@span_getters': 'spacy-transformers.strided_spans.v1', 'window': 128, 'stride': 96}, 'tokenizer_config': {'use_fast': True}}}}, 'corpora': {'dev': {'@readers': 'spacy.Corpus.v1', 'max_length': 0, 'gold_preproc': False, 'limit': 0, 'augmenter': None}, 'train': {'@readers': 'spacy.Corpus.v1', 'max_length': 0, 'gold_preproc': False, 'limit': 0, 'augmenter': None}}, 'initialize': {'vectors': None, 'init_tok2vec': None, 'vocab_data': None, 'lookups': None, 'before_init': None, 'after_init': None, 'components': {'attribute_ruler': {'patterns': {'@readers': 'srsly.read_json.v1', 'path': 'data/augments/attribute_ruler_patterns.json'}}, 'parser': {'labels': {'@readers': 'spacy.read_labels.v1', 'path': 'data/labels/parser.json', 'require': False}}, 'tagger': {'labels': {'@readers': 'spacy.read_labels.v1', 'path': 'data/labels/tagger.json', 'require': False}}}}, 'nlp': {'lang': 'grc', 'pipeline': ['transformer', 'morphologizer', 'tagger', 'parser', 'trainable_lemmatizer', 'attribute_ruler', 'sentencizer'], 'batch_size': 128, 'disabled': ['trainable_lemmatizer'], 'before_creation': None, 'after_creation': None, 'after_pipeline_creation': None, 'tokenizer': {'@tokenizers': 'spacy.Tokenizer.v1'}}, 'paths': {'vectors': None, 'init_tok2vec': None}, 'system': {'gpu_allocator': 'pytorch', 'seed': 0}, 'training': {'accumulate_gradient': 3, 'dev_corpus': 'corpora.dev', 'train_corpus': 'corpora.train', 'seed': 0, 'gpu_allocator': 'pytorch', 'dropout': 0.1, 'patience': 5000, 'max_epochs': 0, 'max_steps': 20000, 'eval_frequency': 200, 'frozen_components': ['trainable_lemmatizer'], 'annotating_components': [], 'before_to_disk': None, 'before_update': None, 'batcher': {'@batchers': 'spacy.batch_by_padded.v1', 'discard_oversize': True, 'size': 2000, 'buffer': 256, 'get_length': None}, 'logger': {'@loggers': 'spacy.WandbLogger.v3', 'project_name': 'Lemmatizer (Ancient Greek)', 'remove_config_values': ['paths.train', 'paths.dev', 'corpora.train.path', 'corpora.dev.path'], 'log_dataset_dir': './corpus', 'model_log_interval': 1000, 'entity': None, 'run_name': 'greCy_mostfiles_full_pipeline_pytorch2_0.2'}, 'optimizer': {'@optimizers': 'Adam.v1', 'beta1': 0.9, 'beta2': 0.999, 'l2_is_weight_decay': True, 'l2': 0.01, 'grad_clip': 1.0, 'use_averages': True, 'eps': 1e-08, 'learn_rate': {'@schedules': 'warmup_linear.v1', 'warmup_steps': 250, 'total_steps': 20000, 'initial_rate': 5e-05}}, 'score_weights': {'pos_acc': 0.06, 'morph_acc': 0.06, 'morph_per_feat': None, 'tag_acc': 0.12, 'dep_uas': 0.06, 'dep_las': 0.06, 'dep_las_per_type': None, 'sents_p': None, 'sents_r': None, 'sents_f': 0.01, 'lemma_acc': 0.63}}}\"\n",
      "Arguments: ()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/Projects/Atlomy/git/greCy_ATLOMY/wandb/run-20230324_223829-oco0a1pd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgood-serenity-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/oco0a1pd\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS MORPH...  LOSS TAGGER  LOSS PARSER  POS_ACC  MORPH_ACC  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  LEMMA_ACC  SCORE \n",
      "---  ------  -------------  -------------  -----------  -----------  -------  ---------  -------  -------  -------  -------  ---------  ------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./corpus)... Done. 0.2s\n",
      "  0       0         766.75         428.58       410.35       787.48     4.53       2.28     5.11     7.46     7.46     0.14      92.20    0.60\n",
      "  1     200      542178.94      355306.31    320113.55    415712.30    43.61      27.42    54.70    46.97    31.43    11.53      92.20    0.74\n",
      "  3     400      292994.76      302493.78    109981.13    251310.83    93.78      46.86    95.50    69.02    59.95    42.19      92.20    0.86\n",
      "  5     600      246842.02      174491.84     17436.99    198574.38    96.84      77.26    97.74    74.77    67.76    50.83      92.20    0.89\n",
      "  7     800      214566.99       87736.13      8158.59    166454.00    97.60      87.01    98.05    78.60    72.38    59.38      92.20    0.91\n",
      "  9    1000      189238.34       57317.92      5228.10    143611.61    97.88      89.72    98.22    80.36    74.74    64.58      92.20    0.91\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 2.1s\n",
      " 11    1200      168711.72       43326.52      3711.72    127073.00    98.18      91.24    98.27    81.20    76.24    60.44      92.20    0.91\n",
      " 13    1400      153416.39       36131.99      2863.55    114584.41    98.15      91.85    98.24    81.94    77.12    66.99      92.20    0.91\n",
      " 15    1600      136258.07       29993.31      2110.94    100863.52    98.16      92.40    98.37    82.76    78.11    65.70      92.20    0.92\n",
      " 17    1800      124426.19       26158.38      1777.72     92941.02    98.22      92.81    98.35    83.39    78.80    69.05      92.20    0.92\n",
      " 19    2000      110306.39       22516.33      1343.00     82765.95    98.23      93.17    98.35    83.69    79.40    68.25      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.6s\n",
      " 21    2200      100165.39       19640.17      1081.03     75449.58    98.20      93.25    98.32    84.04    79.83    69.53      92.20    0.92\n",
      " 23    2400       94073.19       17138.90       948.16     70132.60    98.21      93.41    98.33    84.07    80.00    68.38      92.20    0.92\n",
      " 25    2600       85840.48       15354.38       783.25     64698.83    98.33      93.53    98.41    84.27    80.30    70.44      92.20    0.92\n",
      " 27    2800       80529.72       13163.55       685.37     60824.20    98.27      93.58    98.43    84.15    80.07    70.56      92.20    0.92\n",
      " 28    3000       73140.68       11562.50       594.46     55825.58    98.26      93.66    98.43    84.13    80.29    69.57      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.3s\n",
      " 30    3200       70834.00       10098.06       482.00     53799.06    98.25      93.90    98.40    84.13    80.41    68.08      92.20    0.92\n",
      " 32    3400       63210.01        8704.03       502.63     48817.55    98.34      93.85    98.45    84.30    80.71    69.36      92.20    0.92\n",
      " 34    3600       62345.78        7779.15       408.13     47924.59    98.27      93.93    98.36    84.35    80.64    70.96      92.20    0.92\n",
      " 36    3800       62168.89        6636.00       351.55     46447.20    98.31      93.91    98.43    84.62    80.80    69.85      92.20    0.92\n",
      " 38    4000       51469.50        5730.68       322.11     41577.98    98.34      94.03    98.43    84.77    80.95    70.91      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      " 40    4200       48019.40        5165.03       286.10     39788.38    98.29      93.81    98.36    84.41    80.71    71.31      92.20    0.92\n",
      " 42    4400       44606.33        4396.22       299.75     37716.42    98.32      94.04    98.41    84.82    81.17    70.31      92.20    0.92\n",
      " 44    4600       43628.47        3810.78       239.08     37469.19    98.36      93.96    98.48    84.85    81.15    71.26      92.20    0.92\n",
      " 46    4800       40818.94        3263.46       216.93     35443.08    98.37      94.07    98.40    85.00    81.42    71.14      92.20    0.92\n",
      " 48    5000       40042.72        3018.53       261.87     34276.22    98.29      94.04    98.43    84.78    81.23    70.35      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      " 50    5200       36778.06        2561.62       159.30     33317.08    98.25      93.93    98.37    84.54    80.96    71.18      92.20    0.92\n",
      " 52    5400       34748.03        2268.40       161.52     32108.29    98.30      93.99    98.45    84.69    81.18    69.82      92.20    0.92\n",
      " 54    5600       32562.66        1980.55       138.99     31267.17    98.34      94.04    98.41    85.01    81.38    70.24      92.20    0.92\n",
      " 56    5800       31986.01        1765.25       155.15     30367.12    98.33      94.08    98.43    85.15    81.64    72.06      92.20    0.92\n",
      " 58    6000       31708.88        1526.98       154.48     30075.53    98.35      94.13    98.41    84.68    81.15    70.79      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.5s\n",
      " 60    6200       29142.48        1318.83       126.04     29170.60    98.33      94.28    98.41    84.75    81.23    71.10      92.20    0.92\n",
      " 62    6400       29033.94        1177.53       139.53     29037.55    98.36      94.10    98.43    85.09    81.64    72.10      92.20    0.92\n",
      " 64    6600       27420.84        1059.83       129.05     28144.00    98.31      94.04    98.43    85.20    81.70    70.96      92.20    0.92\n",
      " 66    6800       26898.10         919.69       114.87     27617.66    98.26      93.91    98.34    85.09    81.73    70.47      92.20    0.92\n",
      " 68    7000       25439.77         851.54        97.46     27293.56    98.34      94.15    98.40    85.14    81.57    70.46      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.4s\n",
      " 70    7200       24229.67         739.36        87.77     26890.37    98.34      94.15    98.43    85.13    81.61    70.74      92.20    0.92\n",
      " 71    7400       24146.20         713.25       127.24     26755.65    98.32      94.07    98.39    85.36    81.86    71.63      92.20    0.92\n",
      " 73    7600       24025.59         678.18        92.50     26999.47    98.33      93.95    98.41    85.17    81.72    70.87      92.20    0.92\n",
      " 75    7800       22631.46         570.75        78.36     26467.89    98.30      94.03    98.40    85.37    81.78    71.90      92.20    0.92\n",
      " 77    8000       22464.73         581.24        75.72     26423.18    98.33      93.99    98.43    85.20    81.71    71.35      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.6s\n",
      " 79    8200       21154.44         477.55        78.09     25483.14    98.32      93.89    98.43    85.09    81.63    71.12      92.20    0.92\n",
      " 81    8400       20794.14         479.59        80.51     25327.79    98.37      94.13    98.40    85.20    81.65    71.11      92.20    0.92\n",
      " 83    8600       19960.25         435.68        69.88     25150.12    98.37      93.94    98.40    84.90    81.40    70.75      92.20    0.92\n",
      " 85    8800       20333.37         430.08        76.31     25469.86    98.38      94.00    98.46    85.02    81.61    71.42      92.20    0.92\n",
      " 87    9000       19979.39         374.49        59.67     25193.06    98.37      94.04    98.45    85.34    81.92    71.74      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      " 89    9200       19186.29         367.52        59.61     24580.58    98.37      94.15    98.43    85.47    81.97    71.04      92.20    0.92\n",
      " 91    9400       18495.04         331.99        51.83     24975.71    98.41      94.06    98.48    85.23    81.80    71.81      92.20    0.92\n",
      " 93    9600       18091.83         315.00        72.66     24520.19    98.37      93.88    98.44    85.49    81.85    71.16      92.20    0.92\n",
      " 95    9800       18031.73         290.72        59.20     24518.61    98.38      93.92    98.48    85.45    81.93    72.00      92.20    0.92\n",
      " 97   10000       17655.37         279.53        51.38     24233.77    98.37      94.02    98.46    85.53    81.97    71.59      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 0.9s\n",
      " 99   10200       17263.77         262.76        39.84     24371.86    98.37      93.89    98.47    85.47    82.02    71.69      92.20    0.92\n",
      "101   10400       16992.15         244.23        38.16     24205.11    98.43      93.91    98.52    85.13    81.72    71.61      92.20    0.92\n",
      "103   10600       16505.01         257.34        58.71     23363.14    98.46      93.99    98.56    85.28    81.80    71.41      92.20    0.92\n",
      "105   10800       16312.29         210.53        30.79     24087.30    98.41      94.04    98.45    85.36    81.90    72.43      92.20    0.92\n",
      "107   11000       16122.31         205.18        29.54     23974.94    98.39      93.90    98.44    85.37    81.89    71.80      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 0.9s\n",
      "108   11200       15361.41         191.08        36.48     23631.91    98.39      93.95    98.48    85.46    82.07    72.48      92.20    0.92\n",
      "110   11400       15427.93         195.26        35.69     23811.38    98.40      94.11    98.45    85.51    82.08    72.25      92.20    0.92\n",
      "112   11600       15281.70         177.03        31.73     23840.92    98.44      94.10    98.50    85.71    82.24    72.11      92.20    0.92\n",
      "114   11800       15098.48         147.73        40.87     23464.94    98.40      94.15    98.47    85.64    82.22    72.89      92.20    0.92\n",
      "116   12000       14696.03         153.36        44.89     23204.80    98.44      93.96    98.51    85.38    82.00    71.85      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      "118   12200       14849.60         146.61        25.15     23631.27    98.45      94.12    98.49    85.80    82.43    73.62      92.20    0.92\n",
      "120   12400       14289.57         155.06        28.81     23404.40    98.43      93.89    98.50    85.65    82.22    72.30      92.20    0.92\n",
      "122   12600       13864.79         130.98        29.05     23061.04    98.43      94.07    98.52    85.52    82.09    73.17      92.20    0.92\n",
      "124   12800       13528.43         104.62        25.37     23155.34    98.47      94.12    98.56    85.69    82.32    72.32      92.20    0.92\n",
      "126   13000       13411.33         132.40        30.79     23318.83    98.42      94.02    98.49    85.68    82.24    72.06      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.1s\n",
      "128   13200       13438.95          95.43        17.13     23411.82    98.43      94.06    98.55    85.41    82.05    72.48      92.20    0.92\n",
      "130   13400       13273.93         114.05        20.82     23090.52    98.41      94.02    98.55    85.61    82.24    72.62      92.20    0.92\n",
      "132   13600       12661.58         111.70        16.19     22822.93    98.45      94.06    98.51    85.48    82.08    71.67      92.20    0.92\n",
      "134   13800       12358.25         105.94        20.77     22541.72    98.44      94.07    98.53    85.49    82.11    72.13      92.20    0.92\n",
      "136   14000       12668.44          97.03        15.25     23024.86    98.43      94.04    98.51    85.53    82.12    71.15      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.2s\n",
      "138   14200       12493.08          79.39        18.68     22655.87    98.39      93.98    98.45    85.54    82.16    71.16      92.20    0.92\n",
      "140   14400       12259.27         105.06        16.85     22834.81    98.45      93.97    98.50    85.75    82.31    72.75      92.20    0.92\n",
      "142   14600       12145.74          83.66        21.45     22855.40    98.43      94.06    98.49    85.69    82.24    72.38      92.20    0.92\n",
      "144   14800       11963.96          92.52        11.60     22809.51    98.41      94.04    98.45    85.65    82.25    72.09      92.20    0.92\n",
      "145   15000       11850.36          65.19        19.87     22729.92    98.39      94.00    98.45    85.62    82.19    71.89      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.0s\n",
      "147   15200       11921.50          78.62        21.60     22518.23    98.45      94.07    98.50    85.68    82.28    72.20      92.20    0.92\n",
      "149   15400       11540.69          65.48        20.82     22568.14    98.39      93.92    98.48    85.61    82.19    72.83      92.20    0.92\n",
      "151   15600       11723.51          74.24        17.93     22803.69    98.43      93.96    98.51    85.64    82.18    72.96      92.20    0.92\n",
      "153   15800       11380.61          64.37         9.56     22500.97    98.42      94.00    98.50    85.58    82.25    72.67      92.20    0.92\n",
      "155   16000       11048.91          54.25         9.56     22323.90    98.41      93.96    98.51    85.47    82.12    72.45      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 1.7s\n",
      "157   16200       11129.87          56.47        13.77     22845.78    98.41      94.05    98.48    85.45    82.13    72.33      92.20    0.92\n",
      "159   16400       11065.60          54.66        13.91     22573.81    98.39      94.06    98.48    85.58    82.17    72.88      92.20    0.92\n",
      "161   16600       11052.11          55.25        22.73     22415.48    98.40      94.12    98.45    85.54    82.22    72.00      92.20    0.92\n",
      "163   16800       11108.62          53.52         6.13     22705.11    98.43      94.07    98.50    85.46    82.14    72.40      92.20    0.92\n",
      "165   17000       10502.80          55.73        13.88     22007.97    98.41      94.08    98.48    85.67    82.25    72.10      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./training/lemmatizer_mostfiles/assembled/model-last)... Done. 2.4s\n",
      "167   17200       10451.96          57.42        10.24     22215.94    98.41      94.03    98.48    85.53    82.09    72.00      92.20    0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 16416.046 MB of 16416.062 MB uploaded (8470.896 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_las ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_uas ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lemma_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_morphologizer ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_parser ‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_tagger ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_transformer ‚ñÅ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          morph_acc ‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_f ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_p ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_r ‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            pos_acc ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              score ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_f ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_p ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_r ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              speed ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñà‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            tag_acc ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_f ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_p ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_r ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_las 0.82091\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            dep_uas 0.85526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          lemma_acc 0.92199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss_morphologizer 57.4178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_parser 22215.9397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_tagger 10.23548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_transformer 10451.9632\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          morph_acc 0.9403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_f 0.98198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_p 0.98205\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      morph_micro_r 0.98191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            pos_acc 0.9841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              score 0.92226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_f 0.72004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_p 0.70384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            sents_r 0.737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              speed 8289.53275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            tag_acc 0.98476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          token_acc 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_f 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_p 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            token_r 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mgood-serenity-26\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/atlomy-nlp/Lemmatizer%20%28Ancient%20Greek%29/runs/oco0a1pd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 374 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230324_223829-oco0a1pd/logs\u001b[0m\n",
      "\u001b[38;5;2m‚úî Saved pipeline to output directory\u001b[0m\n",
      "training/lemmatizer_mostfiles/assembled/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train configs/transformer.cfg --output training/lemmatizer_mostfiles/assembled --gpu-id 0 --paths.train corpus/train/grc_proiel-ud-train.spacy --paths.dev corpus/dev/grc_proiel-ud-dev.spacy --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      98.25 \n",
      "POS      98.16 \n",
      "MORPH    93.98 \n",
      "LEMMA    92.30 \n",
      "UAS      84.93 \n",
      "LAS      81.33 \n",
      "SENT P   69.30 \n",
      "SENT R   69.44 \n",
      "SENT F   69.37 \n",
      "SPEED    3917  \n",
      "\n",
      "\u001b[1m\n",
      "============================== MORPH (per feat) ==============================\u001b[0m\n",
      "\n",
      "                P        R        F\n",
      "Case        98.88    98.80    98.84\n",
      "Gender      94.03    94.39    94.21\n",
      "Number      99.56    99.44    99.50\n",
      "Aspect      98.46    98.39    98.42\n",
      "Mood        99.01    98.72    98.86\n",
      "Person      98.77    98.02    98.40\n",
      "Tense       98.54    98.43    98.48\n",
      "VerbForm    99.63    99.49    99.56\n",
      "Voice       97.84    97.74    97.79\n",
      "PronType    98.52    98.03    98.27\n",
      "Degree      95.70    96.20    95.95\n",
      "Definite    99.33    99.39    99.36\n",
      "Reflex     100.00   100.00   100.00\n",
      "Polarity    99.54    98.64    99.09\n",
      "Poss       100.00    92.31    96.00\n",
      "\n",
      "\u001b[1m\n",
      "=============================== LAS (per type) ===============================\u001b[0m\n",
      "\n",
      "                  P       R       F\n",
      "iobj          77.13   78.72   77.92\n",
      "root          86.08   86.25   86.16\n",
      "nsubj         81.24   80.61   80.92\n",
      "advmod        76.11   71.67   73.82\n",
      "advcl         72.93   75.95   74.41\n",
      "ccomp         68.71   62.35   65.37\n",
      "discourse     84.50   84.73   84.62\n",
      "obj           86.46   87.32   86.89\n",
      "det           94.67   94.34   94.51\n",
      "nmod          78.04   76.83   77.43\n",
      "cop           81.86   83.02   82.44\n",
      "appos         55.65   39.02   45.88\n",
      "case          96.17   97.16   96.66\n",
      "acl           60.42   53.05   56.49\n",
      "mark          90.50   92.59   91.53\n",
      "obl           77.85   79.90   78.86\n",
      "nsubj:pass    73.08   80.00   76.38\n",
      "xcomp         69.23   71.25   70.23\n",
      "cc            73.16   72.87   73.02\n",
      "conj          67.37   66.49   66.93\n",
      "dislocated    30.00   10.71   15.79\n",
      "amod          81.13   79.63   80.37\n",
      "parataxis      0.00    0.00    0.00\n",
      "dep            0.00    0.00    0.00\n",
      "nummod        88.57   88.57   88.57\n",
      "fixed         85.71   75.00   80.00\n",
      "obl:agent     86.67   59.09   70.27\n",
      "orphan        25.00   20.00   22.22\n",
      "csubj:pass    75.00   42.86   54.55\n",
      "vocative      83.93   77.05   80.34\n",
      "aux            0.00    0.00    0.00\n",
      "aux:pass       0.00    0.00    0.00\n",
      "flat:name    100.00   85.71   92.31\n",
      "\n",
      "\u001b[38;5;2m‚úî Generated 200 parses as HTML\u001b[0m\n",
      "training/lemmatizer_mostfiles/assembled/model-best\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark accuracy training/lemmatizer_mostfiles/assembled/model-best corpus/test/grc_proiel-ud-test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path training/lemmatizer_mostfiles/assembled/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      -     \n",
      "POS      -     \n",
      "MORPH    -     \n",
      "LEMMA    -     \n",
      "UAS      -     \n",
      "LAS      -     \n",
      "SENT P   -     \n",
      "SENT R   -     \n",
      "SENT F   -     \n",
      "SPEED    802   \n",
      "\n",
      "\u001b[38;5;2m‚úî Generated 200 parses as HTML\u001b[0m\n",
      "/root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy evaluate /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "œÜŒª·Ω≥œà = \"œÜŒª·Ω≥œà\"\n",
    "#normalize texwor\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "œÜŒª·Ω≥œà = normalize_text(œÜŒª·Ω≥œà)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttest = normalize_text(\"Œ§Œ±·ø¶œÑ‚Äô Œø·ΩñŒΩ Œµ·º¥œÇ œÑŒµ œÑ·Ω∏ŒΩ œÄŒ±œÅœåŒΩœÑŒ± Œ∫Œ±·Ω∂ œÑ·Ω∏ŒΩ ·ºëŒæ·øÜœÇ ·ºÖœÄŒ±ŒΩœÑŒ± ŒªœåŒ≥ŒøŒΩ Œø·º∑ŒøŒΩ ·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ œÑŒπŒΩ·Ω∞œÇ œÑ·ø∂ŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ·ºêŒΩ ·ºëŒ∫Œ¨œÉœÑ·ø≥ œÑ·ø∂ŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ œÑ·Ω¥ŒΩ ·ºêŒæ Œ±·ΩêœÑ·ø∂ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·ºêœÅŒø·ø¶ŒºŒµŒΩ ·ºÄœÄ·Ω∏ œÑ·ø∂ŒΩ Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ±·ΩñŒ∏ŒπœÇ ·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ. ·ºêœÄŒµŒπŒ¥·Ω¥ Œ≥·Ω∞œÅ œÑ·Ω¥ŒΩ œÑ·ø∂ŒΩ ·ΩÄœÉœÑ·ø∂ŒΩ Œ∫Œ±œÑŒ±œÉŒ∫ŒµœÖ·Ω¥ŒΩ ·ºêŒΩ Œ±·ΩêœÑŒø·øñœÇ ·ºêœÄŒπœÑŒ∑Œ¥ŒµŒπŒøœÑŒ¨œÑŒ∑ŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩŒøŒπœÇ ·ºÄŒΩœÑŒπŒªŒ∑œÄœÑŒπŒ∫Œø·øñœÇ ·º° œÜœçœÉŒπœÇ ·ºêœÄŒøŒØŒ∑œÉŒµŒΩ, ·º¶ŒΩ Œ¥‚Äô ·ºÄŒºŒÆœáŒ±ŒΩŒøŒΩ Œ±·ΩêœÑŒø·øñœÇ œÑŒø·øñœÇ ·ΩÄœÉœÑŒø·øñœÇ Œø·ΩïœÑœâ Œ≥ŒµœéŒ¥ŒµœÉŒØ œÑŒµ Œ∫Œ±·Ω∂ ŒªŒπŒ∏œéŒ¥ŒµœÉŒπŒΩ Œø·ΩñœÉŒπ ŒºŒµœÑŒ±Œ¥Œø·ø¶ŒΩŒ±Œπ œÑ·øÜœÇ Œ∫Œ±Œ∏‚Äô ·ΩÅœÅŒº·Ω¥ŒΩ Œ∫ŒπŒΩŒÆœÉŒµœâœÇ, ·ºêŒæŒµ·ø¶œÅŒµŒΩ, ·ΩÖœÑ·ø≥ œÑœÅœåœÄ·ø≥ Œ¥Œπ‚Äô ·ºëœÑŒ≠œÅœâŒΩ Œ±·ΩêœÑ·Ω∞ Œ∫ŒπŒΩŒÆœÉŒµŒπ. œÑ·ø∂ŒΩ Œø·ΩñŒΩ Œ∫Œ±œÑ·Ω∞ œÑ·Ω∏ŒΩ œÄ·øÜœáœÖŒΩ ŒºœÖ·ø∂ŒΩ ·ºÄœÄŒøœÜœçœÉŒ±œÉŒ± œÑŒ≠ŒΩŒøŒΩœÑŒ±œÇ Œµ·ΩêŒ∏·Ω∫ œÑ·ø∂ŒΩ Œ¥Œ±Œ∫œÑœçŒªœâŒΩ ·º§Œ≥Œ±Œ≥ŒµŒΩ. ·ºÉ Œ≥·Ω∞œÅ Œø·º± œÄŒ±ŒªŒ±ŒπŒø·Ω∂ Œ∫Œ±ŒªŒø·ø¶œÉŒπ ŒΩŒµ·ø¶œÅŒ±, œÑŒ±œÖœÑ·Ω∂ œÑ·Ω∞ œÄœÅŒøœÜŒ±ŒΩ·øÜ, œÑ·Ω∞ Œ∫ŒπŒΩŒø·ø¶ŒΩœÑŒ± œÑŒø·Ω∫œÇ Œ¥Œ±Œ∫œÑœçŒªŒøœÖœÇ, Œø·º± œÑŒ≠ŒΩŒøŒΩœÑŒ≠œÇ Œµ·º∞œÉŒπŒΩ¬∑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ§Œ±œÖÕÇœÑ‚Äô ŒøœÖÃìÕÇŒΩ ŒµŒπÃìÃÅœÇ œÑŒµ œÑŒøÃÄŒΩ œÄŒ±œÅŒøÃÅŒΩœÑŒ± Œ∫Œ±ŒπÃÄ œÑŒøÃÄŒΩ ŒµÃîŒæŒ∑ÕÇœÇ Œ±ÃîÃÅœÄŒ±ŒΩœÑŒ± ŒªŒøÃÅŒ≥ŒøŒΩ ŒøŒπÃîÕÇŒøŒΩ œÖÃîœÄŒøŒ∏ŒµÃÅœÉŒµŒπœÇ œÑŒπŒΩŒ±ÃÄœÇ œÑœâÕÇŒΩ Œ±ÃìœÄŒøŒ¥ŒµŒπÃÅŒæŒµœâŒΩ ŒªŒ±ŒºŒ≤Œ±ÃÅŒΩŒøŒΩœÑŒµœÇ ŒµÃìŒΩ ŒµÃîŒ∫Œ±ÃÅœÉœÑœâÕÖ œÑœâÕÇŒΩ ŒøÃìœÅŒ≥Œ±ÃÅŒΩœâŒΩ œÑŒ∑ÃÄŒΩ ŒµÃìŒæ Œ±œÖÃìœÑœâÕÇŒΩ œâÃìœÜŒµÃÅŒªŒµŒπŒ±ŒΩ ŒµÃìœÅŒøœÖÕÇŒºŒµŒΩ Œ±ÃìœÄŒøÃÄ œÑœâÕÇŒΩ Œ¥Œ±Œ∫œÑœÖÃÅŒªœâŒΩ Œ±œÖÃìÕÇŒ∏ŒπœÇ Œ±ÃìœÅŒæŒ±ÃÅŒºŒµŒΩŒøŒπ.\n",
      "ŒµÃìœÄŒµŒπŒ¥Œ∑ÃÄ Œ≥Œ±ÃÄœÅ œÑŒ∑ÃÄŒΩ œÑœâÕÇŒΩ ŒøÃìœÉœÑœâÕÇŒΩ Œ∫Œ±œÑŒ±œÉŒ∫ŒµœÖŒ∑ÃÄŒΩ ŒµÃìŒΩ Œ±œÖÃìœÑŒøŒπÕÇœÇ ŒµÃìœÄŒπœÑŒ∑Œ¥ŒµŒπŒøœÑŒ±ÃÅœÑŒ∑ŒΩ ŒøÃìœÅŒ≥Œ±ÃÅŒΩŒøŒπœÇ Œ±ÃìŒΩœÑŒπŒªŒ∑œÄœÑŒπŒ∫ŒøŒπÕÇœÇ Œ∑Ãî œÜœÖÃÅœÉŒπœÇ ŒµÃìœÄŒøŒπÃÅŒ∑œÉŒµŒΩ, Œ∑ÃìÕÇŒΩ Œ¥‚Äô Œ±ÃìŒºŒ∑ÃÅœáŒ±ŒΩŒøŒΩ Œ±œÖÃìœÑŒøŒπÕÇœÇ œÑŒøŒπÕÇœÇ ŒøÃìœÉœÑŒøŒπÕÇœÇ ŒøœÖÃîÃÅœÑœâ Œ≥ŒµœâÃÅŒ¥ŒµœÉŒπÃÅ œÑŒµ Œ∫Œ±ŒπÃÄ ŒªŒπŒ∏œâÃÅŒ¥ŒµœÉŒπŒΩ ŒøœÖÃìÕÇœÉŒπ ŒºŒµœÑŒ±Œ¥ŒøœÖÕÇŒΩŒ±Œπ œÑŒ∑ÕÇœÇ Œ∫Œ±Œ∏‚Äô ŒøÃîœÅŒºŒ∑ÃÄŒΩ Œ∫ŒπŒΩŒ∑ÃÅœÉŒµœâœÇ, ŒµÃìŒæŒµœÖÕÇœÅŒµŒΩ\n",
      ", ŒøÃîÃÅœÑœâÕÖ œÑœÅŒøÃÅœÄœâÕÖ Œ¥Œπ‚Äô ŒµÃîœÑŒµÃÅœÅœâŒΩ Œ±œÖÃìœÑŒ±ÃÄ Œ∫ŒπŒΩŒ∑ÃÅœÉŒµŒπ\n",
      ".\n",
      "œÑœâÕÇŒΩ ŒøœÖÃìÕÇŒΩ Œ∫Œ±œÑŒ±ÃÄ œÑŒøÃÄŒΩ œÄŒ∑ÕÇœáœÖŒΩ ŒºœÖœâÕÇŒΩ Œ±ÃìœÄŒøœÜœÖÃÅœÉŒ±œÉŒ± œÑŒµÃÅŒΩŒøŒΩœÑŒ±œÇ ŒµœÖÃìŒ∏œÖÃÄ œÑœâÕÇŒΩ Œ¥Œ±Œ∫œÑœÖÃÅŒªœâŒΩ Œ∑ÃìÃÅŒ≥Œ±Œ≥ŒµŒΩ\n",
      ".\n",
      "Œ±ÃîÃÄ Œ≥Œ±ÃÄœÅ ŒøŒπÃî œÄŒ±ŒªŒ±ŒπŒøŒπÃÄ Œ∫Œ±ŒªŒøœÖÕÇœÉŒπ ŒΩŒµœÖÕÇœÅŒ±, œÑŒ±œÖœÑŒπÃÄ œÑŒ±ÃÄ œÄœÅŒøœÜŒ±ŒΩŒ∑ÕÇ, œÑŒ±ÃÄ Œ∫ŒπŒΩŒøœÖÕÇŒΩœÑŒ± œÑŒøœÖÃÄœÇ Œ¥Œ±Œ∫œÑœÖÃÅŒªŒøœÖœÇ, ŒøŒπÃî œÑŒµÃÅŒΩŒøŒΩœÑŒµÃÅœÇ ŒµŒπÃìœÉŒπŒΩ¬∑\n",
      "Œ§Œ±œÖÕÇœÑ ADJ obj  Œ§Œ±œÖÕÇœÑ\n",
      "‚Äô ADV discourse  punc1\n",
      "ŒøœÖÃìÕÇŒΩ ADV discourse  ŒøœÖÃìÕÇŒΩ\n",
      "ŒµŒπÃìÃÅœÇ ADP case  ŒµŒπÃìÃÅœÇ\n",
      "œÑŒµ CCONJ cc  œÑŒµ\n",
      "œÑŒøÃÄŒΩ DET det  œÑŒøÃÄŒΩ\n",
      "œÄŒ±œÅŒøÃÅŒΩœÑŒ± VERB obj  œÄŒ±œÅŒøÃÅŒΩœÑŒ±\n",
      "Œ∫Œ±ŒπÃÄ CCONJ cc  Œ∫Œ±ŒπÃÅ\n",
      "œÑŒøÃÄŒΩ DET det  œÑŒøÃÄŒΩ\n",
      "ŒµÃîŒæŒ∑ÕÇœÇ ADV amod  ŒµÃîŒæŒ∑ÕÇœÇ\n",
      "Œ±ÃîÃÅœÄŒ±ŒΩœÑŒ± ADJ amod  Œ±ÃîÃÅœÄŒ±œÇ\n",
      "ŒªŒøÃÅŒ≥ŒøŒΩ NOUN conj  ŒªŒøÃÅŒ≥ŒøœÇ\n",
      "ŒøŒπÃîÕÇŒøŒΩ ADV advmod  ŒøŒπÃîÕÇŒøœÇ\n",
      "œÖÃîœÄŒøŒ∏ŒµÃÅœÉŒµŒπœÇ NOUN obj  œÖÃîœÄŒøŒ∏ŒµÃÅœÉŒµŒπœÇ\n",
      "œÑŒπŒΩŒ±ÃÄœÇ DET det  œÑŒπŒΩŒ±ÃÄœÇ\n",
      "œÑœâÕÇŒΩ DET det  ŒøÃî\n",
      "Œ±ÃìœÄŒøŒ¥ŒµŒπÃÅŒæŒµœâŒΩ NOUN nmod  Œ±ÃìœÄŒøŒ¥ŒµŒπÃÅŒæŒπœÇ\n",
      "ŒªŒ±ŒºŒ≤Œ±ÃÅŒΩŒøŒΩœÑŒµœÇ VERB advcl  ŒªŒ±ŒºŒ≤Œ±ÃÅŒΩœâ\n",
      "ŒµÃìŒΩ ADP case  ŒµÃìŒΩ\n",
      "ŒµÃîŒ∫Œ±ÃÅœÉœÑœâÕÖ ADJ obl  ŒµÃîŒ∫Œ±ÃÅœÉœÑœâÕÖ\n",
      "œÑœâÕÇŒΩ DET det  ŒøÃî\n",
      "ŒøÃìœÅŒ≥Œ±ÃÅŒΩœâŒΩ NOUN nmod  ŒøÃìœÅŒ≥Œ±ÃÅŒΩœâŒΩ\n",
      "œÑŒ∑ÃÄŒΩ DET det  œÑŒ∑ÃÄŒΩ\n",
      "ŒµÃìŒæ ADP case  ŒµÃìŒ∫\n",
      "Œ±œÖÃìœÑœâÕÇŒΩ PRON nmod  Œ±œÖÃìœÑŒøÃÅœÇ\n",
      "œâÃìœÜŒµÃÅŒªŒµŒπŒ±ŒΩ NOUN obj  œâÃìœÜŒµÃÅŒªŒµŒπŒ±\n",
      "ŒµÃìœÅŒøœÖÕÇŒºŒµŒΩ VERB ROOT  ŒµÃìœÅŒøœÖÕÇŒºŒµŒΩ\n",
      "Œ±ÃìœÄŒøÃÄ ADP case  Œ±ÃìœÄŒøÃÄ\n",
      "œÑœâÕÇŒΩ DET det  ŒøÃî\n",
      "Œ¥Œ±Œ∫œÑœÖÃÅŒªœâŒΩ NOUN obl  Œ¥Œ±Œ∫œÑœÖÕÇŒªŒøŒΩ\n",
      "Œ±œÖÃìÕÇŒ∏ŒπœÇ ADV advmod  Œ±œÖÃìÕÇŒ∏ŒπœÇ\n",
      "Œ±ÃìœÅŒæŒ±ÃÅŒºŒµŒΩŒøŒπ VERB advcl  Œ±ÃìœÅŒæŒ±ÃÅŒºŒµŒΩŒøŒπ\n",
      ". PUNCT advmod  .\n",
      "ŒµÃìœÄŒµŒπŒ¥Œ∑ÃÄ SCONJ mark  ŒµÃìœÄŒµŒπŒ¥Œ∑ÃÄœÇ\n",
      "Œ≥Œ±ÃÄœÅ ADV discourse  Œ≥Œ±ÃÄœÅ\n",
      "œÑŒ∑ÃÄŒΩ DET det  œÑŒ∑ÃÄŒΩ\n",
      "œÑœâÕÇŒΩ DET det  ŒøÃî\n",
      "ŒøÃìœÉœÑœâÕÇŒΩ NOUN nmod  ŒøÃìœÉÃÅœÑŒøœÇ\n",
      "Œ∫Œ±œÑŒ±œÉŒ∫ŒµœÖŒ∑ÃÄŒΩ NOUN obj  Œ∫Œ±œÑŒ±œÉŒ∫ŒµœÖŒ∑ÃÅ\n",
      "ŒµÃìŒΩ ADP case  ŒµÃìŒΩ\n",
      "Œ±œÖÃìœÑŒøŒπÕÇœÇ PRON obl  Œ±œÖÃìœÑŒøÃÅœÇ\n",
      "ŒµÃìœÄŒπœÑŒ∑Œ¥ŒµŒπŒøœÑŒ±ÃÅœÑŒ∑ŒΩ ADJ amod  ŒµÃìœÄŒπœÑŒ∑Œ¥ŒµŒπŒøœÑŒ±ÃÅœÑŒ∑ŒΩ\n",
      "ŒøÃìœÅŒ≥Œ±ÃÅŒΩŒøŒπœÇ NOUN xcomp  ŒøÃìœÅŒ≥Œ±ÕÇŒΩŒøŒΩ\n",
      "Œ±ÃìŒΩœÑŒπŒªŒ∑œÄœÑŒπŒ∫ŒøŒπÕÇœÇ ADJ amod  Œ±ÃìŒΩœÑŒπŒªŒ∑œÄœÑŒπŒ∫ŒøÃÅœÇ\n",
      "Œ∑Ãî DET det  ŒøÃî\n",
      "œÜœÖÃÅœÉŒπœÇ NOUN nsubj  œÜœÖÃÅœÉŒπœÇ\n",
      "ŒµÃìœÄŒøŒπÃÅŒ∑œÉŒµŒΩ VERB advcl  ŒµÃìœÄŒøŒπÃÅŒ∑œÉŒµŒΩ\n",
      ", PUNCT dep  ,\n",
      "Œ∑ÃìÕÇŒΩ AUX cop  Œ∑ÃìÕÇŒΩ\n",
      "Œ¥‚Äô ADV discourse  ŒøÃî ŒºŒµÃÅŒΩ...ŒøÃî Œ¥ŒµÃÅ\n",
      "Œ±ÃìŒºŒ∑ÃÅœáŒ±ŒΩŒøŒΩ ADJ parataxis  Œ±ÃìŒºŒ∑ÃÅœáŒ±ŒΩŒøœÇ\n",
      "Œ±œÖÃìœÑŒøŒπÕÇœÇ DET det  Œ±œÖÃìœÑŒøÃÅœÇ\n",
      "œÑŒøŒπÕÇœÇ DET det  ŒøÃî\n",
      "ŒøÃìœÉœÑŒøŒπÕÇœÇ NOUN iobj  ŒøÃìœÉœÑŒøŒπÕÇœÇ\n",
      "ŒøœÖÃîÃÅœÑœâ ADV advmod  ŒøœÖÃîÃÅœÑœâœÇ\n",
      "Œ≥ŒµœâÃÅŒ¥ŒµœÉŒπÃÅ ADJ advcl  Œ≥ŒµœâÃÅŒ¥ŒµœÉŒπÃÅ\n",
      "œÑŒµ CCONJ cc  œÑŒµ\n",
      "Œ∫Œ±ŒπÃÄ CCONJ cc  Œ∫Œ±ŒπÃÅ\n",
      "ŒªŒπŒ∏œâÃÅŒ¥ŒµœÉŒπŒΩ ADJ conj  ŒªŒπŒ∏œâÃÅŒ¥Œ∑œÇ\n",
      "ŒøœÖÃìÕÇœÉŒπ AUX cop  ŒøœÖÃìÕÇœÇ\n",
      "ŒºŒµœÑŒ±Œ¥ŒøœÖÕÇŒΩŒ±Œπ VERB ccomp  ŒºŒµœÑŒ±Œ¥ŒøœÖÕÇŒΩŒ±Œπ\n",
      "œÑŒ∑ÕÇœÇ DET det  ŒøÃî\n",
      "Œ∫Œ±Œ∏‚Äô ADP case  Œ∫Œ±Œ∏Œ±ÃÅ\n",
      "ŒøÃîœÅŒºŒ∑ÃÄŒΩ NOUN nmod  ŒøÃîœÅŒºŒ∑ÃÅ\n",
      "Œ∫ŒπŒΩŒ∑ÃÅœÉŒµœâœÇ NOUN iobj  Œ∫ŒπŒΩŒ∑ÃÅœÉŒµœâœÇ\n",
      ", PUNCT advmod  ,\n",
      "ŒµÃìŒæŒµœÖÕÇœÅŒµŒΩ VERB ROOT  ŒµÃìŒæŒµœÖÕÇœÅŒµŒΩ\n",
      ", PUNCT advmod  ,\n",
      "ŒøÃîÃÅœÑœâÕÖ PRON det  ŒøÃîÃÅœÑœâÕÖ\n",
      "œÑœÅŒøÃÅœÄœâÕÖ NOUN obl  œÑœÅŒøÃÅœÄœâÕÖ\n",
      "Œ¥Œπ‚Äô ADP case  Œ¥ŒπŒ±ÃÅ\n",
      "ŒµÃîœÑŒµÃÅœÅœâŒΩ ADJ advmod  ŒµÃîœÑŒµÃÅœÅœâŒΩ\n",
      "Œ±œÖÃìœÑŒ±ÃÄ PRON obj  Œ±œÖÃìœÑŒ±ÃÄ\n",
      "Œ∫ŒπŒΩŒ∑ÃÅœÉŒµŒπ VERB ROOT  Œ∫ŒπŒΩŒ∑ÃÅœâ\n",
      ". PUNCT ROOT  .\n",
      "œÑœâÕÇŒΩ DET det  ŒøÃî\n",
      "ŒøœÖÃìÕÇŒΩ ADV discourse  ŒøœÖÃìÕÇŒΩ\n",
      "Œ∫Œ±œÑŒ±ÃÄ ADP case  Œ∫Œ±œÑŒ±ÃÄ\n",
      "œÑŒøÃÄŒΩ DET det  œÑŒøÃÄŒΩ\n",
      "œÄŒ∑ÕÇœáœÖŒΩ NOUN nmod  œÄŒ∑ÕÇœáœÖœÇ\n",
      "ŒºœÖœâÕÇŒΩ NOUN obj  ŒºÃÅœÖŒøœÇ\n",
      "Œ±ÃìœÄŒøœÜœÖÃÅœÉŒ±œÉŒ± VERB advcl  Œ±ÃìœÄŒøœÜœÖÃÅœâ\n",
      "œÑŒµÃÅŒΩŒøŒΩœÑŒ±œÇ NOUN obj  œÑŒµÃÅŒΩœâŒΩ\n",
      "ŒµœÖÃìŒ∏œÖÃÄ ADV advmod  ŒµœÖÃìŒ∏œÖÃÅœÇ\n",
      "œÑœâÕÇŒΩ DET det  ŒøÃî\n",
      "Œ¥Œ±Œ∫œÑœÖÃÅŒªœâŒΩ NOUN iobj  Œ¥Œ±Œ∫œÑœÖÕÇŒªŒøŒΩ\n",
      "Œ∑ÃìÃÅŒ≥Œ±Œ≥ŒµŒΩ VERB ROOT  Œ∑ÃìÃÅŒ≥Œ±Œ≥ŒµŒΩ\n",
      ". PUNCT ROOT  .\n",
      "Œ±ÃîÃÄ PRON dep  Œ±ÃîÃÄ\n",
      "Œ≥Œ±ÃÄœÅ ADV discourse  Œ≥Œ±ÃÄœÅ\n",
      "ŒøŒπÃî DET det  ŒøŒπÃî\n",
      "œÄŒ±ŒªŒ±ŒπŒøŒπÃÄ ADJ nsubj  œÄŒ±ŒªŒ±ŒπŒøŒπÃÄ\n",
      "Œ∫Œ±ŒªŒøœÖÕÇœÉŒπ VERB acl  Œ∫Œ±ŒªŒµÃÅœâ\n",
      "ŒΩŒµœÖÕÇœÅŒ± NOUN dep  ŒΩŒµœÖÕÇœÅŒøŒΩ\n",
      ", PUNCT dep  ,\n",
      "œÑŒ±œÖœÑŒπÃÄ DET det  œÑŒ±œÖœÑŒπÃÄ\n",
      "œÑŒ±ÃÄ DET det  œÑŒ±ÃÄ\n",
      "œÄœÅŒøœÜŒ±ŒΩŒ∑ÕÇ ADJ conj  œÄœÅŒøœÜŒ±ŒΩŒ∑ÃÅœÇ\n",
      ", PUNCT dep  ,\n",
      "œÑŒ±ÃÄ DET det  œÑŒ±ÃÄ\n",
      "Œ∫ŒπŒΩŒøœÖÕÇŒΩœÑŒ± VERB conj  Œ∫ŒπŒΩŒøœÖÕÇŒΩœÑŒ±\n",
      "œÑŒøœÖÃÄœÇ DET det  œÑŒøœÖÃÄœÇ\n",
      "Œ¥Œ±Œ∫œÑœÖÃÅŒªŒøœÖœÇ NOUN obj  Œ¥Œ±ÃÅŒ∫œÑœÖŒªŒøœÇ\n",
      ", PUNCT dep  ,\n",
      "ŒøŒπÃî DET det  ŒøŒπÃî\n",
      "œÑŒµÃÅŒΩŒøŒΩœÑŒµÃÅœÇ NOUN dep  œÑŒµÃÅŒΩŒøŒΩœÑŒµÃÅœÇ\n",
      "ŒµŒπÃìœÉŒπŒΩ AUX cop  ŒµŒπÃìŒºŒØ\n",
      "¬∑ PUNCT ROOT  ¬∑\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texttest)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc:\n",
    "     print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4m‚Ñπ Using GPU: 0\u001b[0m\n",
      "Warming up for 3 epochs...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 315/315 [00:04<00:00, 72.84doc/s]\n",
      "\n",
      "Benchmarking 50 batches...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6400/6400 [02:47<00:00, 38.32doc/s]\n",
      "\n",
      "Outliers: 20.0%, extreme outliers: 0.0%\n",
      "Mean: 6767.7 words/s (95% CI: -701.5 +659.6)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark speed /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/grc_proiel-ud-test.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "#normalize texwor\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"Œ∫Œ±ŒπÃÄ Œ≥Œ±ÃÄœÅ Œ±ÃìŒΩŒ±œÄŒΩŒµŒπÕÇ Œ∫Œ±ŒπÃÄ ŒµÃìŒ∫œÄŒΩŒµŒπÕÇ œÑŒ±œÖÃÅœÑŒ∑ÕÖ Œ∫Œ±ŒπÃÄ ŒøÃî œÄœÑŒ±œÅŒºŒøÃÄœÇ Œ¥ŒπŒ±ÃÄ œÑŒ±œÖÃÅœÑŒ∑œÇ Œ≥ŒπÃÅŒΩŒµœÑŒ±Œπ œÄŒΩŒµœÖÃÅŒºŒ±œÑŒøœÇ Œ±ÃìŒ∏œÅŒøÃÅŒøœÖ ŒµÃìÃÅŒæŒøŒ¥ŒøœÇ œÉŒ∑ŒºŒµŒπÕÇŒøŒΩ ŒøŒπÃìœâŒΩŒπœÉœÑŒπŒ∫ŒøÃÄŒΩ Œ∫Œ±ŒπÃÄ ŒπÃîŒµœÅŒøÃÄŒΩ ŒºŒøÃÅŒΩŒøŒΩ œÑœâÕÇŒΩ œÄŒΩŒµœÖŒºŒ±ÃÅœÑœâŒΩ\"\n",
    "text =(\"Œ§Œ±·ø¶œÑ‚Äô Œø·ΩñŒΩ Œµ·º¥œÇ œÑŒµ œÑ·Ω∏ŒΩ œÄŒ±œÅœåŒΩœÑŒ± Œ∫Œ±·Ω∂ œÑ·Ω∏ŒΩ ·ºëŒæ·øÜœÇ ·ºÖœÄŒ±ŒΩœÑŒ± ŒªœåŒ≥ŒøŒΩ Œø·º∑ŒøŒΩ ·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ œÑŒπŒΩ·Ω∞œÇ œÑ·ø∂ŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ·ºêŒΩ ·ºëŒ∫Œ¨œÉœÑ·ø≥ œÑ·ø∂ŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ œÑ·Ω¥ŒΩ ·ºêŒæ Œ±·ΩêœÑ·ø∂ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·ºêœÅŒø·ø¶ŒºŒµŒΩ ·ºÄœÄ·Ω∏ œÑ·ø∂ŒΩ Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ±·ΩñŒ∏ŒπœÇ ·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ.\")\n",
    "text_n = normalize_text(\"Œ§Œ±·ø¶œÑ‚Äô Œø·ΩñŒΩ Œµ·º¥œÇ œÑŒµ œÑ·Ω∏ŒΩ œÄŒ±œÅœåŒΩœÑŒ± Œ∫Œ±·Ω∂ œÑ·Ω∏ŒΩ ·ºëŒæ·øÜœÇ ·ºÖœÄŒ±ŒΩœÑŒ± ŒªœåŒ≥ŒøŒΩ Œø·º∑ŒøŒΩ ·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ œÑŒπŒΩ·Ω∞œÇ œÑ·ø∂ŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ·ºêŒΩ ·ºëŒ∫Œ¨œÉœÑ·ø≥ œÑ·ø∂ŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ œÑ·Ω¥ŒΩ ·ºêŒæ Œ±·ΩêœÑ·ø∂ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·ºêœÅŒø·ø¶ŒºŒµŒΩ ·ºÄœÄ·Ω∏ œÑ·ø∂ŒΩ Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ±·ΩñŒ∏ŒπœÇ ·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"training/transformer_mostfiles/assembled/model-best/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ§Œ±·ø¶œÑ Œ§Œ±·ø¶œÑ ADJ obj\n",
      "‚Äô punc1 ADV discourse\n",
      "Œø·ΩñŒΩ Œø·ΩñŒΩ ADV discourse\n",
      "Œµ·º¥œÇ Œµ·º¥œÇ ADP case\n",
      "œÑŒµ œÑŒµ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "œÄŒ±œÅœåŒΩœÑŒ± œÄŒ¨œÅŒµŒπŒºŒπ VERB dep\n",
      "Œ∫Œ±·Ω∂ Œ∫Œ±ŒπÃÅ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "·ºëŒæ·øÜœÇ ·ºëŒæ·øÜœÇ ADV amod\n",
      "·ºÖœÄŒ±ŒΩœÑŒ± ·ºÖœÄŒ±œÇ ADJ amod\n",
      "ŒªœåŒ≥ŒøŒΩ ŒªœåŒ≥ŒøœÇ NOUN conj\n",
      "Œø·º∑ŒøŒΩ Œø·º∑ŒøœÇ ADV advmod\n",
      "·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ ·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ NOUN obj\n",
      "œÑŒπŒΩ·Ω∞œÇ œÑŒπŒΩ·Ω∞œÇ DET det\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒπœÇ NOUN nmod\n",
      "ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ŒªŒ±ŒºŒ≤Œ¨ŒΩœâ VERB advcl\n",
      "·ºêŒΩ ·ºêŒΩ ADP case\n",
      "·ºëŒ∫Œ¨œÉœÑ·ø≥ ·ºïŒ∫Œ±œÉœÑŒøœÇ ADJ obl\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩŒøŒΩ NOUN nmod\n",
      "œÑ·Ω¥ŒΩ œÑ·Ω¥ŒΩ DET det\n",
      "·ºêŒæ ·ºêŒ∫ ADP case\n",
      "Œ±·ΩêœÑ·ø∂ŒΩ Œ±œÖÃìœÑŒøÃÅœÇ PRON nmod\n",
      "·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ± NOUN obj\n",
      "·ºêœÅŒø·ø¶ŒºŒµŒΩ ·ºêœÅŒø·ø¶ŒºŒµŒΩ VERB ROOT\n",
      "·ºÄœÄ·Ω∏ ·ºÄœÄ·Ω∏ ADP case\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ¥Œ¨Œ∫œÑœÖŒªŒøœÇ NOUN obl\n",
      "Œ±·ΩñŒ∏ŒπœÇ Œ±·ΩñŒ∏ŒπœÇ ADV advmod\n",
      "·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ ·ºÑœÅœáœâ VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "#test non nfkd\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ§Œ±œÖÕÇœÑ Œ§Œ±œÖÕÇœÑ ADJ obj\n",
      "‚Äô punc1 ADV discourse\n",
      "ŒøœÖÃìÕÇŒΩ ŒøœÖÃìÕÇŒΩ ADV discourse\n",
      "ŒµŒπÃìÃÅœÇ ŒµŒπÃìÃÅœÇ ADP case\n",
      "œÑŒµ œÑŒµ CCONJ cc\n",
      "œÑŒøÃÄŒΩ œÑŒøÃÄŒΩ DET det\n",
      "œÄŒ±œÅŒøÃÅŒΩœÑŒ± œÄŒ±œÅŒøÃÅŒΩœÑŒ± VERB dep\n",
      "Œ∫Œ±ŒπÃÄ Œ∫Œ±ŒπÃÅ CCONJ cc\n",
      "œÑŒøÃÄŒΩ œÑŒøÃÄŒΩ DET det\n",
      "ŒµÃîŒæŒ∑ÕÇœÇ ŒµÃîŒæŒ∑ÕÇœÇ ADV amod\n",
      "Œ±ÃîÃÅœÄŒ±ŒΩœÑŒ± Œ±ÃîÃÅœÄŒ±œÇ ADJ amod\n",
      "ŒªŒøÃÅŒ≥ŒøŒΩ ŒªŒøÃÅŒ≥ŒøœÇ NOUN conj\n",
      "ŒøŒπÃîÕÇŒøŒΩ ŒøŒπÃîÕÇŒøœÇ ADV advmod\n",
      "œÖÃîœÄŒøŒ∏ŒµÃÅœÉŒµŒπœÇ œÖÃîœÄŒøŒ∏ŒµÃÅœÉŒµŒπœÇ NOUN obj\n",
      "œÑŒπŒΩŒ±ÃÄœÇ œÑŒπŒΩŒ±ÃÄœÇ DET det\n",
      "œÑœâÕÇŒΩ ŒøÃî DET det\n",
      "Œ±ÃìœÄŒøŒ¥ŒµŒπÃÅŒæŒµœâŒΩ Œ±ÃìœÄŒøŒ¥ŒµŒπÃÅŒæŒπœÇ NOUN nmod\n",
      "ŒªŒ±ŒºŒ≤Œ±ÃÅŒΩŒøŒΩœÑŒµœÇ ŒªŒ±ŒºŒ≤Œ±ÃÅŒΩœâ VERB advcl\n",
      "ŒµÃìŒΩ ŒµÃìŒΩ ADP case\n",
      "ŒµÃîŒ∫Œ±ÃÅœÉœÑœâÕÖ ŒµÃîŒ∫Œ±ÃÅœÉœÑœâÕÖ ADJ obl\n",
      "œÑœâÕÇŒΩ ŒøÃî DET det\n",
      "ŒøÃìœÅŒ≥Œ±ÃÅŒΩœâŒΩ ŒøÃìœÅŒ≥Œ±ÃÅŒΩŒøŒΩ NOUN nmod\n",
      "œÑŒ∑ÃÄŒΩ œÑŒ∑ÃÄŒΩ DET det\n",
      "ŒµÃìŒæ ŒµÃìŒ∫ ADP case\n",
      "Œ±œÖÃìœÑœâÕÇŒΩ Œ±œÖÃìœÑŒøÃÅœÇ PRON nmod\n",
      "œâÃìœÜŒµÃÅŒªŒµŒπŒ±ŒΩ œâÃìœÜŒµÃÅŒªŒµŒπŒ± NOUN obj\n",
      "ŒµÃìœÅŒøœÖÕÇŒºŒµŒΩ ŒµÃìœÅŒøœÖÕÇŒºŒµŒΩ VERB ROOT\n",
      "Œ±ÃìœÄŒøÃÄ Œ±ÃìœÄŒøÃÄ ADP case\n",
      "œÑœâÕÇŒΩ ŒøÃî DET det\n",
      "Œ¥Œ±Œ∫œÑœÖÃÅŒªœâŒΩ Œ¥Œ±Œ∫œÑœÖÃÅŒªœâŒΩ NOUN obl\n",
      "Œ±œÖÃìÕÇŒ∏ŒπœÇ Œ±œÖÃìÕÇŒ∏ŒπœÇ ADV advmod\n",
      "Œ±ÃìœÅŒæŒ±ÃÅŒºŒµŒΩŒøŒπ Œ±ÃìœÅŒæŒ±ÃÅŒºŒµŒΩŒøŒπ VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "#test nfkd\n",
    "doc = nlp(text_n)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ§Œ±·ø¶œÑ Œ§Œ±·ø¶œÑ ADJ obj\n",
      "‚Äô punc1 ADV discourse\n",
      "Œø·ΩñŒΩ Œø·ΩñŒΩ ADV discourse\n",
      "Œµ·º¥œÇ Œµ·º¥œÇ ADP case\n",
      "œÑŒµ œÑŒµ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "œÄŒ±œÅœåŒΩœÑŒ± œÄŒ¨œÅŒµŒπŒºŒπ VERB dep\n",
      "Œ∫Œ±·Ω∂ Œ∫Œ±ŒπÃÅ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "·ºëŒæ·øÜœÇ ·ºëŒæ·øÜœÇ ADV amod\n",
      "·ºÖœÄŒ±ŒΩœÑŒ± ·ºÖœÄŒ±œÇ ADJ amod\n",
      "ŒªœåŒ≥ŒøŒΩ ŒªœåŒ≥ŒøœÇ NOUN conj\n",
      "Œø·º∑ŒøŒΩ Œø·º∑ŒøœÇ ADV advmod\n",
      "·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ ·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ NOUN obj\n",
      "œÑŒπŒΩ·Ω∞œÇ œÑŒπŒΩ·Ω∞œÇ DET det\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒπœÇ NOUN nmod\n",
      "ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ŒªŒ±ŒºŒ≤Œ¨ŒΩœâ VERB advcl\n",
      "·ºêŒΩ ·ºêŒΩ ADP case\n",
      "·ºëŒ∫Œ¨œÉœÑ·ø≥ ·ºïŒ∫Œ±œÉœÑŒøœÇ ADJ obl\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩŒøŒΩ NOUN nmod\n",
      "œÑ·Ω¥ŒΩ œÑ·Ω¥ŒΩ DET det\n",
      "·ºêŒæ ·ºêŒ∫ ADP case\n",
      "Œ±·ΩêœÑ·ø∂ŒΩ Œ±œÖÃìœÑŒøÃÅœÇ PRON nmod\n",
      "·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ± NOUN obj\n",
      "·ºêœÅŒø·ø¶ŒºŒµŒΩ ·ºêœÅŒø·ø¶ŒºŒµŒΩ VERB ROOT\n",
      "·ºÄœÄ·Ω∏ ·ºÄœÄ·Ω∏ ADP case\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ¥Œ¨Œ∫œÑœÖŒªŒøœÇ NOUN obl\n",
      "Œ±·ΩñŒ∏ŒπœÇ Œ±·ΩñŒ∏ŒπœÇ ADV advmod\n",
      "·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ ·ºÑœÅœáœâ VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ§Œ±·ø¶œÑ Œ§Œ±·ø¶œÑ ADJ obj\n",
      "‚Äô punc1 ADV discourse\n",
      "Œø·ΩñŒΩ Œø·ΩñŒΩ ADV discourse\n",
      "Œµ·º¥œÇ Œµ·º¥œÇ ADP case\n",
      "œÑŒµ œÑŒµ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "œÄŒ±œÅœåŒΩœÑŒ± œÄŒ¨œÅŒµŒπŒºŒπ VERB obj\n",
      "Œ∫Œ±·Ω∂ Œ∫Œ±ŒπÃÅ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "·ºëŒæ·øÜœÇ ·ºëŒæ·øÜœÇ ADV amod\n",
      "·ºÖœÄŒ±ŒΩœÑŒ± ·ºÖœÄŒ±œÇ ADJ amod\n",
      "ŒªœåŒ≥ŒøŒΩ ŒªœåŒ≥ŒøœÇ NOUN conj\n",
      "Œø·º∑ŒøŒΩ Œø·º∑ŒøœÇ ADV advmod\n",
      "·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ ·ΩëœÄœåŒ∏ŒµœÉŒπœÇ NOUN obj\n",
      "œÑŒπŒΩ·Ω∞œÇ œÑŒØœÇ DET det\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒπœÇ NOUN nmod\n",
      "ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ŒªŒ±ŒºŒ≤Œ¨ŒΩœâ VERB advcl\n",
      "·ºêŒΩ ·ºêŒΩ ADP case\n",
      "·ºëŒ∫Œ¨œÉœÑ·ø≥ ·ºïŒ∫Œ±œÉœÑŒøœÇ ADJ obl\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ ·ΩÑœÅŒ≥Œ±ŒΩŒøŒΩ NOUN nmod\n",
      "œÑ·Ω¥ŒΩ œÑ·Ω¥ŒΩ DET det\n",
      "·ºêŒæ ·ºêŒ∫ ADP case\n",
      "Œ±·ΩêœÑ·ø∂ŒΩ Œ±·ΩêœÑœåœÇ PRON nmod\n",
      "·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ± NOUN obj\n",
      "·ºêœÅŒø·ø¶ŒºŒµŒΩ ŒªŒ≠Œ≥œâ VERB ROOT\n",
      "·ºÄœÄ·Ω∏ ·ºÄœÄ·Ω∏ ADP case\n",
      "œÑ·ø∂ŒΩ ŒøÃî DET det\n",
      "Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ¥Œ¨Œ∫œÑœÖŒªŒøœÇ NOUN obl\n",
      "Œ±·ΩñŒ∏ŒπœÇ Œ±·ΩñŒ∏ŒπœÇ ADV advmod\n",
      "·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ ·ºÑœÅœáœâ VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"training/transformer/assembled/model-best/\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Œ§Œ±·ø¶œÑ Œ§Œ±·ø¶œÑ ADJ obj\n",
      "‚Äô ‚Äô ADV discourse\n",
      "Œø·ΩñŒΩ Œø·ΩñŒΩ ADV discourse\n",
      "Œµ·º¥œÇ Œµ·º¥œÇ ADP case\n",
      "œÑŒµ œÑŒµ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "œÄŒ±œÅœåŒΩœÑŒ± œÄŒ±œÅœåŒΩœÑŒ± VERB obj\n",
      "Œ∫Œ±·Ω∂ Œ∫Œ±ŒπÃÅ CCONJ cc\n",
      "œÑ·Ω∏ŒΩ œÑ·Ω∏ŒΩ DET det\n",
      "·ºëŒæ·øÜœÇ ·ºëŒæ·øÜœÇ ADV amod\n",
      "·ºÖœÄŒ±ŒΩœÑŒ± ·ºÖœÄŒ±ŒΩœÑŒ± ADJ amod\n",
      "ŒªœåŒ≥ŒøŒΩ ŒªœåŒ≥ŒøŒΩ NOUN conj\n",
      "Œø·º∑ŒøŒΩ Œø·º∑ŒøŒΩ ADV advmod\n",
      "·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ ·ΩëœÄŒøŒ∏Œ≠œÉŒµŒπœÇ NOUN obj\n",
      "œÑŒπŒΩ·Ω∞œÇ œÑŒπŒΩ·Ω∞œÇ DET det\n",
      "œÑ·ø∂ŒΩ œÑ·ø∂ŒΩ DET det\n",
      "·ºÄœÄŒøŒ¥ŒµŒØŒæŒµœâŒΩ ·ºÄœÄŒøŒ¥ŒµŒØŒæŒµŒøŒΩ NOUN nmod\n",
      "ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ ŒªŒ±ŒºŒ≤Œ¨ŒΩŒøŒΩœÑŒµœÇ VERB advcl\n",
      "·ºêŒΩ ·ºêŒΩ ADP case\n",
      "·ºëŒ∫Œ¨œÉœÑ·ø≥ ·ºëŒ∫Œ¨œÉœÑ·ø≥ ADJ obl\n",
      "œÑ·ø∂ŒΩ œÑ·ø∂ŒΩ DET det\n",
      "·ΩÄœÅŒ≥Œ¨ŒΩœâŒΩ ·ΩÄœÅŒ≥Œ¨ŒΩŒøŒΩ NOUN nmod\n",
      "œÑ·Ω¥ŒΩ œÑ·Ω¥ŒΩ DET det\n",
      "·ºêŒæ ·ºêŒæ ADP case\n",
      "Œ±·ΩêœÑ·ø∂ŒΩ Œ±·ΩêœÑ·ø∂ŒΩ PRON nmod\n",
      "·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ ·Ω†œÜŒ≠ŒªŒµŒπŒ±ŒΩ NOUN obj\n",
      "·ºêœÅŒø·ø¶ŒºŒµŒΩ ·ºêœÅŒø·ø¶ŒºŒµŒΩ VERB ROOT\n",
      "·ºÄœÄ·Ω∏ ·ºÄœÄ·Ω∏ ADP case\n",
      "œÑ·ø∂ŒΩ œÑœåœÇ DET det\n",
      "Œ¥Œ±Œ∫œÑœçŒªœâŒΩ Œ¥Œ±Œ∫œÑœçŒª NOUN obl\n",
      "Œ±·ΩñŒ∏ŒπœÇ Œ±·ΩñŒ∏ŒπœÇ ADV advmod\n",
      "·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ ·ºÄœÅŒæŒ¨ŒºŒµŒΩŒøŒπ VERB advcl\n",
      ". . PUNCT ROOT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"training/NER/model-best/\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
