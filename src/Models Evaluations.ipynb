{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11315,
     "status": "ok",
     "timestamp": 1655285213416,
     "user": {
      "displayName": "Roey Zaworbach",
      "userId": "02944633912131916379"
     },
     "user_tz": -180
    },
    "id": "GSq6f9MgNhG_",
    "outputId": "212eea25-3e3a-46f3-926a-d090faf6efa2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "#!pip install --upgrade spacy\n",
    "import spacy\n",
    "from spacy.util import compounding, minibatch\n",
    "from spacy import displacy\n",
    "# Uncomment if you want Spacy to use GPU for training. Note - this will use transformer architecture\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata as ud\n",
    "import warnings\n",
    "\n",
    "\n",
    "class LemmaEvaluator:\n",
    "    def __init__(self, nlp1, nlp2, nlp3=None, norm_method=None):\n",
    "        self.nlp1 = nlp1\n",
    "        self.nlp2 = nlp2\n",
    "        self.nlp3 = nlp3\n",
    "        self.norm_method = norm_method\n",
    "        \n",
    "        # check if normalization method is specified\n",
    "        if self.norm_method is None:\n",
    "            warnings.warn(\"Normalization method not specified. Text may not be normalized correctly.\", UserWarning)\n",
    "\n",
    "    def evaluate_lemmas(self, docs):\n",
    "        same_lemmas = 0\n",
    "        diff_lemmas = 0\n",
    "        correct_lemmas1 = 0\n",
    "        correct_lemmas2 = 0\n",
    "        correct_lemmas3 = 0\n",
    "        total_lemmas = 0\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for doc in tqdm(docs, desc=\"Evaluating models\", total=len(docs)):\n",
    "            for token in doc:\n",
    "                gold_lemma = self.clean_text(token.lemma_)\n",
    "                lemma1 = self.nlp1(self.clean_text(doc.text))[token.i].lemma_\n",
    "                lemma2 = self.nlp2(self.clean_text(doc.text))[token.i].lemma_\n",
    "                if self.nlp3 is not None:\n",
    "                    lemma3 = self.nlp3(self.clean_text(doc.text))[token.i].lemma_\n",
    "                if self.nlp3 is not None and lemma1 == lemma2 == lemma3 ==gold_lemma:\n",
    "                    result = \"All lemmas are the same\"\n",
    "                    same_lemmas += 1\n",
    "                    correct_lemmas1 += 1\n",
    "                    correct_lemmas2 += 1\n",
    "                    correct_lemmas3 += 1\n",
    "                elif lemma1 == lemma2 == gold_lemma:\n",
    "                    result = \"All lemmas are the same\"\n",
    "                    same_lemmas += 1\n",
    "                    correct_lemmas1 += 1\n",
    "                    correct_lemmas2 += 1\n",
    "                else:\n",
    "                    result = \"\"\n",
    "                    diff_lemmas += 1\n",
    "                    if lemma1 == gold_lemma:\n",
    "                        correct_lemmas1 += 1\n",
    "                        result += f\", Model 1 correct ({lemma1})\"\n",
    "                    else:\n",
    "                        result += f\", Model 1 incorrect ({lemma1})\"\n",
    "                    if lemma2 == gold_lemma:\n",
    "                        correct_lemmas2 += 1\n",
    "                        result += f\", Model 2 correct ({lemma2})\"\n",
    "                    else:\n",
    "                        result += f\", Model 2 incorrect ({lemma2})\"\n",
    "                    if self.nlp3 is not None:\n",
    "                        if lemma3 == gold_lemma:\n",
    "                            correct_lemmas3 += 1\n",
    "                            result += f\", Model 3 correct ({lemma3})\"\n",
    "                        else:\n",
    "                            result += f\", Model 3 incorrect ({lemma3})\"\n",
    "                if self.nlp3 is not None:\n",
    "                    data.append([doc.text, token.text, gold_lemma, lemma1, lemma2, lemma3, result])\n",
    "                else:\n",
    "                    data.append([doc.text, token.text, gold_lemma, lemma1, lemma2, result])\n",
    "                total_lemmas += 1\n",
    "\n",
    "        if self.nlp3 is not None:\n",
    "            df_evaluate = pd.DataFrame(data, columns=[\"Text\", \"Token\", \"Gold Lemma\", \"Model 1 Lemma\", \"Model 2 Lemma\", \"Model 3 Lemma\", \"Result\"])\n",
    "            print(df_evaluate)\n",
    "            print(f\"Total Lemmas: {total_lemmas}\")\n",
    "            print(f\"Total same lemmas: {same_lemmas}\")\n",
    "            print(f\"Total different lemmas: {diff_lemmas}\")\n",
    "            print(f\"Total correct lemmas for Model 1: {correct_lemmas1}\")\n",
    "            print(f\"Total correct lemmas for Model 2: {correct_lemmas2}\")\n",
    "            print(f\"Total correct lemmas for Model 3: {correct_lemmas3}\")\n",
    "        else:\n",
    "            df_evaluate = pd.DataFrame(data, columns=[\"Text\", \"Token\", \"Gold Lemma\", \"Model 1 Lemma\", \"Model 2 Lemma\", \"Result\"])\n",
    "            print(df_evaluate)\n",
    "            print(f\"Total Lemmas: {total_lemmas}\")\n",
    "            print(f\"Total same lemmas: {same_lemmas}\")\n",
    "            print(f\"Total different lemmas: {diff_lemmas}\")\n",
    "            print(f\"Total correct lemmas for Model 1: {correct_lemmas1}\")\n",
    "            print(f\"Total correct lemmas for Model 2: {correct_lemmas2}\")\n",
    "            \n",
    "        # calculate and print accuracy\n",
    "        if self.nlp3 is not None:\n",
    "            print(f\"Model 1 accuracy: {correct_lemmas1/total_lemmas:.2%}\")\n",
    "            print(f\"Model 2 accuracy: {correct_lemmas2/total_lemmas:.2%}\")\n",
    "            print(f\"Model 3 accuracy: {correct_lemmas3/total_lemmas:.2%}\")\n",
    "        else:\n",
    "            print(f\"Model 1 accuracy: {correct_lemmas1/total_lemmas:.2%}\")\n",
    "            print(f\"Model 2 accuracy: {correct_lemmas2/total_lemmas:.2%}\")\n",
    "            \n",
    "        return df_evaluate\n",
    "    \n",
    "    \n",
    "    def evaluate_line(self, line, model_name=\"Model\"):\n",
    "        doc = self.nlp1(line)\n",
    "        gold_lemmas = [token.lemma_ for token in doc]\n",
    "        lemma1 = [token.lemma_ for token in self.nlp1(line)]\n",
    "        lemma2 = [token.lemma_ for token in self.nlp2(line)]\n",
    "        if self.nlp3 is not None:\n",
    "            lemma3 = [token.lemma_ for token in self.nlp3(line)]\n",
    "        else:\n",
    "            lemma3 = None\n",
    "        # add NER labels to dataframe\n",
    "        NER1 = [token.ent_type_ for token in self.nlp1(line)]\n",
    "        NER2 = [token.ent_type_ for token in self.nlp2(line)]\n",
    "        if self.nlp3 is not None:\n",
    "            NER3 = [token.ent_type_ for token in self.nlp3(line)]\n",
    "        else:\n",
    "            NER3 = None    \n",
    "        data = {\"Gold Lemma\": gold_lemmas, f\"{model_name} 1 Lemma\": lemma1, f\"{model_name} 2 Lemma\": lemma2, f\"{model_name} 3 Lemma\": lemma3, f\"{model_name} 1 NER\": NER1, f\"{model_name} 2 NER\": NER2, f\"{model_name} 3 NER\": NER3}\n",
    "        if self.nlp3 is None:\n",
    "            del data[f\"{model_name} 3 Lemma\"]\n",
    "\n",
    "        df_evaluate_line = pd.DataFrame(data)\n",
    "        print(df_evaluate_line)\n",
    "        return df_evaluate_line\n",
    "        #print(data)\n",
    "        #return data\n",
    "    \n",
    "    def evaluate_ner(self, docs):\n",
    "        same_ner = 0\n",
    "        diff_ner = 0\n",
    "        correct_ner1 = 0\n",
    "        correct_ner2 = 0\n",
    "        correct_ner3 = 0\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for doc in tqdm(docs, desc=\"Evaluating models\", total=len(docs)):\n",
    "            for ent in doc.ents:\n",
    "                gold_label = self.clean_text(ent.label_)\n",
    "                label1 = None\n",
    "                label2 = None\n",
    "                label3 = None\n",
    "                result=\"\"\n",
    "                for token in doc:\n",
    "                    if token.idx == ent.start_char:\n",
    "                        label1 = self.nlp1(self.clean_text(token.text))[0].ent_type_\n",
    "                        label2 = self.nlp2(self.clean_text(token.text))[0].ent_type_\n",
    "                        if self.nlp3 is not None:\n",
    "                            label3 = self.nlp3(self.clean_text(token.text))[0].ent_type_\n",
    "                            #print label3 character count\n",
    "                            if len(label3) > 0:\n",
    "                                print(token.text, \" | \", self.nlp3(self.clean_text(token.text))[0])\n",
    "                        break\n",
    "\n",
    "                if label3 is None and label1 == label2 == gold_label:\n",
    "                    result = \"All NER labels are the same\"\n",
    "                    same_ner += 1\n",
    "                    correct_ner1 += 1\n",
    "                    correct_ner2 += 1\n",
    "                elif label1 == label2 == label3 == gold_label:\n",
    "                    result = \"All NER labels are the same\"\n",
    "                    same_ner += 1\n",
    "                    correct_ner1 += 1\n",
    "                    correct_ner2 += 1\n",
    "                    correct_ner3 += 1\n",
    "                else:\n",
    "                    result = \"NER labels are different\"\n",
    "                diff_ner += 1\n",
    "                if label1 == gold_label:\n",
    "                    correct_ner1 += 1\n",
    "                    result += f\", Model 1 correct ({label1})\"\n",
    "                else:\n",
    "                    result += f\", Model 1 incorrect ({label1})\"\n",
    "                if label2 == gold_label:\n",
    "                    correct_ner2 += 1\n",
    "                    result += f\", Model 2 correct ({label2})\"\n",
    "                else:\n",
    "                    result += f\", Model 2 incorrect ({label2})\"\n",
    "                if self.nlp3 is not None and label3 == gold_label:\n",
    "                        correct_ner3 += 1\n",
    "                        result += f\", Model 3 correct ({label3})\"\n",
    "                else:\n",
    "                    result += f\", Model 3 incorrect ({label3})\"\n",
    "                if self.nlp3 is not None:\n",
    "                    data.append([doc.text, ent.text, gold_label, label1, label2, label3, result])\n",
    "                else:\n",
    "                    data.append([doc.text, ent.text, gold_label, label1, label2, result])\n",
    "        if self.nlp3 is not None:\n",
    "            df_evaluate_ner = pd.DataFrame(data, columns=[\"Text\", \"Entity\", \"Gold Label\", \"Model 1 Label\", \"Model 2 Label\", \"Model 3 Label\", \"Result\"])\n",
    "        else:\n",
    "            df_evaluate_ner = pd.DataFrame(data, columns=[\"Text\", \"Entity\", \"Gold Label\", \"Model 1 Label\", \"Model 2 Label\", \"Result\"])\n",
    "        print(df_evaluate_ner)\n",
    "\n",
    "        total = same_ner + diff_ner\n",
    "        print(f\"Total same NER labels: {same_ner} ({same_ner/total:.2%})\")\n",
    "        print(f\"Total different NER labels: {diff_ner} ({diff_ner/total:.2%})\")\n",
    "        print(f\"Model 1 accuracy: {correct_ner1/total:.2%}\")\n",
    "        print(f\"Model 2 accuracy: {correct_ner2/total:.2%}\")\n",
    "        if self.nlp3 is not None:\n",
    "            print(f\"Model 3 accuracy: {correct_ner3/total:.2%}\")\n",
    "\n",
    "        return df_evaluate_ner\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        # Check if the normalization method is valid\n",
    "        if self.norm_method is not None and self.norm_method not in ['NFD', 'NFC', 'NFKD', 'NFKC']:\n",
    "            raise ValueError(\"Normalization method is not valid. Must be one of ['NFD', 'NFC', 'NFKD', 'NFKC'].\")\n",
    "        elif self.norm_method is not None:\n",
    "            cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "            cleaned = ud.normalize(self.norm_method, cleaned)\n",
    "        else:\n",
    "            cleaned = text\n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line=\"Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸς καὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\"\n",
    "#line = \"Τείνει δὲ πρῶτον μὲν ἄνω ἀπὸ τῆς καρδίας τῆς μεγάλης φλεβὸς μόριον πρὸς τὸν πλεύμονα καὶ τὴν σύναψιν τῆς ἀορτῆς, ἄσχιστος καὶ μεγάλη οὖσα φλέψ\"\n",
    "#line = \"'ᾗ δὲ συνήρτηται κοῖλόν ἐστιν.\"\n",
    "line = \" Ἔχει δὲ διαφορὰς πολλάς, καθάπερ ἡ κοιλία, καὶ τοῦτο τὸ μόριον\"\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "#Lemma Evaluations:\n",
    "nlp1 = spacy.load(\"grc_proiel_trf\")\n",
    "nlp2 = spacy.load(\"grc_odycy_joint_trf\")\n",
    "nlp3 = spacy.load('../training/grc_ud_proiel_trf_Lem_NER/model-best') #this is an old model from march 23\n",
    "\n",
    "evaluator = LemmaEvaluator(nlp1, nlp2, nlp3)\n",
    "\n",
    "evaluate_quote = evaluator.evaluate_line(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line=\"Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸς καὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\"\n",
    "#line = \" Ἔχει δὲ διαφορὰς πολλάς, καθάπερ ἡ κοιλία, καὶ τοῦτο τὸ μόριον\"\n",
    "\n",
    "nlp1 = spacy.load('../training/ATLOMY_G_NER_pipeline/sm/model-best')\n",
    "linenlp = nlp1(line)\n",
    "for token in linenlp:\n",
    "    print(token.text, token.ent_iob_, token.ent_type_, \"| \", token.lemma_)\n",
    "#for ent in linenlp.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \" Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸς καὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\"\n",
    "line = \"Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸςκαὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\"\n",
    "# get gold entities for docs[1]\n",
    "# if doc.text == line:\n",
    "for doc in docs:\n",
    "    if doc.text == line:\n",
    "        print(doc.text)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "#Lemma Evaluations:\n",
    "#nlp1 = spacy.load(\"../training/ATLOMY_G_NER_pipeline/sm/model-best\")\n",
    "nlp2 = spacy.load(\"../training/ATLOMY_G_NER_pipeline/sm_3_sep/model-best\") # this is a model from Sep 3\n",
    "nlp3 = spacy.load('../training/grc_ud_proiel_trf_Lem_NER/model-best') #this is an old model from march 23\n",
    "nlp1 = spacy.load('../training/ATLOMY_G_NER_pipeline/sm_29_sep/model-best') # this is a model from Sep 15\n",
    "\n",
    "\n",
    "evaluator = LemmaEvaluator(nlp1, nlp2, nlp3, norm_method='NFKD')\n",
    "\n",
    "test_docs = DocBin().from_disk('../corpus/test/lemma_test/test_lemma_NFKD.spacy')\n",
    "docs = list(test_docs.get_docs(nlp1.vocab))[:10]\n",
    "\n",
    "df_evaluate_lemmas = evaluator.evaluate_lemmas(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load('../training/ATLOMY_G_NER_pipeline/sm_15_sep/model-best') # this is a model from Sep 15\n",
    "test_docs = DocBin().from_disk('../corpus/dev/ner_dev/ner_dev_NFKC.spacy')\n",
    "docs = list(test_docs.get_docs(nlp1.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs\n",
    "#ἀποκαμφθεῖσα δὲ κάτω ἐπὶ σπονδύλους καταβαίνει ἔστ ̓ ἂν ἀφίκηται"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "#NER Evaluations:\n",
    "nlp1 = spacy.load(\"../training/ATLOMY_G_NER_pipeline/sm_29_sep/model-best\")\n",
    "nlp2 = spacy.load(\"../training/ATLOMY_G_NER_pipeline/sm_3_sep/model-best\")\n",
    "#nlp3 = spacy.load('../training/ATLOMY_G_NER_pipeline/A_GreekBert_sm/model-best')\n",
    "#nlp3 = spacy.load('../training/grc_ud_proiel_trf_Lem_NER/model-best')\n",
    "#nlp3 = spacy.load('../training/ATLOMY_G_NER_pipeline/sm_NFKD_only/model-best')\n",
    "nlp3 = spacy.load('../training/ATLOMY_G_NER_pipeline/sm_15_sep/model-best')\n",
    "\n",
    "evaluator = LemmaEvaluator(nlp1, nlp2, nlp3, norm_method='NFKC')\n",
    "\n",
    "test_docs = DocBin().from_disk('../corpus/test/ner_test/ner_test_NFKC.spacy')\n",
    "docs = list(test_docs.get_docs(nlp1.vocab))\n",
    "\n",
    "df_evaluate_ner = evaluator.evaluate_ner(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6ChLyYk1pmVs68IB/yCSC",
   "name": "Spacy-TOPO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "atlomy_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
