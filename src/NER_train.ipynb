{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import compounding, minibatch\n",
    "from spacy import displacy\n",
    "# Uncomment if you want Spacy to use GPU for training. Note - this will use transformer architecture\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Weights and Biases platform to log the model training\n",
    "import wandb\n",
    "wandb.init(project=\"Category Types (Greek NER)\", entity=\"atlomy-nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for first time using a new pretrained model pipeline\n",
    "# nlp.config.to_disk('./grc_ud_proiel_trf.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if you need spaCy to fill missing default values in config\n",
    "#!python -m spacy init fill-config --diff ../Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf-trainable-lemmatizer.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/assembled/model-best\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW_NER - SElf-built from source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ner training (2X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for config validity\n",
    "!python -m spacy debug config /root/Projects/Atlomy/git/greCy_ATLOMY/configs/ner_2Xnorm.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug data /root/Projects/Atlomy/git/greCy_ATLOMY/configs/ner_2Xnorm.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/ner_2Xnorm.cfg --output ../training/transformer/NER --gpu-id 0 --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/NER/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/ner_test/ner_test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path /root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/NER/model-best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ner training (old files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug data /root/Projects/Atlomy/git/greCy_ATLOMY/configs/ner.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/ner.cfg --output ../training/NER --gpu-id 0 --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy /root/Projects/Atlomy/git/greCy_ATLOMY/training/NER/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path /root/Projects/Atlomy/git/greCy_ATLOMY/training/NER/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug config /root/Projects/Atlomy/git/greCy_ATLOMY/configs/ner2.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/ner2.cfg --output ../training/NER2 --gpu-id 0 --nlp.lang=grc --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark the model\n",
    "!python -m spacy benchmark accuracy /root/Projects/Atlomy/git/greCy_ATLOMY/training/NER2/model-best /root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path /root/Projects/Atlomy/git/greCy_ATLOMY/training/NER2/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "# load the model you just trained\n",
    "nlp = spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/NER/model-best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "test_data = DocBin().from_disk('/root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/test.spacy')\n",
    "# get the docs from the test data\n",
    "test_docs = list(test_data.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_docs:\n",
    "      docced = nlp(doc)\n",
    "      #let's print the lemmas of the tokens\n",
    "      for token in docced:\n",
    "          print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define WandB sweep for NER_S_grc_ud_proiel_trf-new_db_3-accumulate-gradient.cfg\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"ents_f\",\n",
    "        \"goal\": \"maximize\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"training\": {\n",
    "            \"n_iter\": {\n",
    "                \"values\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "            },\n",
    "            \"batch_size\": {\n",
    "                \"values\": [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "            },\n",
    "            \"accumulate_gradient\": {\n",
    "                \"values\": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "            },\n",
    "            \"dropout\": {\n",
    "                \"values\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "            },\n",
    "            \"learning_rate\": {\n",
    "                \"values\": [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "            },\n",
    "            \"weight_decay\": {\n",
    "                \"values\": [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "            },\n",
    "            \"grad_norm\": {\n",
    "                \"values\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 10,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "from pathlib import Path\n",
    "from spacy.training.loop import train\n",
    "from spacy.training.initialize import init_nlp\n",
    "from spacy import util\n",
    "from thinc.api import Config\n",
    "import wandb\n",
    "spacy.require_gpu()\n",
    "\n",
    "\n",
    "def sweep_ner(default_config: Path, output_path: Path):\n",
    "    sweep_config = {\"method\": \"bayes\"}\n",
    "    #metric_a = {\"name\": \"ents_f\", \"goal\": \"maximize\"}\n",
    "    #metric_b = {\"name\": \"ents_p\", \"goal\": \"maximize\"}\n",
    "    #metric_c = {\"name\": \"ents_r\", \"goal\": \"maximize\"}\n",
    "    #metric_combined = 0.5*metric_a + 0.3*metric_b + 0.2*metric_c\n",
    "    #metric = {\"name\": \"metric_combined\", \"goal\": \"maximize\"}\n",
    "    metric = {\"name\": \"ents_f\", \"goal\": \"maximize\"}\n",
    "    sweep_config[\"metric\"] = metric\n",
    "    early_terminate = {\"type\": \"hyperband\", \"min_iter\": 3, \"s\": 2, \"eta\": 3}\n",
    "    parameters_dict = {\n",
    "        \"training.dropout\": {\"distribution\": \"uniform\",\"min\": 0.1, \"max\": 0.4},\n",
    "        \"training.optimizer.learn_rate\": {\"distribution\": \"uniform\",\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"training.batcher.size\": {\"distribution\": \"int_uniform\",\"min\": 64,\"max\": 1000,},\n",
    "        \"components.ner.model.maxout_pieces\": {\"values\": [1, 2, 3]},\n",
    "        \"components.ner.model.hidden_width\": {\"values\": [64, 128, 256, 512, 1024]},\n",
    "        \"components.ner.model.extra_state_tokens\": {\"values\": [True, False]},\n",
    "        \"components.ner.model.use_upper\": {\"values\": [True, False]},\n",
    "        #\"corpora.train.augmenter\": {\"values\": [None, \"lemmatizer\"]},\n",
    "        \"nlp.batch_size\": {\"values\": [128, 256, 512, 1024]},\n",
    "    }\n",
    "    sweep_config[\"early_terminate\"] = early_terminate\n",
    "    sweep_config[\"parameters\"] = parameters_dict\n",
    "    def train_spacy():\n",
    "        loaded_local_config = util.load_config(default_config)\n",
    "        with wandb.init() as run:\n",
    "            sweeps_config = Config(util.dot_to_dict(run.config))\n",
    "            merged_config = Config(loaded_local_config).merge(sweeps_config)\n",
    "            print(merged_config)\n",
    "            nlp = init_nlp(merged_config, use_gpu=0)\n",
    "            output_path.mkdir(parents=True, exist_ok=True)\n",
    "            train(nlp, output_path, use_gpu=0)\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"Greek_NER_sweeps\")\n",
    "    wandb.agent(sweep_id, train_spacy, count=20)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_ner(default_config=Path(\"../Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf-sweep.cfg\"), output_path=Path(\"../Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf-sweep2/\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solo Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin train model\n",
    "!python -m spacy train ../Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf_updating-newtest2-changes.cfg --gpu-id 0 --output ./Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf_updating-newtest2-changes/ --verbose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "!python -m spacy evaluate ../Models/grc_ud_proiel_trf/grc_ud_proiel_trf_Lem_NER/model-best ../Corpus/test.spacy --gpu-id 0 --displacy-limit 200 --displacy-path ../Models/grc_ud_proiel_trf/grc_ud_proiel_trf_Lem_NER/model-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark model speed - NEW feature, needs spacy V3.5\n",
    "# If using 3.5 make sure to run !python -m spacy validate to check installed pipelines compatibility to current version\n",
    "!python -m spacy benchmark speed  ./Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf_0.2/model-best ./Models/grc_ud_proiel_trf/corpus --gpu-id 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process text prompt. A normalization step similiar to the normalization done on the model dataset.\n",
    "import unicodedata as ud\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    cleaned = ud.normalize('NFKD', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "output_dir = Path(\"../Models/grc_ud_proiel_trf/grc_ud_proiel_trf_Lem_NER/model-best\")\n",
    "\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp = spacy.load(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Examples:\n",
    "Ἀνατομικὰς ἐγχειρήσεις ἔγραψα μὲν καὶ πρόσθεν, ἡνίκα τὸ πρῶτον ἀνῆλθον ἔναγχος εἰς Ῥώμην, ἄρχειν ἠργμένου τοῦ καὶ νῦν ἡμῖν ἄρχοντος Ἀντωνίνου, γράφειν δ' αὖθις ἄλλας ἔοικα ταύτας διὰ διττὴν αἰτίαν.\n",
    "Κατά την υπόθεση που είχε ανακοινωθεί, οι δύο άνδρες υπό περιπολία συνελήφθησαν στον οδό Μελίνου και Κορινθίου στην Αθήνα, μετά από συνομιλία με την αστυνομία."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι. \n",
    "ἐπειδὴ γὰρ τὴν τῶν ὀστῶν κατασκευὴν ἐν αὐτοῖς ἐπιτηδειοτάτην ὀργάνοις ἀντιληπτικοῖς ἡ φύσις ἐποίησεν, ἦν δ’ ἀμήχανον αὐτοῖς τοῖς ὀστοῖς οὕτω γεώδεσί τε καὶ λιθώδεσιν οὖσι μεταδοῦναι τῆς καθ’ ὁρμὴν κινήσεως, ἐξεῦρεν, ὅτῳ τρόπῳ δι’ ἑτέρων αὐτὰ κινήσει. \n",
    "τῶν οὖν κατὰ τὸν πῆχυν μυῶν ἀποφύσασα τένοντας εὐθὺ τῶν δακτύλων ἤγαγεν. \n",
    "ἃ γὰρ οἱ παλαιοὶ καλοῦσι νεῦρα, ταυτὶ τὰ προφανῆ, τὰ κινοῦντα τοὺς δακτύλους, οἱ τένοντές εἰσιν· "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_noclean = nlp(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι. ἐπειδὴ γὰρ τὴν τῶν ὀστῶν κατασκευὴν ἐν αὐτοῖς ἐπιτηδειοτάτην ὀργάνοις ἀντιληπτικοῖς ἡ φύσις ἐποίησεν, ἦν δ’ ἀμήχανον αὐτοῖς τοῖς ὀστοῖς οὕτω γεώδεσί τε καὶ λιθώδεσιν οὖσι μεταδοῦναι τῆς καθ’ ὁρμὴν κινήσεως, ἐξεῦρεν, ὅτῳ τρόπῳ δι’ ἑτέρων αὐτὰ κινήσει. τῶν οὖν κατὰ τὸν πῆχυν μυῶν ἀποφύσασα τένοντας εὐθὺ τῶν δακτύλων ἤγαγεν. ἃ γὰρ οἱ παλαιοὶ καλοῦσι νεῦρα, ταυτὶ τὰ προφανῆ, τὰ κινοῦντα τοὺς δακτύλους, οἱ τένοντές εἰσιν·\")\n",
    "for sent in doc_noclean.sents:\n",
    "    print(sent.text)\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc_noclean:\n",
    "     print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test text prompt\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "#clean text\n",
    "test = clean_text(test_text)\n",
    "#predict\n",
    "doc = nlp(test)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc:\n",
    "     print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleancomparetext = 'Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι. ἐπειδὴ γὰρ τὴν τῶν ὀστῶν κατασκευὴν ἐν αὐτοῖς ἐπιτηδειοτάτην ὀργάνοις ἀντιληπτικοῖς ἡ φύσις ἐποίησεν, ἦν δ’ ἀμήχανον αὐτοῖς τοῖς ὀστοῖς οὕτω γεώδεσί τε καὶ λιθώδεσιν οὖσι μεταδοῦναι τῆς καθ’ ὁρμὴν κινήσεως, ἐξεῦρεν, ὅτῳ τρόπῳ δι’ ἑτέρων αὐτὰ κινήσει. τῶν οὖν κατὰ τὸν πῆχυν μυῶν ἀποφύσασα τένοντας εὐθὺ τῶν δακτύλων ἤγαγεν. ἃ γὰρ οἱ παλαιοὶ καλοῦσι νεῦρα, ταυτὶ τὰ προφανῆ, τὰ κινοῦντα τοὺς δακτύλους, οἱ τένοντές εἰσιν·'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare text nlp - clean and uncleaned:\n",
    "doc_noclean = nlp(cleancomparetext)\n",
    "doc = nlp(clean_text(cleancomparetext))\n",
    "for token, token2, in zip(doc, doc_noclean):\n",
    "    #check if the lemmas are the same\n",
    "    if token.lemma_ != token2.lemma_:\n",
    "        print(token.text, \": \", token.lemma_, \" or \", token2.lemma_, '? ')\n",
    "        #print(token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_)\n",
    "        #print(token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m')\n",
    "    else:\n",
    "        print(token.text, \": \", \"is the Lemma \", token.lemma_, \" ? \")\n",
    "        #print(token.text, \"|\", \"1: \", token.lemma_, \"==\" , \"2: \", token2.lemma_)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = clean_text(cleancomparetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.get_pipe(\"trainable_lemmatizer\").label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print('TOKEN: ', token.text, 'LEMMA: ', token.lemma_, 'POS: ', token.pos_, 'TAG: ', token.tag_, 'DEP: ', token.dep_,\n",
    "            'SHAPE: ', token.shape_, ' ALPHA: ', token.is_alpha, 'STOP: ', token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "output_dir2 = Path(\"../Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf_updating\\model-best\")\n",
    "\n",
    "print(\"Loading from\", output_dir2)\n",
    "nlp2 = spacy.load(output_dir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test text prompt\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "#clean text\n",
    "test = clean_text(test_text)\n",
    "#predict\n",
    "doc2 = nlp2(test)\n",
    "for sent in doc2.sents:\n",
    "    print(sent.text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train_one_epoch(epoch, lr, bs): \n",
    "  acc = 0.25 + ((epoch/30) +  (random.random()/10))\n",
    "  loss = 0.2 + (1 - ((epoch-1)/10 +  random.random()/5))\n",
    "  return acc, loss\n",
    "\n",
    "def evaluate_one_epoch(epoch): \n",
    "  acc = 0.1 + ((epoch/20) +  (random.random()/10))\n",
    "  loss = 0.25 + (1 - ((epoch-1)/10 +  random.random()/6))\n",
    "  return acc, loss\n",
    "  \n",
    "config = {\n",
    "    'lr' : 0.0001,\n",
    "    'bs' : 16,\n",
    "    'epochs': 5\n",
    "}\n",
    "\n",
    "def main():\n",
    "    # Note that we define values from `wandb.config` instead of \n",
    "    # defining hard values\n",
    "    lr = config['lr']\n",
    "    bs = config['bs']\n",
    "    epochs = config['epochs']\n",
    "\n",
    "    for epoch in np.arange(1, epochs):\n",
    "      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n",
    "      val_acc, val_loss = evaluate_one_epoch(epoch)\n",
    "      \n",
    "      print('epoch: ', epoch)\n",
    "      print('training accuracy:', train_acc,'training loss:', train_loss)\n",
    "      print('validation accuracy:', val_acc,'training loss:', val_loss)\n",
    "\n",
    "# Call the main function.       \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "# Define sweep config\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'sweep',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10, 15]},\n",
    "        'lr': {'max': 0.1, 'min': 0.0001}\n",
    "     }\n",
    "}\n",
    "\n",
    "# Initialize sweep by passing in config. (Optional) Provide a name of the project.\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='my-first-sweep')\n",
    "\n",
    "# Define training function that takes in hyperparameter values from `wandb.config` and uses them to train a model and return metric\n",
    "def train_one_epoch(epoch, lr, bs): \n",
    "  acc = 0.25 + ((epoch/30) +  (random.random()/10))\n",
    "  loss = 0.2 + (1 - ((epoch-1)/10 +  random.random()/5))\n",
    "  return acc, loss\n",
    "\n",
    "def evaluate_one_epoch(epoch): \n",
    "  acc = 0.1 + ((epoch/20) +  (random.random()/10))\n",
    "  loss = 0.25 + (1 - ((epoch-1)/10 +  random.random()/6))\n",
    "  return acc, loss\n",
    "\n",
    "def main():\n",
    "    run = wandb.init()\n",
    "\n",
    "    # note that we define values from `wandb.config` instead \n",
    "    # of defining hard values\n",
    "    lr  =  wandb.config.lr\n",
    "    bs = wandb.config.batch_size\n",
    "    epochs = wandb.config.epochs\n",
    "\n",
    "    for epoch in np.arange(1, epochs):\n",
    "      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n",
    "      val_acc, val_loss = evaluate_one_epoch(epoch)\n",
    "\n",
    "      wandb.log({\n",
    "        'epoch': epoch, \n",
    "        'train_acc': train_acc,\n",
    "        'train_loss': train_loss, \n",
    "        'val_acc': val_acc, \n",
    "        'val_loss': val_loss\n",
    "      })\n",
    "\n",
    "# Start sweep job.\n",
    "wandb.agent(sweep_id, function=main, count=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default spaCy:\n",
    "Ταῦτ’ -> οὗτος\n",
    "οὖν -> οὖν\n",
    "εἴς -> εἰς\n",
    "τε -> τε\n",
    "τὸν -> ὁ\n",
    "παρόντα -> πάρειμι\n",
    "καὶ -> καί\n",
    "τὸν -> ὁ\n",
    "ἑξῆς -> ἑξής\n",
    "ἅπαντα -> ἅπας\n",
    "λόγον -> λόγος\n",
    "οἷον -> οἷος\n",
    "ὑποθέσεις -> ὑπόθεσις\n",
    "τινὰς -> τις\n",
    "τῶν -> ὁ\n",
    "ἀποδείξεων -> ἀπόδειξις\n",
    "λαμβάνοντες -> λαμβάνω\n",
    "ἐν -> ἐν\n",
    "\n",
    "Grecy:\n",
    "Ταῦτ’ -> οὗτος\n",
    "οὖν -> οὖν\n",
    "εἴς -> εἰς\n",
    "τε -> τε\n",
    "τὸν -> ὁ\n",
    "παρόντα -> πάρειμι\n",
    "καὶ -> καί\n",
    "τὸν -> ὁ\n",
    "ἑξῆς -> ἑξής\n",
    "ἅπαντα -> ἅπας\n",
    "λόγον -> λόγος\n",
    "οἷον -> οἷος\n",
    "ὑποθέσεις -> ὑπόθεσις\n",
    "τινὰς -> τις"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, DocBin, Span\n",
    "train_data = DocBin().from_disk('../Corpus/train.spacy')\n",
    "train_data_docs = list(train_data.get_docs(nlp.vocab))\n",
    "dev_data = DocBin().from_disk('../Corpus/dev.spacy')\n",
    "dev_data_docs = list(dev_data.get_docs(nlp.vocab))\n",
    "test_data = DocBin().from_disk('../Corpus/test.spacy')\n",
    "test_data_docs = list(test_data.get_docs(nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "φλέψ = \"φλέψ\"\n",
    "#normalize texwor\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "φλέψ = normalize_text(φλέψ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texwor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textartery = 'ἀρτηρία'\n",
    "textartery = normalize_text(textartery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in train_data_docs_norm:\n",
    "    docced = nlp(doc)\n",
    "    # if \"ὥσπερ αἱ φλέβες\" is in the text of the doc, then print the text and the lemma\n",
    "    if \"φλέβες\" in docced.text:\n",
    "        print(docced.text)\n",
    "        for token in docced:\n",
    "            print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in train_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    for token in docced:\n",
    "        if token.lemma_ == texwor:\n",
    "            print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "        #if token.lemma_ == textartery:\n",
    "          #  print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in dev_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    for token in docced:\n",
    "        #if token.lemma_ == texwor:\n",
    "        #    print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "        if token.lemma_ == textartery:\n",
    "            print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    for token in docced:\n",
    "        if token.lemma_ == texwor:\n",
    "            print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "            #displacy.render(docced, style='ent', jupyter=True)\n",
    "        #if token.lemma_ == textartery:\n",
    "        #    print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "            # displacy\n",
    "        #    displacy.render(docced, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    for token in docced:\n",
    "        print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    # if entity is Body Part then print the sentence and move to the next sentnce\n",
    "    for ent in docced.ents:\n",
    "        if ent.label_ ==\"Body Part\":\n",
    "            print(docced, ' | ', ent.text, ent.label_, token, token.lemma_)\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test body part and lemma finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Προϊόντι γὰρ καὶ καταβαίνοντι τῷ περιττώματι εὐρυχωρία γίνεται, καὶ πρὸς τὸ μεταβάλλειν ἱσταμένῳ τοῖς εὐχιλοτέροις τῶν ζῴων καὶ πλείονος δεομένοις τροφῆς, διὰ τὸ μέγεθος ἢ τὴν θερμότητα τῶν τόπων. Εἶτ' ἐντεῦθεν πάλιν, ὥσπερ ἀπὸ τῆς ἄνω κοιλίας δέχεται στενώτερον ἔντερον, οὕτως ἐκ τοῦ κώλου καὶ τῆς εὐρυχωρίας ἐν τῇ κάτω κοιλίᾳ πάλιν εἰς στενώτερον ἔρχεται καὶ εἰς τὴν ἕλικα τὸ περίττωμα ἐξικμασμένον πάμπαν.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"Body Part\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
    "options = {\"ents\": [\"Body Part\"], \"colors\": colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"../training/NER2/model-best\")\n",
    "nlp_lemmatizer = spacy.load(\"../training/transformer/lemmatizer/model-best\")\n",
    "\n",
    "nlp.add_pipe('lemmatizer', source=nlp_lemmatizer, before='trainable_lemmatizer')\n",
    "nlp.remove_pipe('trainable_lemmatizer')\n",
    "\n",
    "nlp.remove_pipe('transformer')\n",
    "nlp.add_pipe('transformer', source=nlp_lemmatizer, before='morphologizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"../training/NER2/model-best\")\n",
    "nlp = spacy.load(\"../training/transformer/lemmatizer/model-best\")\n",
    "\n",
    "nlp.add_pipe('ner', source=nlp_ner, last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to attribute ruler that when the lemma is \"κοιλία\" the entity label is \"Body Part\"\n",
    "ruler = nlp.add_pipe(\"attribute_ruler\", before=\"ner\")\n",
    "patterns = [[{\"LEMMA\": \"κοιλία\"}]]\n",
    "attrs = {\"ENT_TYPE\": \"BodyPart\"}\n",
    "ruler.add(patterns=patterns, attrs=attrs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../assets/Evaluations/Aristotle_Partibus_Animalium_675a31-6a5.txt\", encoding='utf8') as text:\n",
    "    text = text.read()\n",
    "    docs = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.tokens import Doc, DocBin, Span\n",
    "#test_data = DocBin().from_disk(\"../Archive/corpus_morefiles_model/test/test.spacy\")\n",
    "#docs = list(test_data.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Μετὰ δὲ τὴν κοιλίαν ἡ τῶν ἐντέρων ἔγκειται φύσις πᾶσι τοῖς ζῴοις. Ἔχει δὲ διαφορὰς πολλάς, καθάπερ ἡ κοιλία, καὶ τοῦτο τὸ μόριον. Τοῖς μὲν γὰρ ἁπλοῦν ἐστι καὶ ὅμοιον ἀναλυόμενον, τοῖς δ' ἀνόμοιον· ἐνίοις μὲν γὰρ εὐρύτερον τὸ πρὸς τῇ κοιλίᾳ, τὸ δὲ πρὸς τῷ τέλει στενώτερον (διόπερ αἱ κύνες μετὰ πόνου προΐενται τὴν τοιαύτην περίττωσιν), τοῖς δὲ πλείοσιν ἄνωθεν στενώτερον, πρὸς τῷ τέλει δ' εὐρύτερον.[…] Πᾶσι δὲ τοῖς μὴ εὐθυεντέροις προϊοῦσιν εὐρύτερον γίνεται τὸ μόριον τοῦτο, καὶ τὸ καλούμενον κόλον ἔχουσι, καὶ τοῦ ἐντέρου τυφλόν τι καὶ ὀγκῶδες, εἶτ' ἐκ τούτου πάλιν στενώτερον καὶ εἱλιγμένον. Τὸ δὲ μετὰ τοῦτο εὐθὺ πρὸς τὴν ἔξοδον διατείνει τοῦ περιττώματος, καὶ τοῖς μὲν τοῦτο τὸ μόριον, ὁ καλούμενος ἀρχός, κνισσώδης ἐστί, τοῖς δ' ἀπίμελος.\"\n",
    "#text = \"Ὑπὸ δὲ τὸ ὑπόζωμα κεῖται ἡ κοιλία τοῖς ζῴοις, τοῖς μὲν ἔχουσιν οἰσοφάγον ᾗ τελευτᾷ τοῦτο τὸ μόριον, τοῖς δὲ μὴ ἔχουσιν εὐθὺς πρὸς τῷ στόματι· τῆς δὲ κοιλίας ἐχόμενον τὸ καλούμενον ἔντερον. Δι' ἣν δ' αἰτίαν ἔχει ταῦτα τὰ μόρια τῶν ζῴων ἕκαστον, φανερὸν πᾶσιν. Καὶ γὰρ δέξασθαι τὴν εἰσελθοῦσαν τροφὴν καὶ τὴν ἐξικμασμένην ἀναγκαῖον ἐκπέμψαι, καὶ μὴ τὸν αὐτὸν τόπον εἶναι τῆς τε ἀπέπτου καὶ τοῦ περιττώματος, εἶναί τέ τινα δεῖ τόπον ἐν ᾧ μεταβάλλει.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add BodyPart label to ner in pipeline\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"BodyPart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner.pipe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/transformer/NER/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the text\n",
    "doc = nlp(text)\n",
    "# parse to sentences with spacy\n",
    "for token in doc:\n",
    "    print (token.text, token.ent_type_)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the text\n",
    "doc = nlp(text)\n",
    "# parse to sentences with spacy\n",
    "for token in doc:\n",
    "    print (token.text, token.ent_type_)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.lemma_ == \"κοιλία\":\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docced = nlp(text)\n",
    "for token in docced:\n",
    "    if token.lemma_ == \"κοιλία\":\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docced = nlp(text)\n",
    "for sent in docced:\n",
    "    print(sent.text, sent.lemma_, sent.pos_, sent.tag_, sent.dep_, sent.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in docced:\n",
    "        displacy.render(docced, style='ent', jupyter=True, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lemma in doclines sentences. If found, print sentence and move to next sentence\n",
    "docced = nlp(text)\n",
    "#for doc in docs:\n",
    "#    docced = nlp(doc)\n",
    "for token in docced:\n",
    "    #print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "    if token.ent_type_ == \"Body Part\" and token.lemma_ == \"κοιλία\":\n",
    "        print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "        #print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "        #print(docced, token.lemma_, token.ent_type_)\n",
    "        displacy.render(docced, style='ent', jupyter=True, options=options)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in doclines:\n",
    "    #docced = nlp(doc)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in doclines:\n",
    "    #docced = nlp(doc)\n",
    "    for token in doc:\n",
    "        #find lemma\n",
    "        if token.ent_type_ == \"Body Part\" and token.lemma_ == φλέψ:\n",
    "            displacy.render(doc, style='ent', jupyter=True, options=options)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in docs:\n",
    "    #find lemma of κοιλία and print the sentence\n",
    "    if token.lemma_ == 'κοιλία':\n",
    "        displacy.render(token.sents, style='ent', jupyter=True, options=options)\n",
    "        break\n",
    "    \n",
    "    if token.ent_type_ == \"Body Part\" and token.lemma_ == κοιλία:\n",
    "        displacy.render(docs.sents, style='ent', jupyter=True, options=options)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    for token in docced:\n",
    "        #find lemma\n",
    "        if token.ent_type_ == \"Body Part\" and token.lemma_ == φλέψ:\n",
    "            displacy.render(docced, style='ent', jupyter=True, options=options)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_data_docs:\n",
    "    docced = nlp(doc)\n",
    "    for token in docced:\n",
    "        # if entity is \"Body Part\" then print the text\n",
    "        if token.ent_type_ == \"Body Part\":\n",
    "            displacy.render(docced, style='ent', jupyter=True)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy package ../Models/grc_ud_proiel_trf/grc_ud_proiel_trf_Lem_NER/model-best ../Models/grc_ud_proiel_trf/trf_Lem_NER --name trf_Lem_NER --version 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../Models/grc_ud_proiel_trf/grc_ud_proiel_trf_Lem_NER/model-best\")\n",
    "\n",
    "\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp = spacy.load(output_dir)\n",
    "# disable parser\n",
    "#nlp.disable_pipes(\"parser\")\n",
    "\n",
    "nlp2 = spacy.load(\"el_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = 'Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι. ἐπειδὴ γὰρ τὴν τῶν ὀστῶν κατασκευὴν ἐν αὐτοῖς ἐπιτηδειοτάτην ὀργάνοις ἀντιληπτικοῖς ἡ φύσις ἐποίησεν, ἦν δ’ ἀμήχανον αὐτοῖς τοῖς ὀστοῖς οὕτω γεώδεσί τε καὶ λιθώδεσιν οὖσι μεταδοῦναι τῆς καθ’ ὁρμὴν κινήσεως, ἐξεῦρεν, ὅτῳ τρόπῳ δι’ ἑτέρων αὐτὰ κινήσει. τῶν οὖν κατὰ τὸν πῆχυν μυῶν ἀποφύσασα τένοντας εὐθὺ τῶν δακτύλων ἤγαγεν. ἃ γὰρ οἱ παλαιοὶ καλοῦσι νεῦρα, ταυτὶ τὰ προφανῆ, τὰ κινοῦντα τοὺς δακτύλους, οἱ τένοντές εἰσιν·'\n",
    "#paragraph = normalize_text(paragraph)\n",
    "\n",
    "docced = nlp(paragraph)\n",
    "# print sentences\n",
    "#for sent in docced.sents:\n",
    "    #print(sent, ' | ', \"sentence\")\n",
    "\n",
    "#for doc in docced:\n",
    "#    print(doc.text, doc.pos_, doc.dep_, doc.ent_type_, doc.lemma_)\n",
    "for token in docced:\n",
    "#    if token.lemma_ == texwor:\n",
    "    #if token.pos_ == 'PUNCT':\n",
    "        print(token, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "        #displacy.render(docced, style='ent', jupyter=True)\n",
    "    #if token.lemma_ == textartery:\n",
    "    #    print(docced, ' | ', token.text, token.pos_, token.dep_, token.ent_type_, token.lemma_)\n",
    "        # displacy\n",
    "    #displacy.render(sent, style='dep', jupyter=True)\n",
    "    #for token in sent:\n",
    "    #    print(sent, ' | ', token.text, token.pos_, ' | ',token.dep_, token.ent_type_, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6ChLyYk1pmVs68IB/yCSC",
   "name": "Spacy-TOPO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
