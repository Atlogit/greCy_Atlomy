{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T08:11:16.631297200Z",
     "start_time": "2023-07-20T08:11:07.739576900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import DocBin\n",
    "from conllu import parse\n",
    "import os\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T08:11:20.157065900Z",
     "start_time": "2023-07-20T08:11:16.632300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load ConllU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:18.920032400Z",
     "start_time": "2023-07-20T09:12:18.871959400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SOURCES = [\"../assets/INCEpTION_Conllu/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "for source in SOURCES:\n",
    "    for root, dirs, files in os.walk(source):\n",
    "        for file in files:\n",
    "            if file.endswith(\".conllu\"):\n",
    "                # print file name\n",
    "                print(file)\n",
    "                with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    data += f.read()\n",
    "                    #print(\"Read\", os.path.join(root, file))\n",
    "                    #print(\"Length of data:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_list = ['NFKD', 'NFKC']\n",
    "for NORM in NORM_list:\n",
    "    data = unicodedata.normalize(NORM, data)\n",
    "    sentences = parse(data)\n",
    "\n",
    "    docs = []\n",
    "    apostrophes = [' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\"]  # all possible apostrophes\n",
    "    \n",
    "    for s in sentences:\n",
    "        #words = [t['form'] for t in s]\n",
    "        words = [\"ʼ\" if t['form'] in apostrophes else t['form'] for t in s]\n",
    "        print ('words: ',words)\n",
    "        # spaces are always True unless in t['misc'] and ['SpaceAfter'] is 'No'\n",
    "        spaces = [True for t in s]\n",
    "        for i, t in enumerate(s):\n",
    "            if t['misc'] and t['misc']['SpaceAfter'] == 'No':\n",
    "                spaces[i] = False\n",
    "        #doc = Doc(nlp.vocab, words=[t['form'] for t in s], spaces=spaces)\n",
    "        doc = Doc(nlp.vocab, words=[\"ʼ\" if t['form'] in apostrophes else t['form'] for t in s], spaces=spaces)\n",
    "        print (doc, spaces)\n",
    "        # add tags to doc\n",
    "        for i, t in enumerate(s):\n",
    "            if t['upos'] != None:\n",
    "                # if t['form'] is a punctuation mark, t['upos'] is 'PUNCT'\n",
    "                if t['form'] in string.punctuation:\n",
    "                    doc[i].pos_ = 'PUNCT'\n",
    "                else:\n",
    "                    doc[i].pos_ = '' if t['upos'] == '_' else t['upos']\n",
    "            if t['xpos'] != None:\n",
    "                doc[i].tag_ = t['xpos']\n",
    "            if t['lemma'] != None:\n",
    "                doc[i].lemma_ = '' if t['lemma'] == '_' else t['lemma']\n",
    "        docs.append(doc)\n",
    "\n",
    "    for doc in docs:\n",
    "        print(f\"Source file: {doc}\",)\n",
    "        #cleaned_sentence = str(doc)\n",
    "        cleaned_sentence = ' '.join(str(doc).replace('\\r', ' ').replace('\\n', ' ').split())\n",
    "        print(f\"Cleaned sentence: {cleaned_sentence}\")        \n",
    "            \n",
    "    # split docs to train, dev, test randomly\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pathlib import Path\n",
    "\n",
    "    # split docs to train, dev, test randomly, for each normalization\n",
    "    train_docs_norm, test_docs_norm = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "    train_docs_norm, dev_docs_norm = train_test_split(train_docs_norm, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #print count of docs and characters in each set\n",
    "    print (\"{0}\\n\".format(NORM) + f\"train: {len(train_docs_norm)} ({len(''.join([doc.text_with_ws for doc in train_docs_norm]))} characters)\\ndev: {len(dev_docs_norm)} ({len(''.join([doc.text_with_ws for doc in dev_docs_norm]))} characters)\\ntest: {len(test_docs_norm)} ({len(''.join([doc.text_with_ws for doc in test_docs_norm]))} characters)\")\n",
    "\n",
    "    Path(\"../corpus/train/pos_train\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../corpus/dev/pos_dev\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../corpus/test/pos_test\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_bin_norm = DocBin(docs=train_docs_norm)\n",
    "    train_bin_norm.to_disk(\"../corpus/train/pos_train/pos_train_\" + \"{0}.spacy\".format(NORM))\n",
    "    test_bin_norm = DocBin(docs=test_docs_norm)\n",
    "    test_bin_norm.to_disk(\"../corpus/test/pos_test/pos_test_\" + \"{0}.spacy\".format(NORM))\n",
    "    dev_bin_norm = DocBin(docs=dev_docs_norm)\n",
    "    dev_bin_norm.to_disk(\"../corpus/dev/pos_dev/pos_dev_\" + \"{0}.spacy\".format(NORM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    # print tokens and spaces\n",
    "    print(f\"Tokens: {[t.text for t in doc]}\")\n",
    "    print(f\"Spaces: {[t.whitespace_ for t in doc]}\")\n",
    "    # print tags\n",
    "    print(f\"Tags: {[t.pos_ for t in doc]}\")\n",
    "    print(f\"Tags: {[t.tag_ for t in doc]}\")\n",
    "    print(f\"Tags: {[t.lemma_ for t in doc]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:24.751693500Z",
     "start_time": "2023-07-20T09:12:24.735592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print first sentence with POS\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:25.646718Z",
     "start_time": "2023-07-20T09:12:25.555326800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get tokens of first sentence\n",
    "print(docs[24])\n",
    "tokens = [t for t in docs[24]]\n",
    "for t in tokens:\n",
    "    print(t.text, t.pos_, t.tag_, t.lemma_, t.whitespace_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Spacy docbin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load spacy object\n",
    "# load docs from file\n",
    "docs = DocBin().from_disk(\"../corpus/train/pos_train/pos_train_NFKD.spacy\")\n",
    "test_docbin_docs = list(docs.get_docs(nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_docbin_docs:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create list of rows\n",
    "rows = []\n",
    "for doc in test_docbin_docs:\n",
    "    for token in doc:\n",
    "        row = [token.orth_, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.orth_]\n",
    "        rows.append(row)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(rows, columns=[\"Orth\", \"Lemma\", \"POS\", \"Tag\", \"Dep\", \"Head\"])\n",
    "\n",
    "# print dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in test_docbin_docs[0]:\n",
    "#print attributes\n",
    "    print('text: ', token.text, 'lemma :', token.lemma_, 'POS: ', token.pos_, 'tag: ', token.tag_, 'DEP: ', token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first doc in spacy docbin docs\n",
    "print(test_docbin_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
