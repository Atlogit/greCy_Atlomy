{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T08:11:16.631297200Z",
     "start_time": "2023-07-20T08:11:07.739576900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import DocBin\n",
    "from conllu import parse\n",
    "import os\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T08:11:20.157065900Z",
     "start_time": "2023-07-20T08:11:16.632300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load ConllU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:18.920032400Z",
     "start_time": "2023-07-20T09:12:18.871959400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SOURCES = [\"../assets/INCEpTION_Conllu/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "for source in SOURCES:\n",
    "    for root, dirs, files in os.walk(source):\n",
    "        for file in files:\n",
    "            if file.endswith(\".conllu\"):\n",
    "                with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    data += f.read()\n",
    "                    #print(\"Read\", os.path.join(root, file))\n",
    "                    #print(\"Length of data:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFKD\n",
      "train: 107 (13380 characters)\n",
      "dev: 27 (4163 characters)\n",
      "test: 34 (4553 characters)\n",
      "NFKC\n",
      "train: 107 (10952 characters)\n",
      "dev: 27 (3413 characters)\n",
      "test: 34 (3736 characters)\n"
     ]
    }
   ],
   "source": [
    "NORM_list = ['NFKD', 'NFKC']\n",
    "for NORM in NORM_list:\n",
    "    data = unicodedata.normalize(NORM, data)\n",
    "    sentences = parse(data)\n",
    "\n",
    "    docs = []\n",
    "    for s in sentences:\n",
    "        words = [t['form'] for t in s]\n",
    "        # spaces are always True unless in t['misc'] and ['SpaceAfter'] is 'No'\n",
    "        spaces = [True for t in s]\n",
    "        for i, t in enumerate(s):\n",
    "            if t['misc'] and t['misc']['SpaceAfter'] == 'No':\n",
    "                spaces[i] = False\n",
    "        doc = Doc(nlp.vocab, words=[t['form'] for t in s], spaces=spaces)\n",
    "        # add tags to doc\n",
    "        for i, t in enumerate(s):\n",
    "            if t['upos'] != None:\n",
    "                # if t['form'] is a punctuation mark, t['upos'] is 'PUNCT'\n",
    "                if t['form'] in string.punctuation:\n",
    "                    doc[i].pos_ = 'PUNCT'\n",
    "                else:\n",
    "                    doc[i].pos_ = '' if t['upos'] == '_' else t['upos']\n",
    "            if t['xpos'] != None:\n",
    "                doc[i].tag_ = t['xpos']\n",
    "            if t['lemma'] != None:\n",
    "                doc[i].lemma_ = '' if t['lemma'] == '_' else t['lemma']\n",
    "        docs.append(doc)\n",
    "        \n",
    "        \n",
    "    # split docs to train, dev, test randomly\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pathlib import Path\n",
    "\n",
    "    # split docs to train, dev, test randomly, for each normalization\n",
    "    train_docs_norm, test_docs_norm = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "    train_docs_norm, dev_docs_norm = train_test_split(train_docs_norm, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #print count of docs and characters in each set\n",
    "    print (\"{0}\\n\".format(NORM) + f\"train: {len(train_docs_norm)} ({len(''.join([doc.text_with_ws for doc in train_docs_norm]))} characters)\\ndev: {len(dev_docs_norm)} ({len(''.join([doc.text_with_ws for doc in dev_docs_norm]))} characters)\\ntest: {len(test_docs_norm)} ({len(''.join([doc.text_with_ws for doc in test_docs_norm]))} characters)\")\n",
    "\n",
    "    Path(\"../corpus/train/pos_train\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../corpus/dev/pos_dev\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../corpus/test/pos_test\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_bin_norm = DocBin(docs=train_docs_norm)\n",
    "    train_bin_norm.to_disk(\"../corpus/train/pos_train/pos_train_\" + \"{0}.spacy\".format(NORM))\n",
    "    test_bin_norm = DocBin(docs=test_docs_norm)\n",
    "    test_bin_norm.to_disk(\"../corpus/test/pos_test/pos_test_\" + \"{0}.spacy\".format(NORM))\n",
    "    dev_bin_norm = DocBin(docs=dev_docs_norm)\n",
    "    dev_bin_norm.to_disk(\"../corpus/dev/pos_dev/pos_dev_\" + \"{0}.spacy\".format(NORM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = \"NFKD\"\n",
    "data = unicodedata.normalize(NORM, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:20.698982100Z",
     "start_time": "2023-07-20T09:12:20.687507200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:22.047409200Z",
     "start_time": "2023-07-20T09:12:21.984948200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][9].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:22.165682400Z",
     "start_time": "2023-07-20T09:12:22.165190100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[0][8]['misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:22.659385800Z",
     "start_time": "2023-07-20T09:12:22.648781900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Αἱ',\n",
       " 'δ',\n",
       " '’',\n",
       " 'ὑστέραι',\n",
       " 'τῶν',\n",
       " 'ἐχόντων',\n",
       " 'ὑστέρας',\n",
       " 'ζῴων',\n",
       " 'οὔτε',\n",
       " 'τὸν',\n",
       " 'αὐτὸν',\n",
       " 'τρόπον',\n",
       " 'ἔχουσιν',\n",
       " 'οὔθ',\n",
       " '’',\n",
       " 'ὅμοιαι',\n",
       " 'πάντων',\n",
       " 'εἰσίν',\n",
       " ',',\n",
       " 'ἀλλὰ',\n",
       " 'διαφέρουσι',\n",
       " 'καὶ',\n",
       " 'τῶν',\n",
       " 'ζῳοτοκούντων',\n",
       " 'πρὸς',\n",
       " 'ἄλληλα',\n",
       " 'καὶ',\n",
       " 'τῶν',\n",
       " 'ᾠοτοκούντων',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tokens from first sentence\n",
    "tokens = [t['form'] for t in sentences[0]]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:23.923773600Z",
     "start_time": "2023-07-20T09:12:23.913293800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "docs = []\n",
    "for s in sentences:\n",
    "    words = [t['form'] for t in s]\n",
    "    # spaces are always True unless in t['misc'] and ['SpaceAfter'] is 'No'\n",
    "    spaces = [True for t in s]\n",
    "    for i, t in enumerate(s):\n",
    "        if t['misc'] and t['misc']['SpaceAfter'] == 'No':\n",
    "            spaces[i] = False\n",
    "    doc = Doc(nlp.vocab, words=[t['form'] for t in s], spaces=spaces)\n",
    "    # add tags to doc\n",
    "    for i, t in enumerate(s):\n",
    "        if t['upos'] != None:\n",
    "            # if t['form'] is a punctuation mark, t['upos'] is 'PUNCT'\n",
    "            if t['form'] in string.punctuation:\n",
    "                doc[i].pos_ = 'PUNCT'\n",
    "            else:\n",
    "                doc[i].pos_ = '' if t['upos'] == '_' else t['upos']\n",
    "        if t['xpos'] != None:\n",
    "            doc[i].tag_ = t['xpos']\n",
    "        if t['lemma'] != None:\n",
    "            doc[i].lemma_ = '' if t['lemma'] == '_' else t['lemma']\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:24.751693500Z",
     "start_time": "2023-07-20T09:12:24.735592Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αἱ δ’ ὑστέραι τῶν ἐχόντων ὑστέρας ζῴων οὔτε τὸν αὐτὸν τρόπον ἔχουσιν οὔθ’ ὅμοιαι πάντων εἰσίν, ἀλλὰ διαφέρουσι καὶ τῶν ζῳοτοκούντων πρὸς ἄλληλα καὶ τῶν ᾠοτοκούντων. \n"
     ]
    }
   ],
   "source": [
    "# print first sentence with POS\n",
    "print(docs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:25.646718Z",
     "start_time": "2023-07-20T09:12:25.555326800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αἱ DET  ὁ  \n",
      "δ CCONJ  δέ \n",
      "’     \n",
      "ὑστέραι NOUN  ὑστέρα  \n",
      "τῶν DET  ὁ  \n",
      "ἐχόντων VERB  ἔχω  \n",
      "ὑστέρας NOUN  ὑστέρα  \n",
      "ζῴων NOUN  ζῷον  \n",
      "οὔτε CCONJ  οὔτε  \n",
      "τὸν DET  ὁ  \n",
      "αὐτὸν PRON  αὐτός  \n",
      "τρόπον NOUN  τρόπος  \n",
      "ἔχουσιν VERB  ἔχω  \n",
      "οὔθ CCONJ  οὔθ \n",
      "’     \n",
      "ὅμοιαι ADJ  ὅμοιος  \n",
      "πάντων ADJ  πᾶς  \n",
      "εἰσίν VERB  εἰμί \n",
      ", PUNCT    \n",
      "ἀλλὰ CCONJ  ἀλλὰ  \n",
      "διαφέρουσι VERB  διαφέρω  \n",
      "καὶ CCONJ  καί  \n",
      "τῶν DET  ὁ  \n",
      "ζῳοτοκούντων VERB  ζᾠοτοκέω  \n",
      "πρὸς ADP  πρός  \n",
      "ἄλληλα PRON  ἀλλήλων  \n",
      "καὶ CCONJ  καί  \n",
      "τῶν DET  ὁ  \n",
      "ᾠοτοκούντων VERB  ᾠοτοκέω \n",
      ". PUNCT    \n"
     ]
    }
   ],
   "source": [
    "# get tokens of first sentence\n",
    "tokens = [t for t in docs[0]]\n",
    "for t in tokens:\n",
    "    print(t.text, t.pos_, t.tag_, t.lemma_, t.whitespace_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save docs to binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split docs to train, dev, test randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# split docs to train, dev, test randomly, for each normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 107\n",
      "dev: 27\n",
      "test: 34 for nfkd\n"
     ]
    }
   ],
   "source": [
    "train_docs_nfkd, test_docs_nfkd = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "train_docs_nfkd, dev_docs_nfkd = train_test_split(train_docs_nfkd, test_size=0.2, random_state=42)\n",
    "\n",
    "#train_docs_nfkc, test_docs_nfkc = train_test_split(docs_nfkc, test_size=0.2, random_state=42)\n",
    "#train_docs_nfkc, dev_docs_nfkc = train_test_split(train_docs_nfkc, test_size=0.2, random_state=42)\n",
    "\n",
    "print (f\"train: {len(train_docs_nfkd)}\\ndev: {len(dev_docs_nfkd)}\\ntest: {len(test_docs_nfkd)} for nfkd\")\n",
    "#print (f\"train: {len(train_docs_nfkc)}\\ndev: {len(dev_docs_nfkc)}\\ntest: {len(test_docs_nfkc)} for nfkc\")\n",
    "# save each one to DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:55.390442200Z",
     "start_time": "2023-07-20T09:12:55.370410600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:55.848116100Z",
     "start_time": "2023-07-20T09:12:55.831433Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_bin = DocBin(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:56.376152700Z",
     "start_time": "2023-07-20T09:12:56.356672Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_bin.to_disk(\"../corpus/INCEpTION_POS_NFKD.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Spacy docbin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load spacy object\n",
    "# load docs from file\n",
    "docs = DocBin().from_disk(\"../corpus//train/pos_train/pos_train_NFKD.spacy\")\n",
    "test_docbin_docs = list(docs.get_docs(nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Orth    Lemma    POS Tag Dep          Head\n",
      "0         Ἔχουσι    ἔχω   VERB              Ἔχουσι\n",
      "1             γὰρ     γάρ  CCONJ                  γὰρ\n",
      "2              οἱ       ὁ    DET                   οἱ\n",
      "3          νεφροὶ  νεφρός   NOUN               νεφροὶ\n",
      "4              ἐν      ἐν    ADP                   ἐν\n",
      "...            ...      ...    ...  ..  ..           ...\n",
      "2139    δίκραιός                            δίκραιός\n",
      "2140         ἐστι                                 ἐστι\n",
      "2141      Τοιῇδε                              Τοιῇδε\n",
      "2142  δικραιότητε            NOUN          δικραιότητε\n",
      "2143             ·                                     ·\n",
      "\n",
      "[2144 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create list of rows\n",
    "rows = []\n",
    "for doc in test_docbin_docs:\n",
    "    for token in doc:\n",
    "        row = [token.orth_, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.orth_]\n",
    "        rows.append(row)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(rows, columns=[\"Orth\", \"Lemma\", \"POS\", \"Tag\", \"Dep\", \"Head\"])\n",
    "\n",
    "# print dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orth</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ἔχουσι</td>\n",
       "      <td>ἔχω</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Ἔχουσι</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>γὰρ</td>\n",
       "      <td>γάρ</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>γὰρ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>οἱ</td>\n",
       "      <td>ὁ</td>\n",
       "      <td>DET</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>οἱ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>νεφροὶ</td>\n",
       "      <td>νεφρός</td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>νεφροὶ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ἐν</td>\n",
       "      <td>ἐν</td>\n",
       "      <td>ADP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ἐν</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>δίκραιός</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>δίκραιός</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>ἐστι</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ἐστι</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>Τοιῇδε</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Τοιῇδε</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>δικραιότητε</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>δικραιότητε</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>·</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>·</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2144 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Orth    Lemma    POS Tag Dep          Head\n",
       "0         Ἔχουσι    ἔχω   VERB              Ἔχουσι\n",
       "1             γὰρ     γάρ  CCONJ                  γὰρ\n",
       "2              οἱ       ὁ    DET                   οἱ\n",
       "3          νεφροὶ  νεφρός   NOUN               νεφροὶ\n",
       "4              ἐν      ἐν    ADP                   ἐν\n",
       "...            ...      ...    ...  ..  ..           ...\n",
       "2139    δίκραιός                            δίκραιός\n",
       "2140         ἐστι                                 ἐστι\n",
       "2141      Τοιῇδε                              Τοιῇδε\n",
       "2142  δικραιότητε            NOUN          δικραιότητε\n",
       "2143             ·                                     ·\n",
       "\n",
       "[2144 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  Ἔχουσι lemma : ἔχω POS:  VERB tag:   DEP:  \n",
      "text:  γὰρ lemma : γάρ POS:  CCONJ tag:   DEP:  \n",
      "text:  οἱ lemma : ὁ POS:  DET tag:   DEP:  \n",
      "text:  νεφροὶ lemma : νεφρός POS:  NOUN tag:   DEP:  \n",
      "text:  ἐν lemma : ἐν POS:  ADP tag:   DEP:  \n",
      "text:  μέσῳ lemma : μέσος POS:  ADJ tag:   DEP:  \n",
      "text:  κοῖλον lemma : κοῖλον POS:  NOUN tag:   DEP:  \n",
      "text:  , lemma :  POS:  PUNCT tag:   DEP:  \n",
      "text:  οἱ lemma : ὁ POS:  DET tag:   DEP:  \n",
      "text:  μὲν lemma : μέν POS:  CCONJ tag:   DEP:  \n",
      "text:  μεῖζον lemma : μέγας POS:  ADJ tag:   DEP:  \n",
      "text:  οἱ lemma : ὁ POS:  DET tag:   DEP:  \n",
      "text:  δ lemma : δέ POS:  CCONJ tag:   DEP:  \n",
      "text:  ’ lemma :  POS:   tag:   DEP:  \n",
      "text:  ἔλαττον lemma : ἐλάσσων POS:  ADJ tag:   DEP:  \n",
      "text:  , lemma :  POS:  PUNCT tag:   DEP:  \n",
      "text:  πλὴν lemma : πλήν POS:  ADP tag:   DEP:  \n",
      "text:  οἱ lemma : ὁ POS:  DET tag:   DEP:  \n",
      "text:  τῆς lemma : ὁ POS:  DET tag:   DEP:  \n",
      "text:  φώκης lemma : φώκη POS:  NOUN tag:   DEP:  \n",
      "text:  · lemma :  POS:   tag:   DEP:  \n"
     ]
    }
   ],
   "source": [
    "for token in test_docbin_docs[0]:\n",
    "#print attributes\n",
    "    print('text: ', token.text, 'lemma :', token.lemma_, 'POS: ', token.pos_, 'tag: ', token.tag_, 'DEP: ', token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ἔχουσι γὰρ οἱ νεφροὶ ἐν μέσῳ κοῖλον, οἱ μὲν μεῖζον οἱ δ’ ἔλαττον, πλὴν οἱ τῆς φώκης· \n"
     ]
    }
   ],
   "source": [
    "# print the first doc in spacy docbin docs\n",
    "print(test_docbin_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_docbin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/Projects/Atlomy/git/greCy_ATLOMY/src/greek_POS_conllu.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/Projects/Atlomy/git/greCy_ATLOMY/src/greek_POS_conllu.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# print docs in new docbin\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/Projects/Atlomy/git/greCy_ATLOMY/src/greek_POS_conllu.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m new_docbin\u001b[39m.\u001b[39mget_docs(nlp\u001b[39m.\u001b[39mvocab):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/Projects/Atlomy/git/greCy_ATLOMY/src/greek_POS_conllu.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/Projects/Atlomy/git/greCy_ATLOMY/src/greek_POS_conllu.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(token\u001b[39m.\u001b[39mtext, token\u001b[39m.\u001b[39mlemma_, token\u001b[39m.\u001b[39mpos_, token\u001b[39m.\u001b[39mtag_, token\u001b[39m.\u001b[39mdep_, token\u001b[39m.\u001b[39mshape_, token\u001b[39m.\u001b[39mis_alpha, token\u001b[39m.\u001b[39mis_stop)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_docbin' is not defined"
     ]
    }
   ],
   "source": [
    "# print docs in new docbin\n",
    "for doc in new_docbin.get_docs(nlp.vocab):\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
