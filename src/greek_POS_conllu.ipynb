{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T08:11:16.631297200Z",
     "start_time": "2023-07-20T08:11:07.739576900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# SpaCy for NLP tasks\n",
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin\n",
    "\n",
    "# Parsing CoNLL-U formatted files\n",
    "from conllu import parse\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T08:11:20.157065900Z",
     "start_time": "2023-07-20T08:11:16.632300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"grc_proiel_trf\"\n",
    "\n",
    "# Check if the model is available and load it\n",
    "if model_name in spacy.info()['pipelines']:\n",
    "    nlp = spacy.load(model_name)\n",
    "else:\n",
    "    raise ImportError(f\"The SpaCy model '{model_name}' is not installed.\\n\"\n",
    "                      \"Please install it using 'python -m spacy download {model_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load CoNLL-U Data\n",
    "\n",
    "In this section, we will load and process text data in the CoNLL-U format. CoNLL-U is a standard for annotating text data with linguistic information, such as part-of-speech tags, syntactic dependencies, and morphological features. Our data represents annotated anatomical ancient texts, which is crucial for creating a model that performs well on anatomical texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "from conllu import parse\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from conllu import parse_incr\n",
    "from io import StringIO\n",
    "\n",
    "def remove_accents_to_lowercase(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing diacritics (accents), except for specific characters,\n",
    "    and converting it to lowercase.\n",
    "    \"\"\"\n",
    "    allowed_characters = [' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\", 'ʼ']  # Including the Greek apostrophe\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    try:\n",
    "        non_accent_chars = [c for c in unicodedata.normalize('NFKD', text) \n",
    "        if unicodedata.category(c) != 'Mn' or c in allowed_characters]\n",
    "    # Use str.lower() for converting to lowercase, which works for Unicode characters\n",
    "        return ''.join(non_accent_chars).lower()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # A more generic exception handling if unexpected errors occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return text\n",
    "    \n",
    "\n",
    "def normalize_optional_remove_accents(text: str, norm_form: str, apply_cleaning: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Normalize and optionally clean the given text using specified Unicode normalization form.\n",
    "    \"\"\"    \n",
    "    normalized_text = unicodedata.normalize(norm_form, text)\n",
    "    if apply_cleaning:\n",
    "        # Implement the remove_accents_to_lowercase logic to remove unwanted characters or clean text\n",
    "        text = remove_accents_to_lowercase(normalized_text) # Clean the text\n",
    "    return text\n",
    "\n",
    "def uniform_apostrophe(token, apostrophes=[' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\"]):\n",
    "    \"\"\"Replace specified apostrophes with a uniform representation.\"\"\"\n",
    "    return 'ʼ' if token in apostrophes else token\n",
    "\n",
    "def process_sentences(sentences_data, nlp, file_name, debug=False):\n",
    "    \"\"\"Converts sentences into spaCy's Doc objects.\"\"\"\n",
    "    #print(sentences_data) if debug else None\n",
    "    words = [uniform_apostrophe(t['form']) for t in sentences_data]\n",
    "    spaces = [not (t['misc'] and t['misc'].get('SpaceAfter') == 'No') for t in sentences_data]\n",
    "    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "    set_token_attributes(doc, sentences_data)\n",
    "    doc.user_data[\"source_info\"] = file_name\n",
    "    #print(doc, spaces) if debug else None \n",
    "    return doc\n",
    "\n",
    "def set_token_attributes(doc, sentence, debug=False):\n",
    "    \"\"\"Set attributes like POS tags and lemmas for each token in the doc.\"\"\"\n",
    "    root_token = None  # Initialize root token\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        t = sentence[i]\n",
    "        if t['form'] in string.punctuation:\n",
    "            doc[i].pos_ = 'PUNCT'\n",
    "        else:\n",
    "            doc[i].pos_ = '' if t['upos'] == '_' else t['upos']\n",
    "        doc[i].lemma_ = '' if t['lemma'] in ['', '_', '—', '-'] else t['lemma']\n",
    "        # Tagger adjustment - POS tags\n",
    "        if t['xpos']:\n",
    "            if t['xpos'] in ['', '_', '—', '-', 'X', 'END', 'MID']:\n",
    "                doc[i].tag_ = '' # Setting an empty string for the POS tag\n",
    "        if t['upos'] in ['', '_', '—', '-', 'X', 'END', 'MID']:\n",
    "            doc[i].pos_ = '' # Setting an empty string for the POS tag\n",
    "        else:\n",
    "            doc[i].pos_ = t['upos']\n",
    "        # Dependency parser adjustment, performed during Doc creation based on 'dep' and 'head' relation\n",
    "        if t['deprel'] in ['', '_', '—', '-']:\n",
    "            doc[i].dep_ = 'None'\n",
    "        else:\n",
    "            doc[i].dep_ = t['deprel']\n",
    "            print(\"dep: \", doc[i].dep_) if debug else None\n",
    "        # Identify the root token for later use\n",
    "        if t['deprel'] == 'root':\n",
    "            print(\"root token: \", root_token) if debug else None\n",
    "            root_token = doc[i]\n",
    "        else:\n",
    "            print (\"root token not found\") if debug else None\n",
    "    print(\"roooot: \", root_token if root_token else \"elsee None\") if debug else None\n",
    "    \n",
    "    # Once all other attributes are set, adjust the head\n",
    "    for i, token in enumerate(doc):\n",
    "        t = sentence[i]\n",
    "        # Adjust head - handle case when head is specified\n",
    "        if t['head'] not in [None, '', '_', '—', '-']: # Check if head is specified\n",
    "            print(\"head: \", t['head']) if debug else None # Print head index\n",
    "            head_idx = int(t['head']) - 1  # Adjust index if necessary\n",
    "            if 0 <= head_idx < len(doc): # Ensure head index is within bounds\n",
    "                print(\"0 < head_idx < len(doc): \", head_idx) if debug else None # Print head index\n",
    "                doc[i].head = doc[head_idx] # Assign head token\n",
    "                doc[i].head.pos_ = doc[head_idx].pos_ if doc[head_idx].pos_ else ''\n",
    "                print(\"0 < : \", doc[i].head) if debug else None\n",
    "        else:\n",
    "            # Optionally, assign the root token as head if needed\n",
    "            if root_token and root_token != token: # Check if root token is available and not the current token\n",
    "                doc[i].head = root_token # Assign root token as head \n",
    "                print(\"doc i is root token: \", root_token) if debug else None\n",
    "                \n",
    "        print(f\"Token: {doc[i].text}, POS: {doc[i].pos_}, TAG: {doc[i].tag_}, LEMMA: {doc[i].lemma_}, DEP: {doc[i].dep_}, HEAD: {doc[i].head}\") if debug else None\n",
    "\n",
    "def clean_and_print_docs(docs, debug=False):\n",
    "    \"\"\"Clean and print docs for review.\"\"\"\n",
    "    for doc in docs:\n",
    "        original_sentence = ' '.join(str(doc).split()) # Remove extra spaces\n",
    "        if debug:\n",
    "            print(f\"Original sentence: {original_sentence}\") \n",
    "        cleaned_sentence = ' '.join(str(doc).replace('\\r', ' ').replace('\\n', ' ').split())\n",
    "        if debug:\n",
    "            print(f\"Cleaned sentence: {cleaned_sentence}\")\n",
    "            if original_sentence != cleaned_sentence:\n",
    "                print(f\"Original sentence: {original_sentence}\")\n",
    "                print(f\"Cleaned sentence: {cleaned_sentence}\")\n",
    "            else:\n",
    "                print(f\"Clean sentence: {cleaned_sentence}\")\n",
    "        #for token in doc:\n",
    "        #    print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \", token.head)\n",
    "\n",
    "def serialize_docs(docs: List[Doc], norm_form: str, output_dir: Path, store_user_data: bool = True, debug: bool = False, output_file_name: str = None):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    # Split documents to subsets\n",
    "    if docs:  # Check that docs is non-empty to avoid the error\n",
    "        train_docs, test_docs = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "        train_docs, dev_docs = train_test_split(train_docs, test_size=0.25, random_state=42)  # Results in 0.2 split for dev\n",
    "    else:\n",
    "        \n",
    "        print(f\"No documents found for normalization form {norm_form}. Skipping serialization.\")\n",
    "        return  # Early return if docs is empty to avoid proceeding with undefined variables\n",
    "\n",
    "    \n",
    "    subsets = {'train': train_docs, 'dev': dev_docs, 'test': test_docs}\n",
    "    \n",
    "    for subset_name, subset_docs in subsets.items():\n",
    "        if output_file_name is not None:\n",
    "            output_path = output_dir / f\"{subset_name}/pos_{subset_name}/{output_file_name}_{subset_name}_{norm_form}.spacy\"\n",
    "        else:\n",
    "            output_path = output_dir / f\"{subset_name}/pos_{subset_name}/pos_{subset_name}_{norm_form}.spacy\"\n",
    "        doc_bin = DocBin(docs=subset_docs, store_user_data=store_user_data)\n",
    "        doc_bin.to_disk(output_path)\n",
    "        print(f\"Saved {len(subset_docs)} docs for normalization form {norm_form} to {output_path}\") # Progress indicator\n",
    "\n",
    "def process_conllu_file(file_path, nlp, normalization_forms, docs_by_norm, apply_cleaning=False, debug=False):\n",
    "    file_name = file_path.stem\n",
    "    print(f\"Processing file {file_name}...\")  # Progress indicator\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = f.read()\n",
    "\n",
    "    # Process each normalization form separately\n",
    "    for norm in normalization_forms:\n",
    "        normalized_data = normalize_optional_remove_accents(raw_data, norm, apply_cleaning)\n",
    "        print(normalized_data) if debug else None\n",
    "        # Directly parse the normalized .conllu content into sentences\n",
    "        sentences = parse(normalized_data)\n",
    "        for sentence_data in sentences:\n",
    "        # Process sentences and create doc objects\n",
    "            doc = process_sentences(sentence_data, nlp, debug=debug, file_name=file_name)  # Assuming this function is defined correctly\n",
    "            doc.user_data[\"source_info\"] = file_name\n",
    "            docs_by_norm[norm].extend([doc])  # Correctly using extend to avoid list of lists\n",
    "\n",
    "def process_folder_and_serialize(input_path, nlp, normalization_forms, output_dir, apply_cleaning=False, debug: bool = False, output_file_name: str = None):\n",
    "    docs_by_norm = {norm: [] for norm in normalization_forms}\n",
    "\n",
    "# Assume process_conllu_file updates docs_by_norm with processed docs for each conllu file\n",
    "\n",
    "    for file_path in Path(input_path).glob(\"*.conllu\"):\n",
    "        print(\"file name: \",file_path.name)  # Progress indicator\n",
    "        process_conllu_file(file_path, nlp, normalization_forms, docs_by_norm, apply_cleaning, debug=debug)\n",
    "    \n",
    "    # Serialize once all documents for a normalization form have been accumulated\n",
    "    for norm, docs in docs_by_norm.items():\n",
    "        print(f\"Serializing {len(docs)} documents for normalization form {norm} to {output_dir}... for type {type(docs)}\")  # Progress indicator\n",
    "        serialize_docs(docs, norm, output_dir, debug=debug, output_file_name=output_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_folder_and_serialize(Path(\"../assets/INCEpTION_Conllu/\"), nlp, ['NFKD', 'NFKC'], Path(\"../corpus/\"), apply_cleaning=False, debug=False, output_file_name=\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_folder_and_serialize(Path(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/\"), nlp, ['NFKD', 'NFKC'], Path(\"../corpus/\"), output_file_name=\"UD_Ancient_Greek-Perseus\", apply_cleaning=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_folder_and_serialize(Path(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/\"), nlp, ['NFKD', 'NFKC'], Path(\"../corpus/\"), output_file_name=\"UD_Ancient_Greek-PROIEL\", apply_cleaning=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# load and compare the documents\n",
    "# Define the paths to the files\n",
    "path_to_train_NFKD = Path(\"../corpus/train/pos_train/pos_train_NFKD.spacy\")\n",
    "path_to_train_NFKC = Path(\"../corpus/train/pos_train/pos_train_NFKC.spacy\")\n",
    "path_to_dev_NFKD = Path(\"../corpus/dev/pos_dev/pos_dev_NFKD.spacy\")\n",
    "path_to_dev_NFKC = Path(\"../corpus/dev/pos_dev/pos_dev_NFKC.spacy\")\n",
    "path_to_test_NFKD = Path(\"../corpus/test/pos_test/pos_test_NFKD.spacy\")\n",
    "path_to_test_NFKC = Path(\"../corpus/test/pos_test/pos_test_NFKC.spacy\")\n",
    "\n",
    "\n",
    "# Load the documents\n",
    "doc_bin_train_NFKD = DocBin().from_disk(path_to_train_NFKD)\n",
    "doc_bin_train_NFKC = DocBin().from_disk(path_to_train_NFKC)\n",
    "doc_bin_dev_NFKD = DocBin().from_disk(path_to_dev_NFKD)\n",
    "doc_bin_dev_NFKC = DocBin().from_disk(path_to_dev_NFKC)\n",
    "doc_bin_test_NFKD = DocBin().from_disk(path_to_test_NFKD)\n",
    "doc_bin_test_NFKC = DocBin().from_disk(path_to_test_NFKC)\n",
    "#\n",
    "# Get the documents as a list\n",
    "docs_train_NFKD = list(doc_bin_train_NFKD.get_docs(nlp.vocab))\n",
    "docs_train_NFKC = list(doc_bin_train_NFKC.get_docs(nlp.vocab))\n",
    "docs_dev_NFKD = list(doc_bin_dev_NFKD.get_docs(nlp.vocab))\n",
    "docs_dev_NFKC = list(doc_bin_dev_NFKC.get_docs(nlp.vocab))\n",
    "docs_test_NFKD = list(doc_bin_test_NFKD.get_docs(nlp.vocab))\n",
    "docs_test_NFKC = list(doc_bin_test_NFKC.get_docs(nlp.vocab))\n",
    "\n",
    "# Load the documents\n",
    "#docs = {name: load_docs(path, nlp) for name, path in paths.items()}\n",
    "\n",
    "# Now you can compare the documents\n",
    "# For example, you can print the first document from each list to see if they are the same\n",
    "#print(docs_NFKD[145].text == docs_NFKC[145].text)\n",
    "#print(docs_NFKD[145].text)\n",
    "# print attributes for tokens of text\n",
    "#for token in docs_NFKD[145]:\n",
    "#    print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \", token.head)\n",
    "\n",
    "#print(docs_NFKC[145].text)\n",
    "#for token in docs_NFKC[145]:\n",
    "#    print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \", token.head)\n",
    "\n",
    "# find in any of the files doc with specific text\n",
    "#sample_text = \"τα γαρ προ αὐτων και τα ἐτι παλαιτερα σαφως\" # sample from persus\n",
    "sample_text = \"ταῦτα δὲ τὰ γεγραμμένα πάσιν ὁμοίως εἰσί, καὶ φλέβες αἱ γεγραμμένοι\"\n",
    "#sample_text = \"πάσιν ὁμοίως εἰσίν, άλλα τε φλαβιά εἰσιν ἄλλοισιν, ἀλλ' οὐκ ἄξια λόγου\"\n",
    "#sample_text = \"ταῦτα δὲ τὰ γεγραμμένα πάσιν ὁμοίως εἰσί\"\n",
    "#sample_text = \"ταῦτα δὲ τὰ γεγραμμένα πάσιν ὁμοίως εἰσί\"\n",
    "#sample_text = \"νόσημα\"\n",
    "#sample_text = \"καὶ οἱ μὲν πλείονας ἔχοντες\"\n",
    "\n",
    "\n",
    "def find_text_in_docs(docs, doc_type):\n",
    "    for doc in docs:\n",
    "        if sample_text in doc.text:\n",
    "            print(f\"Found in {doc_type}\")\n",
    "            print(doc.text)\n",
    "            print(doc.user_data[\"file_name\"])\n",
    "            for token in doc:\n",
    "                print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \",token.head, \"Head POS: \", token.head.pos_,\n",
    "                      \"Children: \",[child for child in token.children])\n",
    "        else:\n",
    "            #print(f\"Not found in {doc_type}\")\n",
    "            continue  # stop after finding the first match\n",
    "\n",
    "\n",
    "find_text_in_docs(docs_train_NFKD, \"train NFKD\")\n",
    "find_text_in_docs(docs_train_NFKC, \"train NFKC\")\n",
    "find_text_in_docs(docs_dev_NFKD, \"dev NFKD\")\n",
    "find_text_in_docs(docs_dev_NFKC, \"dev NFKC\")\n",
    "find_text_in_docs(docs_test_NFKD, \"test NFKD\")\n",
    "find_text_in_docs(docs_test_NFKC, \"test NFKC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_test_NFKC:\n",
    "    for token in doc:\n",
    "        #if token.pos_ not '':\n",
    "        if token.pos_ != '':\n",
    "            print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \", token.head, \"Head POS: \", token.head.pos_,\n",
    "                \"Children: \", [child for child in token.children]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and compare pos_train_NFKD and pos_train_NFKC\n",
    "# Define the paths to the files\n",
    "path_to_NFKD = Path(\"../corpus/train/pos_train/pos_train_NFKD.spacy\")\n",
    "path_to_NFKC = Path(\"../corpus/train/pos_train/pos_train_NFKC.spacy\")\n",
    "\n",
    "# Load the documents\n",
    "doc_bin_NFKD = DocBin().from_disk(path_to_NFKD)\n",
    "doc_bin_NFKC = DocBin().from_disk(path_to_NFKC)\n",
    "\n",
    "# Get the documents as a list\n",
    "docs_NFKD = list(doc_bin_NFKD.get_docs(nlp.vocab))\n",
    "docs_NFKC = list(doc_bin_NFKC.get_docs(nlp.vocab))\n",
    "\n",
    "# Now you can compare the documents\n",
    "# For example, you can print the first document from each list to see if they are the same\n",
    "print(docs_NFKD[145].text == docs_NFKC[145].text)\n",
    "print(docs_NFKD[145].text)\n",
    "# print attributes for tokens of text\n",
    "for token in docs_NFKD[145]:\n",
    "    print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \", token.head)\n",
    "\n",
    "print(docs_NFKC[145].text)\n",
    "for token in docs_NFKC[145]:\n",
    "    print(\"token: \", token.text, \"LEMMA: \", token.lemma_, \"POS: \", token.pos_, \"TAG: \", token.tag_, \"DEP: \", token.dep_, \"HEAD: \", token.head)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print doc that contains the text ταῦτα δὲ τὰ γεγραμμένα πάσιν ὁμοίως εἰσί, καὶ φλέβες αἱ γεγραμμένοι\n",
    "if any(\"ταῦτα δὲ τὰ γεγραμμένα πάσιν ὁμοίως εἰσί, καὶ φλέβες αἱ γεγραμμένοι\" in doc.text for doc in docs_NFKD):\n",
    "    print(\"Found the text in the documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleaned sentence: οἱ δὲ ἐπὶ τοῦ ποδὸς οὐκ ἔχω φάναι διὰ τί παρώφθησαν ἐνίοις, καὶ μάλιστά γʼ ὅσοι τοὺς ἔνδον τῆς χειρὸς ἑπτὰ μύας ἐθεάσαντο.\n",
    "Cleaned sentence: παρέλιπον μὲν γὰρ κἀκεῖ τοὺς ἐν τῷ βάθει κειμένους ἐπʼ αὐτοῖς τοῖς ὀστοῖς, ὡς ἔμπροσθεν εἶπον, οὐ μὴν τούς γε προφανεῖς τοὺς ζʼ.\n",
    "Cleaned sentence: κατὰ δὲ τὸν πόδα τέτταρα γένη μυῶν εἰσιν, οὐχ, ὡς ἐν τῇ χειρί, δύο·\n",
    "Cleaned sentence: τρία μὲν ἐν τοῖς κάτω τοῦ ποδός, ἓν δὲ <ἐν> τοῖς ἄνω κατὰ τοῦ ταρσοῦ τεταγμένον."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T09:12:25.646718Z",
     "start_time": "2023-07-20T09:12:25.555326800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = main(data, nlp, \"../corpus/\", debug=False, normalization_forms=['NFKC'])\n",
    "\n",
    "# get tokens of first sentence\n",
    "print(docs[243])\n",
    "tokens = [t for t in docs[24]]\n",
    "for t in tokens:\n",
    "    print(t.text, t.pos_, t.tag_, t.lemma_, t.whitespace_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Spacy docbin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load spacy object\n",
    "# load docs from file\n",
    "docs = DocBin().from_disk(\"../corpus/train/pos_train/pos_train_NFKD.spacy\")\n",
    "test_docbin_docs = list(docs.get_docs(nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_docbin_docs:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create list of rows\n",
    "rows = []\n",
    "for doc in test_docbin_docs:\n",
    "    for token in doc:\n",
    "        row = [token.orth_, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.orth_]\n",
    "        rows.append(row)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(rows, columns=[\"Orth\", \"Lemma\", \"POS\", \"Tag\", \"Dep\", \"Head\"])\n",
    "\n",
    "# print dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in test_docbin_docs[0]:\n",
    "#print attributes\n",
    "    print('text: ', token.text, 'lemma :', token.lemma_, 'POS: ', token.pos_, 'tag: ', token.tag_, 'DEP: ', token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first doc in spacy docbin docs\n",
    "print(test_docbin_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlomy_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
