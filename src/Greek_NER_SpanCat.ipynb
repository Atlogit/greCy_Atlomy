{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:09:18.200289400Z",
     "start_time": "2023-06-13T07:09:05.302556800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from unicodedata import normalize\n",
    "import regex\n",
    "import random\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Remove all handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Add a new handler to the logger\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Define global parameters as lists\n",
    "INCEPTION_TRAIN_DATA = []\n",
    "TRAIN_DATA = []\n",
    "not_found_list = []\n",
    "MERGED_TRAIN_DATA = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "class Config:\n",
    "    DEFAULT_FORM = 'NFKD'\n",
    "    REMOVE_ACCENTS = False\n",
    "    LOWERCASE = False\n",
    "    STANDARDIZE_APOSTROPHE = True\n",
    "    REMOVE_BRACKETS = False\n",
    "    REMOVE_TRAILING_NUMBERS = False\n",
    "    REMOVE_EXTRA_SPACES = False\n",
    "    DEBUG = False\n",
    "\n",
    "# Instantiating the Config class for application-wide access.\n",
    "app_config = Config()\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# apostrophes and correct_apostrophe are defined as follows:\n",
    "apostrophes = [\"᾽\", \"᾿\", \"'\", \"’\", \"‘\"]\n",
    "correct_apostrophe = \"ʼ\"\n",
    "\n",
    "def clean_and_remove_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing diacritics (accents), except for specific characters,\n",
    "    and converting it to lowercase.\n",
    "    \"\"\"\n",
    "    allowed_characters = [' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\", 'ʼ', '̓']  # Including the Greek apostrophe\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    try:\n",
    "        non_accent_chars = [c for c in unicodedata.normalize('NFKD', text) \n",
    "        if unicodedata.category(c) != 'Mn' or c in allowed_characters]\n",
    "        return ''.join(non_accent_chars)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # A more generic exception handling if unexpected errors occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return text\n",
    "    \n",
    "\n",
    "def normalize_text(text: str, \n",
    "                   form: str = None, \n",
    "                   remove_accents: bool = None, \n",
    "                   lowercase: bool = None, \n",
    "                   standardize_apostrophe: bool = None, \n",
    "                   remove_brackets: bool = None, \n",
    "                   remove_trailing_numbers: bool = None, \n",
    "                   remove_extra_spaces: bool = None, \n",
    "                   debug: bool = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies multiple text normalization and cleaning steps on the input text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be normalized.\n",
    "    - form (str): Unicode normalization form ('NFC', 'NFD', 'NFKC', 'NFKD').\n",
    "    - remove_accents (bool): If True, removes accents from the text.\n",
    "    - lowercase (bool): If True, the text is converted to lowercase.\n",
    "    - standardize_apostrophe (bool): If True, replaces all defined apostrophe characters with a standard one.\n",
    "    - remove_brackets_only (bool): If True, removes the brackets themselves.\n",
    "    - remove_trailing_numbers (bool): If True, strips leading or trailing digits from the text.\n",
    "    - remove_extra_spaces (bool): If True, removes extra spaces and leading/trailing spaces.\n",
    "    - debug (bool): If True, prints before and after states for each operation.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The processed text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch default settings from app_config if not provided\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "    remove_accents = remove_accents if remove_accents is not None else app_config.REMOVE_ACCENTS\n",
    "    lowercase = lowercase if lowercase is not None else app_config.LOWERCASE\n",
    "    standardize_apostrophe = standardize_apostrophe if standardize_apostrophe is not None else app_config.STANDARDIZE_APOSTROPHE\n",
    "    remove_brackets = remove_brackets if remove_brackets is not None else app_config.REMOVE_BRACKETS\n",
    "    remove_trailing_numbers = remove_trailing_numbers if remove_trailing_numbers is not None else app_config.REMOVE_TRAILING_NUMBERS\n",
    "    remove_extra_spaces = remove_extra_spaces if remove_extra_spaces is not None else app_config.REMOVE_EXTRA_SPACES\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "    \n",
    "    normalized_text = text  # Initialize normalized_text with the original text\n",
    "\n",
    "    # Function to print before and after states for each operation during debugging\n",
    "    def debug_print(operation_name, before, after):\n",
    "        if debug:\n",
    "            logger.debug(f\"{operation_name} - Before: {before}\")\n",
    "            logger.debug(f\"{operation_name} - After: {after}\")\n",
    "\n",
    "    # Standardize apostrophe characters if required\n",
    "    if standardize_apostrophe:\n",
    "        before_text = normalized_text\n",
    "        for apos in apostrophes:\n",
    "            normalized_text = normalized_text.replace(apos, correct_apostrophe)\n",
    "        debug_print(\"Standardizing apostrophes\", before_text, normalized_text)\n",
    "        \n",
    "    if remove_accents:\n",
    "        before_text = normalized_text\n",
    "        try:\n",
    "            normalized_text = clean_and_remove_accents(normalized_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred while removing accents: {e}\")\n",
    "            # Decide what to do here: return the original text, a special value, or stop the process\n",
    "            return text        \n",
    "        debug_print(\"Removing accents\", before_text, normalized_text)\n",
    "        \n",
    "    # Convert to lowercase if required\n",
    "    if lowercase:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = normalized_text.lower()\n",
    "        debug_print(\"Lowercase conversion\", before_text, normalized_text)\n",
    "\n",
    "    # Unicode normalization\n",
    "    if form:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = unicodedata.normalize(form, normalized_text)\n",
    "        debug_print(\"Unicode normalization\", before_text, normalized_text)\n",
    "            \n",
    "    # Remove brackets only if required\n",
    "    if remove_brackets:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = re.sub(r'[\\(\\)\\[\\]]', '', normalized_text)\n",
    "        debug_print(\"Removing brackets\", before_text, normalized_text)\n",
    "        \n",
    "    # Remove trailing numbers if required\n",
    "    if remove_trailing_numbers:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = re.sub(r'^\\d+|\\d+$', '', normalized_text)\n",
    "        debug_print(\"Removing trailing numbers\", before_text, normalized_text)\n",
    "\n",
    "    # Remove multiple spaces and leading/trailing spaces\n",
    "    if remove_extra_spaces:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = ' '.join(normalized_text.split()).strip()\n",
    "        debug_print(\"Removing extra spaces\", before_text, normalized_text)\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and process data from Coda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "FILE_PATH = \"../assets/NER_assets/Ancient_Words_12_5_22.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Renaming columns\n",
    "df.rename(columns={'Word': 'Keyword', 'Category Types': 'Label'}, inplace=True)\n",
    "\n",
    "# Filling NaN values more efficiently and appropriately\n",
    "for early_col, new_col in [('Early Quote', 'Quote'), ('Early Word Before', 'Word Before'), \n",
    "                           ('Early Word After', 'Word After'), ('Early Category Type', 'Label')]:\n",
    "    df[new_col] = df[new_col].fillna(df[early_col])\n",
    "# Dropping rows with no Keyword and non-Greek Keywords\n",
    "pat = '[ء-ي]+'\n",
    "df = df.dropna(subset=['Keyword']).copy()\n",
    "df = df[~df['Keyword'].str.contains(pat, na=False)]\n",
    "\n",
    "# Cleaning data with combined regex patterns\n",
    "\n",
    "# Define a dictionary of patterns and replacements for the entire dataframe\n",
    "df_replacements = {\n",
    "    r'\\d+': '',  # Numbers\n",
    "    '-': '',  # Hyphens\n",
    "    ' +': ' ',  # Multiple spaces\n",
    "}\n",
    "\n",
    "# Apply the replacements to the text columns ('Early Quote', 'Quote', 'Early Word Before', 'Word Before', 'Early Word After', 'Word After', 'Keyword')\n",
    "for col in ['Early Quote', 'Quote', 'Early Word Before', 'Word Before', 'Early Word After', 'Word After', 'Keyword']:\n",
    "    for pattern, replacement in df_replacements.items():\n",
    "        df[col] = df[col].replace(pattern, replacement, regex=True)\n",
    "# Define a dictionary of patterns and replacements for the 'Keyword' column\n",
    "keyword_replacements = {\n",
    "    '\\n': '',  # New line\n",
    "    ',': '',  # Comma\n",
    "    r'\\.': '',  # Period\n",
    "    r'\\·': '',  # Interpunkt\n",
    "    r'\\s+$': ''  # End punctuation\n",
    "}\n",
    "\n",
    "# Apply the replacements to the 'Keyword' column\n",
    "for pattern, replacement in keyword_replacements.items():\n",
    "    df['Keyword'] = df['Keyword'].replace(pattern, replacement, regex=True)\n",
    "# Resetting the dataframe index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalizing the text in the columns\n",
    "columns_to_normalize = ['Early Quote', 'Quote', 'Early Word Before', 'Word Before', 'Early Word After', 'Word After', 'Keyword']\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: normalize_text(x) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_description(row):\n",
    "    formatted_values = []\n",
    "    for col in ['Modern Edition', 'Book', 'Chapter', 'Section', 'Page', 'Line Number']:\n",
    "        value = row[col]\n",
    "        if pd.notna(value) and value != '':\n",
    "            # Handling special case for Modern Edition\n",
    "            formatted_value = str(value) if col == 'Modern Edition' else f\"{col[0]}({int(value)})\" if isinstance(value, (int, float)) else f\"{col[0]}({value})\"\n",
    "            formatted_values.append(formatted_value)\n",
    "    source = ', '.join(formatted_values)\n",
    "    return f\"Coda, {source}\"\n",
    "\n",
    "df['Source'] = df.apply(create_source_description, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fix similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(sentence, n=2):\n",
    "    \"\"\"Returns a list of n-grams from the sentence.\"\"\"\n",
    "    # Convert sentence to lowercase and split into tokens/words\n",
    "    tokens = sentence.lower().split()\n",
    "    # Generate n-grams\n",
    "    return set(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "\n",
    "    debug = app_config.DEBUG\n",
    "\n",
    "    \"\"\"Calculate the Jaccard Similarity between two sets, handling empty sets.\"\"\"\n",
    "    try:\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        print(f\"Intersection: {intersection}\") if debug else None\n",
    "        \n",
    "        union = len(set1.union(set2))\n",
    "        print(f\"Union: {union}\") if debug else None\n",
    "        \n",
    "        return intersection / union if union != 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\") if debug else None\n",
    "        return None\n",
    "\n",
    "# Assuming df_grouped is your DataFrame and 'Quote' is the column with sentences.\n",
    "threshold = 0.5  # Example threshold, adjust based on your requirements\n",
    "groups = []\n",
    "df_grouped = df.copy()\n",
    "df_grouped[\"Quote_ngrams\"] = df_grouped[\"Quote\"].apply(lambda x: ngrams(x, n=2)) # Create n-grams for each quote\n",
    "\n",
    "for i, base_row in df_grouped.iterrows():\n",
    "    similar_found = False\n",
    "    base_ngrams = base_row[\"Quote_ngrams\"]\n",
    "    \n",
    "    for group in groups:\n",
    "        representative_index = group[0]  # Use the first sentence of each group as the representative\n",
    "        representative_ngrams = df_grouped.loc[representative_index, \"Quote_ngrams\"]\n",
    "        \n",
    "        if jaccard_similarity(base_ngrams, representative_ngrams) >= threshold:\n",
    "            group.append(i)\n",
    "            similar_found = True\n",
    "            break\n",
    "            \n",
    "    if not similar_found:\n",
    "        groups.append([i])  # Start a new group\n",
    "        \n",
    "from collections import Counter\n",
    "\n",
    "# Initialize a list to hold the data for the new DataFrame\n",
    "new_data = []\n",
    "\n",
    "for group_indices in groups:\n",
    "    # Extract sentences and their corresponding rows for the current group\n",
    "    group_rows = df_grouped.iloc[group_indices]\n",
    "    \n",
    "    # Determine the most frequent sentence in the group\n",
    "    sentence_counts = Counter(group_rows['Quote'])\n",
    "    most_freq_sentence, _ = sentence_counts.most_common(1)[0]\n",
    "    \n",
    "    # Determine the most complete (longest) sentence in the group after removing whitespace\n",
    "    most_complete_sentence = group_rows.loc[group_rows['Quote'].str.len().idxmax(), 'Quote']\n",
    "    \n",
    "    # Choose the sentence to use based on frequency and completeness\n",
    "    sentence_to_use = most_freq_sentence if len(most_freq_sentence) >= len(most_complete_sentence) else most_complete_sentence\n",
    "    \n",
    "    # Consolidate labels, keywords, words before, and words after into lists\n",
    "    entities = group_rows[['Label', 'Keyword', 'Word Before', 'Word After']].values.tolist()\n",
    "    \n",
    "    source_detail = group_rows.loc[group_rows['Quote'].str.len().idxmax(), 'Source']  # Use source of most complete sentence\n",
    "    \n",
    "    # Append to new data list: the chosen sentence, the consolidated entities and the source\n",
    "    new_data.append({'Quote': sentence_to_use, 'Entities': entities, 'Source': source_detail})\n",
    "\n",
    "# Create a new DataFrame from the new data\n",
    "df_final = pd.DataFrame(new_data)\n",
    "\n",
    "# Print the groups, their sentences with their original index, and the chosen sentence for verification\n",
    "for i, group_indices in enumerate(groups):\n",
    "    print(f\"Group {i + 1}:\")\n",
    "    for index in group_indices:\n",
    "        print(f\"  - Index {index}: {df_grouped.loc[index, 'Quote']}, Source: {df_grouped.loc[index, 'Source']}\")\n",
    "    print(f\"  Chosen sentence: {df_final.loc[i, 'Quote']}, Source: {df_final.loc[i, 'Source']}\\n\")\n",
    "    print(f\"  Entities: {df_final.loc[i, 'Entities']}\\n\")\n",
    "        \n",
    "# Structure of df_final:\n",
    "# 'Quote': The representative sentence of the group.\n",
    "# 'Entities': A list of lists for each sentence, containing label, keyword, word before, and word after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first 5 rows of the dataframe\n",
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fix similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def build_exact_pattern(word):\n",
    "    \"\"\"\n",
    "    Builds a regex pattern that matches the word exactly, including diacritics.\n",
    "    \"\"\"\n",
    "    return re.escape(word)\n",
    "\n",
    "def build_diacritic_agnostic_pattern(word, form=None, debug=None, max_errors=2, debug_patterns=None):\n",
    "    \"\"\"\n",
    "    Builds a regex pattern that matches the word in a diacritic-agnostic manner.\n",
    "    \"\"\"\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "\n",
    "    if not word:\n",
    "        return None  # Return None for empty strings\n",
    "\n",
    "    decomposed_word = unicodedata.normalize(form, word)\n",
    "    pattern_parts = []\n",
    "    for char in decomposed_word:\n",
    "        if unicodedata.category(char) == 'Mn':  # Is a diacritic mark\n",
    "            # This character is a diacritic; make its presence optional in the pattern\n",
    "            pattern_parts.append(r'\\p{M}*')  # Make diacritics optional\n",
    "        else:\n",
    "            # For non-diacritic characters, escape them and option\n",
    "            pattern_parts.append(re.escape(char) + r'\\p{M}*')  # Escape non-diacritics and allow diacritics\n",
    "\n",
    "    pattern = ''.join(pattern_parts)\n",
    "    logger.debug(f\"Regex Pattern: {pattern}\") if debug and debug_patterns else None\n",
    "\n",
    "    # Construct the fuzzy pattern separately\n",
    "    fuzzy_pattern = rf\"(?:{pattern})\"\n",
    "    if max_errors > 0:\n",
    "        fuzzy_pattern += rf\"{{s<={max_errors}}}\"\n",
    "\n",
    "    logger.debug(f\"word: {word}, fuzzy_pattern: {fuzzy_pattern}\")  if debug and debug_patterns else None # Debug print\n",
    "\n",
    "    return fuzzy_pattern\n",
    "\n",
    "def find_word_index(sentence, word, word_before, word_after, form=None, debug=None, debug_patterns=None, max_errors=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Finds the index of a word within a sentence, allowing for various normalization options and\n",
    "    tolerating diacritic variations and minor errors.\n",
    "    Raises a ValueError if the word is not found in the sentence.\n",
    "    \"\"\"\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "\n",
    "    # Override/debug settings are explicitly set here\n",
    "    kwargs['remove_accents'] = False\n",
    "    kwargs['lowercase'] = True  \n",
    "    kwargs['remove_extra_spaces'] = True\n",
    "    kwargs[\"remove_brackets\"] = False\n",
    "    kwargs[\"form\"] = form\n",
    "\n",
    "    if not sentence or not word:\n",
    "        raise ValueError(\"Sentence and word must be provided\")\n",
    "\n",
    "    # Normalize inputs\n",
    "    try:\n",
    "        normalized_sentence = normalize_text(sentence, **{**app_config.__dict__, **kwargs})\n",
    "        normalized_word = normalize_text(word, **{**app_config.__dict__, **kwargs})\n",
    "        normalized_word_before = normalize_text(word_before, **{**app_config.__dict__, **kwargs}) if word_before else \"\"\n",
    "        normalized_word_after = normalize_text(word_after, **{**app_config.__dict__, **kwargs}) if word_after else \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error normalizing inputs: {e}\")\n",
    "        raise ValueError(f\"Could not normalize inputs: sentence={sentence}, word={word}, word_before={word_before}, word_after={word_after}\")\n",
    "\n",
    "    logger.debug(f\"Input: sentence={sentence}, \\n word={word}, word_before={word_before}, word_after={word_after}, form={form}, kwargs={kwargs}\") if debug and debug_patterns else None\n",
    "    logger.debug(f\"Normalized sentence: {normalized_sentence}\") if debug and debug_patterns else None\n",
    "    logger.debug(\"normalized_word: %s, normalized_word_before: %s, normalized_word_after: %s\", normalized_word, normalized_word_before, normalized_word_after) if debug and debug_patterns else None\n",
    "    # Build exact and diacritic-agnostic patterns\n",
    "    try:\n",
    "        #exact_word_pattern = build_exact_pattern(normalized_word)\n",
    "        #word_pattern = build_diacritic_agnostic_pattern(normalized_word, form=form, debug=debug, max_errors=max_errors)\n",
    "        #exact_word_before_pattern = build_exact_pattern(normalized_word_before)\n",
    "        #word_before_pattern = build_diacritic_agnostic_pattern(normalized_word_before, form=form, debug=debug, max_errors=max_errors)\n",
    "        #exact_word_after_pattern = build_exact_pattern(normalized_word_after) if word_after else None\n",
    "        #word_after_pattern = build_diacritic_agnostic_pattern(normalized_word_after, form=form, debug=debug, max_errors=max_errors)\n",
    "        exact_word_pattern = build_exact_pattern(normalized_word)\n",
    "        word_pattern = build_diacritic_agnostic_pattern(normalized_word, form=form, debug=debug, max_errors=max_errors)\n",
    "        \n",
    "        exact_word_before_pattern = build_exact_pattern(normalized_word_before) if normalized_word_before else None\n",
    "        word_before_pattern = build_diacritic_agnostic_pattern(normalized_word_before, form=form, debug=debug, max_errors=max_errors) if normalized_word_before else None\n",
    "\n",
    "        exact_word_after_pattern = build_exact_pattern(normalized_word_after) if normalized_word_after else None\n",
    "        word_after_pattern = build_diacritic_agnostic_pattern(normalized_word_after, form=form, debug=debug, max_errors=max_errors) if normalized_word_after else None\n",
    "        \n",
    "        logger.debug(f\"exact_word_pattern: {exact_word_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"exact_word_before_pattern: {exact_word_before_pattern}\") if debug else None\n",
    "        logger.debug(f\"exact_word_after_pattern: {exact_word_after_pattern}\") if debug else None\n",
    "        logger.debug(f\"word_pattern: {word_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"word_before_pattern: {word_before_pattern}\") if debug else None\n",
    "        logger.debug(f\"word_after_pattern: {word_after_pattern}\") if debug else None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building patterns: {e}\")\n",
    "        raise ValueError(f\"Could not build patterns for: word={word}, word_before={word_before}, word_after={word_after}\")\n",
    "\n",
    "    \n",
    "    #patterns = create_patterns(sentence, exact_word_pattern=exact_word_pattern, exact_word_before_pattern=exact_word_before_pattern, exact_word_after_pattern=exact_word_after_pattern, word_before=word_before, word_after=word_after, max_errors=max_errors, debug=debug, debug_patterns=debug_patterns)\n",
    "    patterns = create_patterns(sentence, exact_word_pattern=exact_word_pattern,\n",
    "                            word_before=word_before, word_after=word_after, \n",
    "                            exact_word_before_pattern=exact_word_before_pattern, exact_word_after_pattern=exact_word_after_pattern)\n",
    "    for pattern in patterns.values():\n",
    "        logger.debug(f\"Attempting with pattern: {pattern}\") if debug and debug_patterns else None\n",
    "        try:\n",
    "            matches = re.finditer(pattern, normalized_sentence)\n",
    "            for match in matches:\n",
    "                match_word = match.group(1).rstrip()\n",
    "                start, end = match.start(1), match.start(1) + len(match_word)\n",
    "                return start, end, normalized_sentence\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Word '{word}' not found in sentence '{sentence}'\")\n",
    "\n",
    "    #patterns = create_patterns(sentence, exact_word_pattern=exact_word_pattern, exact_word_before_pattern=exact_word_before_pattern, exact_word_after_pattern=exact_word_after_pattern, word_before=word_before, word_after=word_after, max_errors=max_errors, debug=debug, debug_patterns=debug_patterns)\n",
    "    patterns = create_patterns(sentence=sentence, word_pattern=word_pattern, \n",
    "                           word_before=word_before, word_after=word_after, word_before_pattern=word_before_pattern, \n",
    "                           word_after_pattern=word_after_pattern)\n",
    "    for pattern in patterns.values():\n",
    "        logger.debug(f\"Attempting with pattern: {pattern}\") if debug and debug_patterns else None\n",
    "        try:\n",
    "            matches = re.finditer(pattern, normalized_sentence)\n",
    "            for match in matches:\n",
    "                match_word = match.group(1).rstrip()\n",
    "                start, end = match.start(1), match.start(1) + len(match_word)\n",
    "                return start, end, normalized_sentence\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Word '{word}' not found in sentence '{sentence}'\")\n",
    "\n",
    "def create_patterns(sentence, exact_word_pattern=None, word_pattern=None, word_before=None, word_after=None, word_before_pattern=None, word_after_pattern=None, exact_word_before_pattern=None, exact_word_after_pattern=None, max_errors=2, debug=False, debug_patterns=False):\n",
    "    patterns = {}\n",
    "    optional_punct_pattern = r\"[,;.!?]?\\s*\"  # Pattern for optional punctuation followed by optional whitespace\n",
    "\n",
    "    logger.debug(f\"exact_word_pattern: {exact_word_pattern}, word_pattern: {word_pattern}, word_before: {word_before}, word_after: {word_after}, word_before_pattern: {word_before_pattern}, word_after_pattern: {word_after_pattern}, exact_word_before_pattern: {exact_word_before_pattern}, exact_word_after_pattern: {exact_word_after_pattern}\") if debug and debug_patterns else None\n",
    "    \n",
    "    # First, try to match the exact word pattern along with the exact word before and after patterns\n",
    "    if exact_word_before_pattern and exact_word_after_pattern:\n",
    "        logger.debug(f\"THE WORD BEFORE PATTERN IS: {word_before_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"THE WORD AFTER PATTERN IS: {word_after_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"THE EXACT WORD BEFORE PATTERN IS: {exact_word_before_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"THE EXACT WORD AFTER PATTERN IS: {exact_word_after_pattern}\") if debug and debug_patterns else None\n",
    "        #patterns['both_exact_before_and_after_pattern'] = rf\"(?:(?<!\\p{{L}}){exact_word_before_pattern}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})(?:{optional_punct_pattern}{exact_word_after_pattern}(?!\\p{{L}}))\"\n",
    "        patterns['both_exact_before_and_after_pattern'] = rf\"(?<={exact_word_before_pattern}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})(?=\\s*{optional_punct_pattern}{exact_word_after_pattern})\"\n",
    "        logger.debug(f\"exact_word_before_pattern in both_before_and_after: {exact_word_before_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"exact_word_after_pattern in both_before_and_after: {exact_word_after_pattern}\") if debug and debug_patterns else None\n",
    "        logger.debug(f\"both_before_and_after pattern: {patterns['both_exact_before_and_after_pattern']}\") if debug and debug_patterns else None\n",
    "\n",
    "    # When both before and after patterns are specified\n",
    "    if word_before_pattern and word_after_pattern:\n",
    "        #patterns['both_before_and_after'] = rf\"(?<=(?<!\\p{{L}}){word_before_pattern}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})(?:{optional_punct_pattern}{word_after_pattern}(?!\\p{{L}}))\"\n",
    "        patterns['both_before_and_after'] = rf\"(?<={word_before_pattern}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})(?=\\s*{optional_punct_pattern}{word_after_pattern})\"\n",
    "        logger.debug(f\"both_before_and_after pattern: {patterns['both_before_and_after']}\") if debug and debug_patterns else None\n",
    "\n",
    "    # When only the before pattern is specified and it's not a single non-word character\n",
    "    logger.debug(f\"THE WORD PATTERN IS: {word_pattern}\") if debug and debug_patterns else None\n",
    "    logger.debug(f\"THE WORD BEFORE PATTERN IS: {word_before_pattern}\") if debug and debug_patterns else None\n",
    "    logger.debug(f\"THE WORD AFTER PATTERN IS: {word_after_pattern}\") if debug and debug_patterns else None\n",
    "    logger.debug(f\"THE EXACT WORD BEFORE PATTERN IS: {exact_word_before_pattern}\") if debug and debug_patterns else None\n",
    "    logger.debug(f\"THE EXACT WORD AFTER PATTERN IS: {exact_word_after_pattern}\") if debug and debug_patterns else None\n",
    "    logger.debug(\"CHECKKKKKEKE %s %s %s\", word_before_pattern, word_before, len(word_before)) if debug and debug_patterns and word_before else None\n",
    "    #if any([exact_word_before_pattern, word_before_pattern]) and len(word_before) > 1:\n",
    "    if word_before is not None and len(word_before.strip()) > 0:\n",
    "        logger.debug(\"found word before!!! WooHOO!\") if debug and debug_patterns else None\n",
    "        #patterns['only_before'] = rf\"(?<=(?<!\\p{{L}}){exact_word_before_pattern}|{word_before_pattern}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})(?!\\p{{L}})\"\n",
    "        patterns['only_before'] = rf\"(?<={exact_word_before_pattern or ''}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})\"\n",
    "        logger.debug(f\"only_before pattern: {patterns['only_before']}\") if debug and debug_patterns else None\n",
    "    \n",
    "    # when only the after pattern is specified and it's not a single non-word character\n",
    "    logger.debug(f\"Checking word before : {word_before_pattern}, {word_before}\") if debug and debug_patterns else None\n",
    "    logger.debug(f\"Checking word after : {word_after_pattern}, {word_after}\") if debug and debug_patterns else None\n",
    "    #if any([exact_word_after_pattern, word_after_pattern]) and len(word_after) > 1:\n",
    "    if word_after is not None and len(word_after.strip()) > 0:\n",
    "        #patterns['only_after'] = rf\"({exact_word_pattern}|{word_pattern})(?:{optional_punct_pattern}{exact_word_after_pattern}|{word_after_pattern}(?!\\p{{L}}))\"\n",
    "        patterns['only_after'] = rf\"({exact_word_pattern}|{word_pattern})(?=\\s*{optional_punct_pattern}{exact_word_after_pattern})\"\n",
    "        logger.debug(f\"only_after pattern: {patterns['only_after']}\") if debug and debug_patterns else None\n",
    "\n",
    "    # When the before pattern is a single non-word character or None\n",
    "    if any([exact_word_before_pattern, word_before_pattern]) and (word_before is None or len(word_before) <= 1):\n",
    "        patterns['before_single_or_empty'] = rf\"(?<=(?<!\\p{{L}}){exact_word_before_pattern}|{word_before_pattern}\\s*{optional_punct_pattern})({exact_word_pattern}|{word_pattern})(?!\\p{{L}})\"\n",
    "        logger.debug(f\"before_single_or_empty pattern: {patterns['before_single_or_empty']}\") if debug and debug_patterns else None\n",
    "        \n",
    "    # When the after pattern is a single non-word character or None\n",
    "    if any([exact_word_after_pattern, word_after_pattern]) and (word_after is None or len(word_after) <= 1):\n",
    "        patterns['after_single_or_empty'] = rf\"({exact_word_pattern}|{word_pattern})(?:(?={optional_punct_pattern}{exact_word_after_pattern}|{word_after_pattern})|$)(?!\\p{{L}})\"\n",
    "        logger.debug(f\"after_single_or_empty pattern: {patterns['after_single_or_empty']}\") if debug and debug_patterns else None\n",
    "\n",
    "    # Word at the start of the sentence (no preceding word boundary)\n",
    "    patterns['start_of_sentence'] = rf\"^({exact_word_pattern}|{word_pattern})(?=\\s|\\p{{P}}|$)\"\n",
    "    logger.debug(f\"start_of_sentence pattern: {patterns['start_of_sentence']}\") if debug and debug_patterns else None\n",
    "\n",
    "    # Word at the end of the sentence (no following word boundary)\n",
    "    #patterns['end_of_sentence'] = rf\"(?<=^|\\s|\\p{{P}})({exact_word_pattern}|{word_pattern}){optional_punct_pattern}?$\"\n",
    "    patterns['end_of_sentence'] = rf\"(?<=^|\\s|\\p{{P}})({exact_word_pattern}|{word_pattern})[{optional_punct_pattern}]?$\"\n",
    "\n",
    "    logger.debug(f\"end_of_sentence pattern: {patterns['end_of_sentence']}\") if debug and debug_patterns else None\n",
    "\n",
    "    # Word anywhere in the sentence, not accounting for specific before/after context\n",
    "    #patterns['anywhere_in_sentence'] = rf\"(?<!\\p{{L}})({exact_word_pattern}|{word_pattern})(?!\\p{{L}})\"\n",
    "    #print(f\"anywhere_in_sentence pattern: {patterns['anywhere_in_sentence']}\") if debug and debug_patterns else None\n",
    "\n",
    "\n",
    "    # Debugging code\n",
    "    if debug and debug_patterns:\n",
    "        # Check if 'both_before_and_after' pattern exists and print it\n",
    "        if 'both_before_and_after' in patterns:\n",
    "            logger.debug(f\"both_before_and_after pattern: {patterns['both_before_and_after']}\") if debug and debug_patterns else None\n",
    "            logger.debug(f\"both_before_and_after pattern matches sentence: {bool(re.search(patterns['both_before_and_after'], sentence))}\") if debug and debug_patterns else None\n",
    "\n",
    "        # Check if \"exact_word_before_pattern\" exists and print it\n",
    "        if exact_word_before_pattern:\n",
    "            logger.debug(f\"exact_word_before_pattern: {exact_word_before_pattern}\") if debug and debug_patterns else None\n",
    "            logger.debug(f\"exact_word_before_pattern matches sentence: {bool(re.search(exact_word_before_pattern, sentence))}\") if debug and debug_patterns else None\n",
    "            \n",
    "        # Check if 'word_before_pattern' exists and print it\n",
    "        if word_before_pattern:\n",
    "            logger.debug(f\"word_before_pattern: {word_before_pattern}\") if debug and debug_patterns else None\n",
    "            logger.debug(f\"word_before_pattern matches sentence: {bool(re.search(word_before_pattern, sentence))}\") if debug and debug_patterns else None\n",
    "\n",
    "        # Check if \"exact_word_after_pattern\" exists and print it\n",
    "        if exact_word_after_pattern:\n",
    "            logger.debug(f\"exact_word_after_pattern: {exact_word_after_pattern}\") if debug and debug_patterns else None\n",
    "            logger.debug(f\"exact_word_after_pattern matches sentence: {bool(re.search(exact_word_after_pattern, sentence))}\") if debug and debug_patterns else None\n",
    "            \n",
    "        # Check if 'word_after_pattern' exists and print it\n",
    "        if word_after_pattern:\n",
    "            logger.debug(f\"word_after_pattern: {word_after_pattern}\") if debug and debug_patterns else None\n",
    "            logger.debug(f\"word_after_pattern matches sentence: {bool(re.search(word_after_pattern, sentence))}\") if debug and debug_patterns else None\n",
    "\n",
    "    return patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df, form=None, debug=None, debug_patterns=None, debug_inception=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Prepares the training data by finding entity indices within normalized sentences.\n",
    "    Returns a list of training data and a list of words not found in their sentences.\n",
    "    \"\"\"\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "    \n",
    "    # Ensuring 'remove_accents' and 'lowercase' have fixed values for this function's purpose\n",
    "    kwargs['remove_accents'] = False\n",
    "    kwargs['lowercase'] = True  # Override/debug settings are explicitly set here\n",
    "    kwargs['remove_extra_spaces'] = True\n",
    "    kwargs[\"remove_brackets\"] = False\n",
    "    kwargs[\"form\"] = form\n",
    "    logger.debug(\"kwargs: \", kwargs) if debug else None\n",
    "\n",
    "    TRAIN_DATA = []\n",
    "    not_found_list = []\n",
    "    found_entities = 0\n",
    "\n",
    "    for i, (_, row) in enumerate(df.iterrows(), start=1):\n",
    "        new_entities = []\n",
    "        sentence = row['Quote']\n",
    "        source = row['Source']\n",
    "        logger.debug(f\"Sentence {i}: {sentence}, Source: {source}\") if debug else None\n",
    "        for entity in row['Entities']:\n",
    "            logger.debug (f\"Entity: {entity}\") if debug else None\n",
    "            try:\n",
    "                category, word, word_before, word_after = entity\n",
    "                logger.debug(f\"Category: {category}, Word: {word}, Word Before: {word_before}, Word After: {word_after}\") if debug else None\n",
    "                start, end, _ = find_word_index(sentence, word, word_before, word_after, **{**app_config.__dict__, **kwargs}, debug=debug)\n",
    "                new_entities.append((start, end, category))\n",
    "                found_entities += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error finding word index for entity {entity}: {e}\")\n",
    "                not_found_list.append((word, sentence))\n",
    "        \n",
    "        logger.debug(f\"Found {len(new_entities)} entities in this sentence.\") if debug else None\n",
    "        normalized_sentence = normalize_text(sentence, **{**app_config.__dict__, **kwargs})\n",
    "        TRAIN_DATA.append((normalized_sentence, {'entities': new_entities}, source))\n",
    "\n",
    "    logger.info(f\"Completed preparing training data from table. Total entries: {len(TRAIN_DATA)}. Entities found and registered: {found_entities}. Entities not found: {len(not_found_list)}\")\n",
    "    return TRAIN_DATA, not_found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataa = []\n",
    "new_entities = []\n",
    "#[['Body Part', 'γένειον', 'τὸ πρόσθιον', ', τὸ δʼ'], ['Body Part', 'γένυς', 'δʼ ὀπίσθιον', '. Κινεῖ δὲ'], ['Adjectives/Qualities', 'πρόσθιον', 'τούτων τὸ', 'γένειον, τὸ'], ['Adjectives/Qualities', 'ὀπίσθιον', 'τὸ δʼ', 'γένυς. Κινεῖ'], ['Division', 'τὸ', 'δύο· τούτων', 'πρόσθιον γένειον'], ['Division', 'τὸ', 'πρόσθιον γένειον,', 'δʼ ὀπίσθιον']]\n",
    "#'Division', 'τὸ', 'δύο· τούτων', 'πρόσθιον γένειον'], ['Division', 'τὸ', 'πρόσθιον γένειον,', 'δʼ ὀπίσθιον']]\n",
    "#sentence=\"τούτων τὸ πρόσθιον γένειον, τὸ δʼ ὀπίσθιον γένυς\"\n",
    "#word='τὸ'\n",
    "#word_before='δύο· τούτων'\n",
    "#word_after='πρόσθιον γένειον'\n",
    "\n",
    "#word='τὸ'\n",
    "#word_before='πρόσθιον γένειον,'\n",
    "#word_after='δʼ ὀπίσθιον'\n",
    "\n",
    "#sentence=\"ἐνίοις μὲν γὰρ εὐρύτερον τὸ πρὸς τῇ κοιλίᾳ, τὸ δὲ πρὸς τῷ τέλει στενώτερον (διόπερ αἱ κύνες μετὰ πόνου προΐενται τὴν τοιαύτην περίττωσιν), τοῖς δὲ πλείοσιν ἄνωθεν στενώτερον, πρὸς τῷ τέλει δ' εὐρύτερον\"\n",
    "#word='εὐρύτερον'\n",
    "#word_before=\"τέλει δ'\"\n",
    "#word_after='. Μείζω δὲ'\n",
    "\n",
    "#sentence = \"ἐν ἑκάστῃ γὰρ ἔχει αἷμα τῶν κοιλιῶν, λεπτότατον δʼ ἐστὶ τὸ ἐν τῇ μέσῃ.\"\n",
    "#word = \"μέσῃ\"\n",
    "#word_before = \"τῇ \"\n",
    "#word_after = \". ὑπό\"\n",
    "\n",
    "#sentence = \"Ἔστι δʼ οὐ πολυσχιδὴς ὁ τοῦ ἀνθρώπου, ὥσπερ ἐνίων ζῳοτόκων, οὐδὲ λεῖος, ἀλλʼ ἔχει ἀνωμαλίαν\"\n",
    "#word = \"ζῳοτόκων\"\n",
    "#word_before = \"ὥσπερ ἐνίων\"\n",
    "#word_after = \" οὐδὲ λεῖος\"\n",
    "\n",
    "#sentence = \"ἐν ἑκάστῃ γὰρ ἔχει αἷμα τῶν κοιλιῶν, λεπτότατον δʼ ἐστὶ τὸ ἐν τῇ μέσῃ\"\n",
    "#word = \"ἑκάστῃ\"\n",
    "#word_before = \"αὑτῇ· ἐν\"\n",
    "#word_after = \"γὰρ ἔχει\"\n",
    "\n",
    "#sentence = \"Ὑπὸ δὲ τὸν πνεύμονά ἐστι τὸ διάζωμα τὸ τοῦ θώρακος, αἱ καλούμεναι φρένες, πρὸς μὲν τὰ πλευρὰ καὶ τὰ ὑποχόνδρια καὶ τὴν ῥάχιν συνηρτημέναι, ἐν μέσῳ δʼ ἔχει τὰ λεπτὰ καὶ ὑμενώδη\"\n",
    "#word = \"πνεύμονα\"\n",
    "#word_before = \"δὲ τὸν\"\n",
    "#word_after = \"ἐστι τὸ\"\n",
    "\n",
    "#sentence = \"Ὑπὸ δὲ τὸν πνεύμονά ἐστι τὸ διάζωμα τὸ τοῦ θώρακος, αἱ καλούμεναι φρένες, πρὸς μὲν τὰ πλευρὰ καὶ τὰ ὑποχόνδρια καὶ τὴν ῥάχιν συνηρτημέναι, ἐν μέσῳ δʼ ἔχει τὰ λεπτὰ καὶ ὑμενώδη\"\n",
    "#word = \"φρένες\"\n",
    "#word_before = \", αἱ καλούμεναι\"\n",
    "#word_after = \", πρός μὲν\"\n",
    "\n",
    "#example that gets some of the previous word by mistake\n",
    "#sentence = \"διατέταται δὲ καὶ ἀπὸ τοῦ σπληνὸς φλὲψ ἐς τὰ ἀριστερὰ καὶ κάτω καὶ ἄνω, ὥσπερ καὶ ἀπὸ τοῦ ἥπατος, λεπτοτέρη δὲ καὶ ἀσθενεστέρη\"\n",
    "#word = \"ἀριστερὰ\"\n",
    "#word_before = \"ἐς\"\n",
    "#word_after = \"καὶ\"\n",
    "\n",
    "#sentence = \"καὶ τὸ μὲν παχύτατον καὶ μέγιστον καὶ κοιλότατον ἐς τὸν ἐγκέφαλον τελευτᾷ, τὸ δὲ ἐς τὸ οὖς τὸ δεξιὸν, τὸ δὲ ἐς τὸν ὀφθαλμὸν τὸν δεξιόν, τὸ δὲ ἐς τὸν μυκτῆρα\"\n",
    "#word = \"δεξιόν\"\n",
    "#word_before = \"τὸν ὀφθαλμὸν \"\n",
    "#word_after = \"τὸ δὲ \"\n",
    "\n",
    "\n",
    "# Call the function with the test data\n",
    "start, end, normalized_sentence = find_word_index(sentence, word, word_before, word_after, form='NFKD', debug=True, debug_patterns=True, max_errors=2)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Start: {start}, End: {end}\") \n",
    "print(f\"Characters between start and end: '{normalized_sentence[start:end]}'\")\n",
    "\n",
    "#normalized_sentence = normalize_text(sentence, form='NFKC', remove_accents=False, lowercase=True, remove_extra_spaces=True)\n",
    "Dataa.append((normalized_sentence, {'entities': [(start, end, 'LABEL')]}, 'SOURCE'))\n",
    "print(Dataa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Add annotations from INCEpTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.320825600Z",
     "start_time": "2023-06-13T07:12:32.146255100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract all files in inception folder to temp folder\n",
    "from cassis import *\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "from cassis import *\n",
    "\n",
    "TYPESYSTEM_FILENAME = \"TypeSystem.xml\"\n",
    "\n",
    "def extract_and_process_zip_files(input_path, form=None, debug=None, debug_inception=None):\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "    debug = app_config.DEBUG\n",
    "\n",
    "    with zipfile.ZipFile(input_path, 'r') as zip_ref:\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            zip_ref.extractall(tempdir)\n",
    "            # Get the basename of the .zip file for use as an identifier\n",
    "            base_zip_filename = os.path.splitext(os.path.basename(input_path))[0]\n",
    "            process_extracted_files(tempdir, base_zip_filename, form, debug, debug_inception)\n",
    "\n",
    "def process_extracted_files(directory, base_zip_filename, form=None, debug=None, debug_inception=None):\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "\n",
    "    typesystem = None\n",
    "    # Load the typesystem only once\n",
    "    try:\n",
    "        with open(os.path.join(directory, TYPESYSTEM_FILENAME), 'rb') as f:\n",
    "            typesystem = load_typesystem(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load TypeSystem: {e}\")\n",
    "        return\n",
    "\n",
    "    for f in os.listdir(directory):\n",
    "        if f.endswith(\".xmi\"):\n",
    "            # Pass the base name of the zip file concatenated with the XMI file name\n",
    "            file_identifier = f\"{os.path.splitext(f)[0]}\"\n",
    "            process_xmi_file(os.path.join(directory, f), typesystem, file_identifier, form, debug, debug_inception)\n",
    "\n",
    "def process_xmi_file(filename, typesystem, file_identifier, form=None, debug=None, debug_inception=None):\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "    debug = app_config.DEBUG\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "            logger.debug(f\"Processing file: {filename} as {file_identifier}\") if debug_inception else None\n",
    "            process_cas(cas, file_identifier, form, debug, debug_inception)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "def process_cas(cas, file_identifier, form=None, debug=None, debug_inception=None):\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    form = form if form is not None else app_config.DEFAULT_FORM\n",
    "    debug = app_config.DEBUG\n",
    "\n",
    "    for sentence in cas.select((\"de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence\")):\n",
    "        process_sentence(sentence, cas, file_identifier, form, debug, debug_inception)\n",
    "\n",
    "def calculate_token_positions(normalized_sentence_text, adjusted_token_start, normalized_token_text, last_match_end, debug=None):\n",
    "    \"\"\"\n",
    "    Attempts to adjust token positions from original indices considering the normalized sentence text, \n",
    "    with awareness of potential non-uniform normalization effects.\n",
    "    \n",
    "    Args:\n",
    "    - normalized_sentence_text (str): The full sentence text after normalization.\n",
    "    - adjusted_token_start (int): Original start index of the token before normalization.\n",
    "    - normalized_token_text (str): The specific token text after normalization.\n",
    "    - debug (bool): If provided and True, enables debug outputs.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[int, int]: Adjusted start and end indices of the token in the normalized sentence.\n",
    "    \"\"\"\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "\n",
    "    # Validate input data\n",
    "    if not normalized_sentence_text:\n",
    "        logger.error(\"Normalized sentence text is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Attempt to find the closest match of the normalized token in the normalized text\n",
    "    try:\n",
    "        window_size = len(normalized_token_text)\n",
    "        start_search = last_match_end if last_match_end is not None else 0\n",
    "        new_token_begin = normalized_sentence_text.find(normalized_token_text, start_search)\n",
    "\n",
    "        if new_token_begin == -1:\n",
    "            logger.warning(f\"Token '{normalized_token_text}' not found in the normalized text after position {start_search}.\")\n",
    "            return None\n",
    "\n",
    "        new_token_end = new_token_begin + window_size\n",
    "    \n",
    "        logger.debug(f\"Adjusted indices: {new_token_begin}-{new_token_end} for token '{normalized_token_text}' in normalized text.\")\n",
    "        return new_token_begin, new_token_end\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error occurred while calculating token positions: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_sentence(sentence, cas, file_identifier, form=None, debug=None, debug_inception=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Processes each sentence from the CAS file, extracts entity tokens along with labels,\n",
    "    calculates corrected token positions based on the cleaned sentence,\n",
    "    and appends to INCEPTION_TRAIN_DATA for NER training.\n",
    "    \"\"\"\n",
    "    # Apply normalization as per configurations or provided form\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "    # Ensuring 'remove_accents' and 'lowercase' have fixed values for this function's purpose\n",
    "    kwargs['remove_accents'] = False\n",
    "    kwargs['lowercase'] = True  # Override/debug settings are explicitly set here\n",
    "    kwargs['remove_extra_spaces'] = True\n",
    "    kwargs[\"remove_brackets\"] = False\n",
    "    kwargs[\"form\"] = form\n",
    "    logger.debug(\"kwargs: %s\", kwargs) if debug else None\n",
    "    \n",
    "    # Normalize inputs\n",
    "    original_sentence_text = sentence.get_covered_text()\n",
    "    normalized_sentence_text = normalize_text(original_sentence_text, **{**app_config.__dict__, **kwargs})\n",
    "    \n",
    "    sentence_start_offset = sentence.begin  # Sentence start relative to the full document\n",
    "\n",
    "    logger.debug(\"Final FORM: %s\", form) if debug_inception else None\n",
    "    logger.debug(\"original sentence: %s\", original_sentence_text) if debug_inception else None\n",
    "    logger.debug(\"normalized_sentence_text: %s\", normalized_sentence_text) if debug_inception else None\n",
    "    \n",
    "    spans = []\n",
    "    last_match_end = None\n",
    "    for token in cas.select_covered('webanno.custom.CategoryType', sentence):\n",
    "        # Adjust token indices to be relative to the start of the sentence\n",
    "        adjusted_token_start = token.begin - sentence_start_offset\n",
    "        adjusted_token_end = token.end - sentence_start_offset\n",
    "        logger.debug(f\"Token index: {token.begin}-{token.end}, Adjusted index for sentence: {adjusted_token_start}-{adjusted_token_end}, Token text: {token.get_covered_text()}\") if debug_inception else None\n",
    "        \n",
    "        normalized_token_text = normalize_text(token.get_covered_text(), **{**app_config.__dict__, **kwargs})\n",
    "        token_positions = calculate_token_positions(normalized_sentence_text, adjusted_token_start, normalized_token_text, last_match_end, debug=debug_inception)\n",
    "        if token_positions is None:\n",
    "            logger.error(f\"Error calculating positions for token '{token.get_covered_text()}' in sentence '{original_sentence_text}'\")\n",
    "            continue\n",
    "        \n",
    "        new_token_begin, new_token_end = token_positions\n",
    "        spans.append((new_token_begin, new_token_end, token.value(\"Value\")))\n",
    "        last_match_end = new_token_end\n",
    "\n",
    "    if spans:\n",
    "        INCEPTION_TRAIN_DATA.append((normalized_sentence_text, {'entities': spans}, file_identifier))\n",
    "        logger.debug(\"entities: %s\", spans) if debug_inception else None\n",
    "        logger.debug(\"normalized sentence: %s\", normalized_sentence_text) if debug_inception else None\n",
    "    return INCEPTION_TRAIN_DATA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.utils import indexable, _safe_indexing\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from sklearn.model_selection._split import _validate_shuffle_split\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Load configuration from a file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "TEST_SIZE = config.getfloat('SPLIT', 'test_size', fallback=0.2)\n",
    "RANDOM_STATE = config.getint('SPLIT', 'random_state', fallback=42)\n",
    "DEBUG = config.getboolean('GENERAL', 'debug', fallback=False)\n",
    "\n",
    "from spacy.tokens import Span, SpanGroup\n",
    "\n",
    "Span.set_extension(\"labels\", default=[], force=True)\n",
    "\n",
    "def process_sentences(data, nlp, output_dir, output_file_name=None, form=None, debug=None):\n",
    "    \"\"\"\n",
    "    Process the input data into a list of Doc objects using the SpanCategorizer approach.\n",
    "\n",
    "    Args:\n",
    "        data: The input data to be processed.\n",
    "        nlp: The natural language processing object.\n",
    "        output_dir: The output directory for serialized files.\n",
    "        output_file_name: The base name for the output file (optional).\n",
    "        form: The normalization form.\n",
    "        debug: Flag to enable debug mode.\n",
    "\n",
    "    Returns:\n",
    "        A list of Doc objects.\n",
    "    \"\"\"\n",
    "\n",
    "    debug = debug if debug is not None else DEBUG\n",
    "    form = form if form is not None else DEBUG\n",
    "\n",
    "    docs = []\n",
    "    missing_spans = []\n",
    "    total_spans = 0\n",
    "    processed_docs = 0\n",
    "    \n",
    "    for data_entry in data:\n",
    "        entity_text_changes = {}\n",
    "\n",
    "        if len(data_entry) == 3:\n",
    "            text, annot, file_identifier = data_entry\n",
    "            entities = annot['entities']\n",
    "            logger.debug(f\"Current data_entry: {text}, {annot}, {file_identifier}\")\n",
    "        else:\n",
    "            text, annot, file_identifier = data_entry\n",
    "            entities = annot['entities']\n",
    "            logger.debug(f\"Current data_entry (without source identifier): {text}, {annot}\")\n",
    "\n",
    "        doc = nlp.make_doc(text)\n",
    "        spans = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            start, end, label = entity\n",
    "        \n",
    "            if isinstance(label, float) and np.isnan(label):\n",
    "                logger.warning(f\"Skipping entity with 'nan' label: {text[start:end]} [{start},{end}] in sentence: {text} in {file_identifier}\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(label, str):\n",
    "                label = [label]  # Convert single label to a list\n",
    "            if all(l == l and l is not None for l in label):\n",
    "                tokens = [token for token in doc if not (token.idx + len(token.text) <= start or token.idx >= end)]\n",
    "                if tokens:\n",
    "                    start = tokens[0].idx\n",
    "                    end = tokens[-1].idx + len(tokens[-1].text)\n",
    "\n",
    "                    # Create a span for each label\n",
    "                    for l in label:\n",
    "                        span = doc.char_span(start, end, label=l, alignment_mode=\"expand\")\n",
    "                        if span:\n",
    "                            spans.append(span)\n",
    "                        else:\n",
    "                            logger.warning(f\"Invalid span: {text[start:end]} [{start},{end}] cannot be mapped to a valid `Doc` span.\")\n",
    "                            missing_spans.append((text[start:end], label))\n",
    "                            logger.warning(f\"Missing span: {text[start:end]} [{start},{end}] with labels {label} in sentence: {text} in {file_identifier}\")\n",
    "\n",
    "        doc.spans[\"sc\"] = spans\n",
    "        doc.user_data[\"source_info\"] = file_identifier\n",
    "        docs.append(doc)\n",
    "        logger.debug(f\"Processed document {processed_docs} with {len(spans)} spans in {file_identifier}\") if debug else None\n",
    "        processed_docs += 1\n",
    "        total_spans += len(spans)\n",
    "\n",
    "        for original_text, new_text in entity_text_changes.items():\n",
    "            logger.debug(f\"Entity text changed from '{original_text}' to '{new_text}'\")\n",
    "\n",
    "    logger.info(f\"{processed_docs} documents processed with a total of {total_spans} spans.\")\n",
    "    if missing_spans:\n",
    "        logger.warning(f\"Total missing spans: {len(missing_spans)}, Missing spans not mapped to SpaCy spans: {missing_spans}\")\n",
    "\n",
    "    if debug:\n",
    "        doc = random.choice(docs)\n",
    "        logger.info(\"Sample SpaCy spans, for line:\")\n",
    "        logger.info(f\"Text: {doc.text}\")\n",
    "        logger.info(f\"Spans: {[(span.text, span.label_) for span in doc.spans['sc']]} from source: {doc.user_data['source_info']}\")\n",
    "\n",
    "    if debug:\n",
    "        for data_entry in data:\n",
    "            entry_text = data_entry[1] if len(data_entry) == 3 else data_entry[0]\n",
    "            if entry_text == doc.text:\n",
    "                logger.info(\"\\nOriginal table spans, for the same line:\")\n",
    "                annot_field = 2 if len(data_entry) == 3 else 1\n",
    "                for span in data_entry[annot_field]['spans']:\n",
    "                    span_text = entry_text[span[0]:span[1]]\n",
    "                    logger.info(f\"{span_text}, {span[2]}\")\n",
    "                break\n",
    "        \n",
    "    return docs\n",
    "\n",
    "def create_label_array(docs: List) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a NumPy array representing the labels for each document.\n",
    "\n",
    "    Args:\n",
    "        docs: A list of Doc objects.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array of shape (len(docs), num_unique_labels).\n",
    "    \"\"\"\n",
    "    all_labels = set()\n",
    "    for doc in docs:\n",
    "        for span in doc.spans[\"sc\"]:\n",
    "            if isinstance(span.label_, list):\n",
    "                all_labels.update(span.label_)\n",
    "            else:\n",
    "                all_labels.add(span.label_)\n",
    "\n",
    "    unique_labels = sorted(all_labels)\n",
    "    labels_nd_array = np.zeros((len(docs), len(unique_labels)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_labels = set(span.label_ for span in doc.spans[\"sc\"])\n",
    "        labels_nd_array[i, [unique_labels.index(label) for label in doc_labels]] = 1\n",
    "\n",
    "    return labels_nd_array\n",
    "\n",
    "def split_docs(merged_train_data, nlp, form, output_dir, store_user_data: bool = True, output_file_name: Optional[str] = None, debug=None):\n",
    "    \"\"\"\n",
    "    Split the input data into train, dev, and test sets, and serialize them into binary files.\n",
    "\n",
    "    Args:\n",
    "        merged_train_data: The input data to be split.\n",
    "        nlp: The natural language processing object.\n",
    "        form: The normalization form.\n",
    "        output_dir: The output directory for serialized files.\n",
    "        store_user_data: Whether to store user data in the serialized files.\n",
    "        output_file_name: The base name for the output file (optional).\n",
    "    \"\"\"\n",
    "\n",
    "    debug = debug if debug is not None else DEBUG\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists\n",
    "    \n",
    "    # Check if spancat component exists\n",
    "    spancat_exists = \"spancat\" in nlp.pipe_names\n",
    "    if not spancat_exists:\n",
    "        logger.warning(\"SpanCategorizer component not found in the pipeline. Adding it to the pipeline.\")\n",
    "        nlp.add_pipe(\"spancat\", last=True)\n",
    "    else:\n",
    "        logger.debug(\"SpanCategorizer component found in the pipeline.\")\n",
    "        spancat = nlp.get_pipe(\"spancat\")\n",
    "        spancat.cfg[\"spans_key\"] = \"sc\"\n",
    "    \n",
    "    # Process the merged train data to get docs\n",
    "    docs = process_sentences(merged_train_data, nlp, output_dir, output_file_name=output_file_name, form=form)\n",
    "\n",
    "    if docs:  # Check that docs is non-empty to avoid errors\n",
    "        # Create a NumPy array representing the labels for each document\n",
    "        labels_nd_array = create_label_array(docs)\n",
    "\n",
    "        # Split the docs into train and test sets using multilabel stratification\n",
    "        X_train, X_test = multilabel_train_test_split(docs, stratify=labels_nd_array, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "        # Split the test set into dev and test sets using multilabel stratification\n",
    "        X_dev, X_test = multilabel_train_test_split(X_test, stratify=create_label_array(X_test), test_size=0.5, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        logger.info(f\"No documents found for normalization form {form}. Skipping serialization.\")\n",
    "        return  # Early return if docs is empty to avoid proceeding with undefined variables\n",
    "\n",
    "    subsets = {'train': X_train, 'dev': X_dev, 'test': X_test}\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel serialization\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for subset_name, subset_docs in subsets.items():\n",
    "            if output_file_name is not None:\n",
    "                output_path = output_dir / f\"{subset_name}/spancat_{subset_name}/{output_file_name}_{subset_name}_{form}.spacy\"\n",
    "            else:\n",
    "                output_path = output_dir / f\"{subset_name}/spancat_{subset_name}/spancat_{subset_name}_{form}.spacy\"\n",
    "\n",
    "            future = executor.submit(serialize_docs, subset_docs, output_path, store_user_data, form, subset_name)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            logger.debug(result) if DEBUG else None\n",
    "\n",
    "    subsets = {'train': X_train, 'dev': X_dev, 'test': X_test}\n",
    "\n",
    "    # Print the number of docs in each set\n",
    "    logger.info(f\"Train: {len(X_train)}\")\n",
    "    logger.info(f\"Test: {len(X_test)}\")\n",
    "    logger.info(f\"Dev: {len(X_dev)}\")\n",
    "\n",
    "def serialize_docs(docs, output_path, store_user_data, form, subset_name, debug=None):\n",
    "    \"\"\"\n",
    "    Serialize a list of Doc objects into a binary file.\n",
    "\n",
    "    Args:\n",
    "        docs: A list of Doc objects.\n",
    "        output_path: The output file path.\n",
    "        store_user_data: Whether to store user data in the serialized file.\n",
    "        form: The normalization form.\n",
    "        subset_name: The name of the subset (train, dev, or test).\n",
    "\n",
    "    Returns:\n",
    "        A success message.\n",
    "    \"\"\"\n",
    "\n",
    "    debug = debug if debug is not None else DEBUG\n",
    "\n",
    "    doc_bin = DocBin(docs=docs, store_user_data=store_user_data)\n",
    "    doc_bin.to_disk(output_path)\n",
    "    return f\"Saved {len(docs)} docs for normalization form {form} to {output_path}\"\n",
    "\n",
    "def multilabel_train_test_split(*arrays,\n",
    "                                test_size=None,\n",
    "                                train_size=None,\n",
    "                                random_state=None,\n",
    "                                shuffle=True,\n",
    "                                stratify=None):\n",
    "    \"\"\"\n",
    "    Train test split for multilabel classification. Uses the algorithm from:\n",
    "    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.\n",
    "    \"\"\"\n",
    "    if stratify is None:\n",
    "        return train_test_split(*arrays, test_size=test_size, train_size=train_size,\n",
    "                                random_state=random_state, stratify=None, shuffle=shuffle)\n",
    "\n",
    "    assert shuffle, \"Stratified train/test split is not implemented for shuffle=False\"\n",
    "\n",
    "    n_arrays = len(arrays)\n",
    "    arrays = indexable(*arrays)\n",
    "    n_samples = _num_samples(arrays[0])\n",
    "    n_train, n_test = _validate_shuffle_split(\n",
    "        n_samples, test_size, train_size, default_test_size=0.25\n",
    "    )\n",
    "    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=RANDOM_STATE)\n",
    "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
    "\n",
    "    return list(\n",
    "        chain.from_iterable(\n",
    "            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences_and_serialize(input_path, nlp, normalization_forms, output_dir, debug=None, debug_patterns=None, debug_inception=None, output_file_name: str = None):\n",
    "\n",
    "    debug = debug if debug is not None else app_config.DEBUG\n",
    "\n",
    "    for form in normalization_forms:\n",
    "        # Prepare the training data for the current normalization form from Coda\n",
    "        global INCEPTION_TRAIN_DATA\n",
    "        global TRAIN_DATA\n",
    "        global not_found_list\n",
    "        global MERGED_TRAIN_DATA\n",
    "        \n",
    "        INCEPTION_TRAIN_DATA = []  # Reset INCEPTION_TRAIN_DATA for each form\n",
    "        \n",
    "        TRAIN_DATA, not_found_list = prepare_training_data(df_final, form=form, debug=debug, debug_patterns=debug_patterns, debug_inception=debug_inception, **app_config.__dict__)\n",
    "        \n",
    "        \n",
    "        # Prepare the training data for the current normalization form from INCEpTION\n",
    "        for file_path in Path(input_path).glob(\"*.zip\"):\n",
    "            print(\"file name: \",file_path.name)  # Progress indicator\n",
    "            extract_and_process_zip_files(file_path, form, debug, debug_inception)\n",
    "        \n",
    "        #merge the two training data, while removing duplicates\n",
    "        MERGED_TRAIN_DATA = TRAIN_DATA + INCEPTION_TRAIN_DATA\n",
    "        \n",
    "        split_docs(MERGED_TRAIN_DATA, nlp, form=form, output_dir=output_dir, output_file_name=output_file_name, debug=debug)\n",
    "        return MERGED_TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "# Assuming `nlp` is your preloaded SpaCy model\n",
    "nlp = spacy.load(\"../training/SageMaker/transformer/PROIEL/Assembled/pos-trf-11-may-2024/model-best/\")  # Example with a blank English model. Replace according to your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "process_sentences_and_serialize(input_path=Path(\"../assets/NER_assets/INCEpTION_files/\"), nlp=nlp, normalization_forms=['NFKC'], output_dir=Path(\"../corpus/\"), debug=False, debug_patterns=False, debug_inception=False, output_file_name=\"spancat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "\n",
    "def visualize_label_distribution(nlp_model_path: spacy.language.Language, choice: str='ents', form: str='NFKD') -> Tuple[List[Doc], List[Doc], List[Doc]]:\n",
    "    # Check if nlp exists\n",
    "    try:\n",
    "        nlp\n",
    "    except NameError:\n",
    "        nlp = spacy.load(nlp_model_path)\n",
    "    \n",
    "    # Check if spancat exists\n",
    "    if 'spancat' not in nlp.pipe_names:\n",
    "        spancat = nlp.add_pipe(\"spancat\")\n",
    "        spancat.cfg[\"spans_key\"] = \"sc\"\n",
    "        \n",
    "    # Load the train, test and dev data\n",
    "    FORMAT = form\n",
    "    folder_name = \"ner\" if choice == \"ents\" else \"spancat\"\n",
    "    file_name = \"ner\" if choice == \"ents\" else \"spancat\"\n",
    "\n",
    "    train_docbin = DocBin().from_disk(\"../corpus/train/{0}_train/{1}_train_{2}.spacy\".format(folder_name, file_name, FORMAT))\n",
    "    train_docs = list(train_docbin.get_docs(nlp.vocab))\n",
    "    dev_docbin = DocBin().from_disk(\"../corpus/dev/{0}_dev/{1}_dev_{2}.spacy\".format(folder_name, file_name, FORMAT))\n",
    "    dev_docs = list(dev_docbin.get_docs(nlp.vocab))\n",
    "    test_docbin = DocBin().from_disk(\"../corpus/test/{0}_test/{1}_test_{2}.spacy\".format(folder_name, file_name, FORMAT))\n",
    "    test_docs = list(test_docbin.get_docs(nlp.vocab))\n",
    "\n",
    "    # Print the number of sentences in train, test and dev data\n",
    "    print (\"train:\", len(train_docs), \"dev:\", len(dev_docs), \"test:\", len(test_docs))\n",
    "    # print names of files\n",
    "    print (\"train:\", \"../corpus/train/{0}_train/{1}_train_{2}.spacy\".format(folder_name, file_name, FORMAT))\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    # Extract labels from each dataset\n",
    "    if choice == 'ents':\n",
    "        train_data_labels = pd.DataFrame([ent.label_ for doc in train_docs for ent in doc.ents], columns=['labels'])\n",
    "        dev_data_labels = pd.DataFrame([ent.label_ for doc in dev_docs for ent in doc.ents], columns=['labels'])\n",
    "        test_data_labels = pd.DataFrame([ent.label_ for doc in test_docs for ent in doc.ents], columns=['labels'])\n",
    "    elif choice == 'spans':\n",
    "        train_data_labels = pd.DataFrame([span.label_ for doc in train_docs for span in doc.spans[\"sc\"]], columns=['labels'])\n",
    "        dev_data_labels = pd.DataFrame([span.label_ for doc in dev_docs for span in doc.spans[\"sc\"]], columns=['labels'])\n",
    "        test_data_labels = pd.DataFrame([span.label_ for doc in test_docs for span in doc.spans[\"sc\"]], columns=['labels'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid choice. Choose between 'ents' and 'spans'.\")\n",
    "\n",
    "    # Add a 'dataset' column to each dataframe\n",
    "    train_data_labels['dataset'] = 'train'\n",
    "    dev_data_labels['dataset'] = 'dev'\n",
    "    test_data_labels['dataset'] = 'test'\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    all_data_labels = pd.concat([train_data_labels, dev_data_labels, test_data_labels], ignore_index=True)\n",
    "\n",
    "    # Plot the distribution of labels in each dataset\n",
    "    plt.figure(figsize=(20,10))\n",
    "    barplot1 = sns.countplot(data=all_data_labels, x='labels', hue='dataset')\n",
    "    plt.title(f\"Distribution of {choice} labels in the train, test, and dev data\")\n",
    "\n",
    "    # Add count numbers above each column\n",
    "    for p in barplot1.patches:\n",
    "        barplot1.annotate(format(p.get_height(), '.0f'), \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                        ha = 'center', va = 'center', \n",
    "                        xytext = (0, 10), \n",
    "                        textcoords = 'offset points')\n",
    "\n",
    "    # Plot the frequency of labels in the train data\n",
    "    train_data_labels = train_data_labels['labels'].value_counts().reset_index()\n",
    "    train_data_labels.columns = ['label', 'count']  # rename columns\n",
    "    plt.figure(figsize=(20,10))\n",
    "    barplot2 = sns.barplot(data=train_data_labels, x='label', y='count')  # use new column names\n",
    "    plt.title(f\"Frequency of {choice} labels in the train data\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add count numbers above each column\n",
    "    for i, row in train_data_labels.iterrows():\n",
    "        barplot2.text(i, row['count'], row['count'], color='black', ha=\"center\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return train_docs, dev_docs, test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the distribution of ner labels in the train, test and dev data\n",
    "#train_docs, dev_docs, test_docs = visualize_label_distribution(\"../training/SageMaker/transformer/PROIEL/Assembled/pos-trf-11-may-2024/model-best/\", choice='ents', form='NFKD')\n",
    "\n",
    "# draw the distribution of spancat labels in the train, test and dev data\n",
    "train_docs, dev_docs, test_docs = visualize_label_distribution(\"../training/SageMaker/transformer/PROIEL/Assembled/pos-trf-11-may-2024/model-best/\", choice='spans', form='NFKD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train, test and dev data\n",
    "#import spacy\n",
    "#from spacy.tokens import DocBin\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")\n",
    "FORMAT = 'NFKD'\n",
    "train_docbin = DocBin().from_disk(\"../corpus/train/ner_train/ner_train_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "train_docs = list(train_docbin.get_docs(nlp.vocab))\n",
    "dev_docbin = DocBin().from_disk(\"../corpus/dev/ner_dev/ner_dev_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "dev_docs = list(dev_docbin.get_docs(nlp.vocab))\n",
    "test_docbin = DocBin().from_disk(\"../corpus/test/ner_test/ner_test_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "test_docs = list(test_docbin.get_docs(nlp.vocab))\n",
    "# count sentences in train, test and dev data\n",
    "print (\"train:\", len(train_docs), \"dev:\", len(dev_docs), \"test:\", len(test_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"../training/SageMaker/transformer/PROIEL/Assembled/pos-trf-11-may-2024/model-best/\")  # Example with a blank English model. Replace according to your model.\n",
    "spancat = nlp.add_pipe(\"spancat\")\n",
    "spancat.cfg[\"spans_key\"] = \"sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train, test and dev data\n",
    "#import spacy\n",
    "from spacy.tokens import DocBin\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")\n",
    "FORMAT = 'NFKD'\n",
    "train_docbin = DocBin().from_disk(\"../corpus/train/spancat_train/spancat_train_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "train_docs = list(train_docbin.get_docs(nlp.vocab))\n",
    "dev_docbin = DocBin().from_disk(\"../corpus/dev/spancat_dev/spancat_dev_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "dev_docs = list(dev_docbin.get_docs(nlp.vocab))\n",
    "test_docbin = DocBin().from_disk(\"../corpus/test/spancat_test/spancat_test_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "test_docs = list(test_docbin.get_docs(nlp.vocab))\n",
    "# count sentences in train, test and dev data\n",
    "print (\"train:\", len(train_docs), \"dev:\", len(dev_docs), \"test:\", len(test_docs))\n",
    "# find sentence and print spans\n",
    "for doc in test_docs:\n",
    "    # checking a sentenc that had a word with two labels\n",
    "    #if doc.text == \"ἢν δὲ ὀλίγον ᾖ καὶ ἐς ἀμφοτέρας τὰς φλέβας τὸν κατάρροον ποιήσηται ἢ ἐς τὰ ἐπὶ θάτερα, περιγίνονται ἐπίσημα ἐόντα\": #NFKD\n",
    "    #if doc.text == \"ἢν δὲ ὀλίγον ᾖ καὶ ἐς ἀμφοτέρας τὰς φλέβας τὸν κατάρροον ποιήσηται ἢ ἐς τὰ ἐπὶ θάτερα, περιγίνονται ἐπίσημα ἐόντα\": #NFKC\n",
    "    #if doc.text == \"ἐνίοις μὲν γὰρ εὐρύτερον τὸ πρὸς τῇ κοιλίᾳ, τὸ δὲ πρὸς τῷ τέλει στενώτερον (διόπερ αἱ κύνες μετὰ πόνου προΐενται τὴν τοιαύτην περίττωσιν), τοῖς δὲ πλείοσιν ἄνωθεν στενώτερον, πρὸς τῷ τέλει δʼ εὐρύτερον\":\n",
    "    #if doc.text == \"φρένες δὲ προσπεφύκασι τῷ ἥπατι ἃς οὐ ῥηΐδιον χωρίσαι. δισσαὶ δὲ ἀπὸ | κληΐδων αἱ μὲν ἔνθεν αἱ δὲ ἔνθεν ὑπὸ στῆθος ἐς ἦτρον·\": # nfkc\n",
    "    #if doc.text == \"φρένες δὲ προσπεφύκασι τῷ ἥπατι ἃς οὐ ῥηΐδιον χωρίσαι. δισσαὶ δὲ ἀπὸ | κληΐδων αἱ μὲν ἔνθεν αἱ δὲ ἔνθεν ὑπὸ στῆθος ἐς ἦτρον·\": # nfkd\n",
    "    #if doc.text ==\"Ἅμα δʼ ἡ ἀνάπνευσις καὶ ἔκπνευσις γίνεται εἰς τὸ στῆθος, καὶ ἀδύνατον χωρὶς τοῖς μυκτῆρσιν ἀναπνεῦσαι ἢ ἐκπνεῦσαι, διὰ τὸ ἐκ τοῦ στήθους εἶναι τὴν ἀναπνοὴν καὶ ἐκπνοὴν κατὰ τὸν γαργαρεῶνα, καὶ μὴ ἐκ τῆς κεφαλῆς τινι μέρει\": # nfkd\n",
    "    # chcking a sentence that had a NaN value before\n",
    "    #if doc.text == \"ἡ δʼ ἑτέρη ἄνω τείνει διὰ τῶν φρενῶν καὶ τοῦ πλεύμονος τῶν δεξιῶν, ἀπέσχισται δὲ καὶ ἐς τὴν καρδίην καὶ ἐς τὸν βραχίονα τὸν δεξιόν, καὶ τὸ δὲ λοιπὸν ἄνω φέρει διὰ τῆς κληῗδος ἐς τὰ δεξιὰ τοῦ αὐχένος, ἐς αὐτὸ τὸ δέρμα ὥστε κατάδηλος εἶναι, παρʼ αὐτὸ δὲ τὸ οὖς κρύπτεται καὶ ἐνταῦθα σχίζεται\":\n",
    "        print(\"doc user data: \", doc.user_data)\n",
    "        # find spans that have the same token and more than one label\n",
    "        \n",
    "        print(doc.text)\n",
    "        for span in doc.spans[\"sc\"]:\n",
    "            print(span.text, span.start_char, span.end_char, span.label_)\n",
    "            # print character text in location of indices of span\n",
    "            #print(doc.text[span.start_char:span.end_char])      \n",
    "        # print spansgroup assuming you have a SpanGroup named 'sc' in doc.spans\n",
    "        span_group = doc.spans[\"sc\"]\n",
    "\n",
    "        print(f\"SpanGroup '{span_group.name}' contains {len(span_group)} spans:\")\n",
    "        for span in span_group:\n",
    "            print(f\"- Span: '{span.text}' [{span.start_char}, {span.end_char}], Labels: {span.label_}\")\n",
    "        for span in doc.spans[\"sc\"]:\n",
    "            print(type(span.text), span.text, type(span.start_char), span.start_char, type(span.end_char), span.end_char, type(span.label_), span.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pipeline for catspan\n",
    "nlp.pipe_names\n",
    "# get spancat component\n",
    "spancat = nlp.get_pipe(\"spancat\")\n",
    "# get spancat component config\n",
    "spancat.cfg\n",
    "# use spans_key to get spans\n",
    "spancat.cfg[\"spans_key\"]\n",
    "from spacy.tokens import Span\n",
    "\n",
    "Span.set_extension(\"labels\", default=None, force=True)\n",
    "\n",
    "for doc in train_docs:\n",
    "    seen_tokens = {}\n",
    "    for span in doc.spans[spancat.cfg[\"spans_key\"]]:\n",
    "        if span.root.i not in seen_tokens:\n",
    "            seen_tokens[span.root.i] = [span.label_]\n",
    "            #print(f\"NOT SEEEEN Token: {doc[span.root.i]}, Labels: {span.label_}\")\n",
    "        else:\n",
    "            seen_tokens[span.root.i].append(span.label_)\n",
    "            #print(f\"ALREADY SEEN Token: {doc[span.root.i]}, Labels: {span.label_}\")\n",
    "    \n",
    "    for token_index, labels in seen_tokens.items():\n",
    "        if len(set(labels)) < len(labels):\n",
    "            print(f\"Token: {doc[token_index]}, Labels: {labels}, {doc.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug ner data in spacy\n",
    "import random\n",
    "# sample random from train_docs\n",
    "doc = random.choice(train_docs)\n",
    "print (doc.text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n",
    "print (\"Spacy entities, for line:\")\n",
    "print (doc.text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def print_words_at_indices(sentence, entities, source):\n",
    "    for start, end, label in entities:\n",
    "        word = sentence[start:end]\n",
    "        display(Markdown(sentence[:start] + f\"**[{word}]**\" + sentence[end:]))\n",
    "        print(f\"Word: {word}, Label: {label}, Source: {source}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming TRAIN_DATA2 is already defined and is not empty.\n",
    "#sentence, entities_dict = TRAIN_DATA[207]  # Get the first sentence and its entities.\n",
    "#entities = entities_dict['entities']  # Get the list of entity tuples.\n",
    "#print_words_at_indices(sentence, entities)\n",
    "sentence, entities_dict, source = MERGED_TRAIN_DATA[252]  # Get the first sentence and its entities.\n",
    "entities = entities_dict['entities']  # Get the list of entity tuples.\n",
    "print_words_at_indices(sentence, entities, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_spans(obj1, obj2):\n",
    "    \"\"\"\n",
    "    Compares the spans of two spaCy objects.\n",
    "    \n",
    "    Args:\n",
    "        obj1 (spacy.tokens.DocBin): The first spaCy object.\n",
    "        obj2 (spacy.tokens.DocBin): The second spaCy object.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    docs1 = list(obj1.get_docs(nlp.vocab))\n",
    "    docs2 = list(obj2.get_docs(nlp.vocab))\n",
    "    \n",
    "    if len(docs1) != len(docs2):\n",
    "        print(f\"Objects have different number of documents: {len(docs1)} vs {len(docs2)}\")\n",
    "        return\n",
    "    \n",
    "    for doc1, doc2 in zip(docs1, docs2):\n",
    "        if doc1.text != doc2.text:\n",
    "            print(f\"Documents have different text:\\n{doc1.text}\\n{doc2.text}\")\n",
    "            continue\n",
    "        \n",
    "        spans1 = list(doc1.spans[\"sc\"])\n",
    "        spans2 = list(doc2.spans[\"sc\"])\n",
    "        \n",
    "        if len(spans1) != len(spans2):\n",
    "            print(f\"Documents have different number of spans: {len(spans1)} vs {len(spans2)}\")\n",
    "            print(\"Spans in doc1:\")\n",
    "            for span in spans1:\n",
    "                print(f\"- Span: '{span.text}' [{span.start_char}, {span.end_char}], Labels: {span.label_}\")\n",
    "            print(\"Spans in doc2:\")\n",
    "            for span in spans2:\n",
    "                print(f\"- Span: '{span.text}' [{span.start_char}, {span.end_char}], Labels: {span.label_}\")\n",
    "            continue\n",
    "        \n",
    "        for span1, span2 in zip(spans1, spans2):\n",
    "            if (span1.start_char != span2.start_char or\n",
    "                span1.end_char != span2.end_char or\n",
    "                span1.label_ != span2.label_):\n",
    "                print(f\"Document 1 text: {doc1.text}\")\n",
    "                print(f\"Document 1 user data: {doc1.user_data}\")\n",
    "                print(f\"Document 2 text: {doc2.text}\")\n",
    "                print(f\"Document 2 user data: {doc2.user_data}\")\n",
    "                print(f\"Span 1: '{span1.text}' [{span1.start_char}, {span1.end_char}], Labels: {span1.label_}\")\n",
    "                #print all the spans in this doc\n",
    "                for span in spans1:\n",
    "                    print(f\"- Span: '{span.text}' [{span.start_char}, {span.end_char}], Labels: {span.label_}\")\n",
    "                print(f\"Span 2: '{span2.text}' [{span2.start_char}, {span2.end_char}], Labels: {span2.label_}\")\n",
    "                 #print all the spans in this doc\n",
    "                for span in spans2:\n",
    "                    print(f\"- Span: '{span.text}' [{span.start_char}, {span.end_char}], Labels: {span.label_}\")\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.load(\"../training/SageMaker/transformer/PROIEL/Assembled/pos-trf-11-may-2024/model-best/\")  # Example with a blank English model. Replace according to your model.\n",
    "spancat = nlp.add_pipe(\"spancat\")\n",
    "spancat.cfg[\"spans_key\"] = \"sc\"\n",
    "\n",
    "# Load your spaCy objects\n",
    "obj1 = DocBin().from_disk(\"../corpus/train/spancat_train/spancat_old_train_NFKD.spacy\")\n",
    "obj2 = DocBin().from_disk(\"../corpus/train/spancat_train/spancat_train_NFKD.spacy\")\n",
    "\n",
    "# Compare the spans\n",
    "compare_spans(obj1, obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"Greek SpanCategorizer sweeps\", entity=\"atlomy-nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "from pathlib import Path\n",
    "from spacy.training.loop import train\n",
    "from spacy.training.initialize import init_nlp\n",
    "from spacy import util\n",
    "from thinc.api import Config\n",
    "import wandb\n",
    "import spacy\n",
    "\n",
    "spacy.require_gpu()\n",
    "\n",
    "def sweep_spancategorizer(default_config: Path, training_data: Path, dev_data: Path, model_source: Path, output_path: Path):\n",
    "    sweep_config = {\"method\": \"bayes\"}\n",
    "    metric = {\"name\": \"spans_sc_f\", \"goal\": \"maximize\"}\n",
    "    sweep_config[\"metric\"] = metric\n",
    "    early_terminate = {\"type\": \"hyperband\", \"min_iter\": 3, \"s\": 2, \"eta\": 3}\n",
    "    parameters_dict = {\n",
    "        \"training.dropout\": {\"distribution\": \"uniform\", \"min\": 0.1, \"max\": 0.4},\n",
    "        \"training.optimizer.learn_rate\": {\"distribution\": \"uniform\", \"min\": 0.00001, \"max\": 0.1},\n",
    "        \"training.batcher.size\": {\"distribution\": \"int_uniform\", \"min\": 64, \"max\": 1000},\n",
    "        \"nlp.batch_size\": {\"values\": [128, 256, 512, 1024]},\n",
    "        \"components.spancat.threshold\": {\"distribution\": \"uniform\", \"min\": 0.1, \"max\": 0.5}\n",
    "    }\n",
    "    sweep_config[\"early_terminate\"] = early_terminate\n",
    "    sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "    def train_spacy():\n",
    "        loaded_local_config = util.load_config(default_config)\n",
    "        with wandb.init() as run:\n",
    "            sweeps_config = Config(util.dot_to_dict(run.config))\n",
    "            merged_config = Config(loaded_local_config).merge(sweeps_config)\n",
    "\n",
    "            # Update paths\n",
    "            merged_config[\"paths\"][\"train\"] = training_data\n",
    "            merged_config[\"paths\"][\"dev\"] = dev_data\n",
    "            merged_config[\"paths\"][\"source_path\"] = model_source\n",
    "            merged_config[\"training\"][\"max_epochs\"] = 30  # Set a reasonable max_epochs value\n",
    "\n",
    "            print(merged_config)\n",
    "            nlp = init_nlp(merged_config, use_gpu=0)\n",
    "            output_path.mkdir(parents=True, exist_ok=True)\n",
    "            train(nlp, output_path, use_gpu=0)\n",
    "\n",
    "            # Save the final model\n",
    "            output_dir = output_path / f\"final_model\"\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            nlp.to_disk(output_dir)\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"Greek SpanCategorizer sweeps\")\n",
    "    wandb.agent(sweep_id, train_spacy, count=20)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_spancategorizer(default_config=\"../configs/ner_2Xnorm-SpanCategorizer_new.cfg\", training_data=\"../corpus/train/spancat_train/\", dev_data=\"../corpus/dev/spancat_dev/\", model_source=\"../training/SageMaker/transformer/PROIEL/Assembled/pos-trf-11-may-2024/model-best\", output_path=Path(\"../training/ATLOMY_G_NER_pipeline/spancat_210524_sweep/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlomy_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
