{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11315,
     "status": "ok",
     "timestamp": 1655285213416,
     "user": {
      "displayName": "Roey Zaworbach",
      "userId": "02944633912131916379"
     },
     "user_tz": -180
    },
    "id": "GSq6f9MgNhG_",
    "outputId": "212eea25-3e3a-46f3-926a-d090faf6efa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "#!pip install --upgrade spacy\n",
    "import spacy\n",
    "from spacy.util import compounding, minibatch\n",
    "from spacy import displacy\n",
    "# Uncomment if you want Spacy to use GPU for training. Note - this will use transformer architecture\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/Lemmatize_transformer_morefiles/lemmatizer/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../assets/NER_assets/Ancient_Words.csv\"\n",
    "# read csv file\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to fit code\n",
    "df.rename(columns = {'Word':'Keyword', 'Category Types':'Label'}, inplace = True)\n",
    "# If a cell is empty (NaN), Fill it with the value in its parallel \"Early\" column\n",
    "for row in df:\n",
    "    df['Quote'].fillna(df['Early Quote'], inplace=True)\n",
    "    df['Word Before'].fillna(df['Early Word Before'], inplace=True)\n",
    "    df['Word After'].fillna(df['Early Word After'], inplace=True)\n",
    "    df['Label'].fillna(df['Early Category Type'], inplace=True)\n",
    "# remove rows with no Keyword\n",
    "df = df.dropna(subset=['Keyword'])\n",
    "# Remove any row that isn't Greek\n",
    "pat = '[ء-ي]+'\n",
    "#df.Keyword.str.contains(pat)\n",
    "df = df[~df.Keyword.str.contains(pat, na=False)]\n",
    "#replace new line in df column\n",
    "df['Keyword'].replace('\\n', '', regex=True, inplace=True)\n",
    "#replace numbers in df\n",
    "df.replace('\\d+', '', regex=True, inplace=True)\n",
    "#replace hyphens in df column\n",
    "df.replace('-', '', regex=True, inplace=True)\n",
    "# replace comma in df column\n",
    "df['Keyword'].replace(',', '', regex=True, inplace=True)\n",
    "#replace period in df column\n",
    "df['Keyword'].replace('\\.', '', regex=True, inplace=True)\n",
    "#replace interpunkt in df column\n",
    "df['Keyword'].replace('\\·', '', regex=True, inplace=True)\n",
    "# replace multiple spaces in df column\n",
    "df.replace(' +', ' ', regex=True, inplace=True)\n",
    "# replace end puntuation spaces in df column\n",
    "df['Keyword'].replace('\\s+$', '', regex=True, inplace=True)\n",
    "\n",
    "\n",
    "#regular normalization\n",
    "#for col in df:\n",
    "#    df[col] = df[col].str.normalize('NFKD')\n",
    "\n",
    "#removing accents normalization\n",
    "#for col in df:\n",
    "#    df[col] = clean_text(df[col].str)\n",
    "\n",
    "\n",
    "# replace NAN in df column\n",
    "df = df.fillna(0)\n",
    "#reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View DataFrame\n",
    "df\n",
    "# if any of the fields \"KeyWord\", \"Quote\", \"Word Before\", \"Word After\" are \"0\", drop the row\n",
    "df = df.drop(df[df.Keyword == 0].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.drop(df[df.Quote == 0].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.drop(df[df['Word Before'] == 0].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.drop(df[df['Word After'] == 0].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.drop(df[df.Label == 0].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.drop(df[df['Lemma'] == 0].index)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a list of lists of keywords and lines, which is the format required for the keyword and lemma function. It also creates a list of keywords that failed to be processed.\n",
    "Keywords_and_lines = []\n",
    "fail = []\n",
    "\n",
    "for i in range (0,len(df['Keyword'])):\n",
    "    try:\n",
    "        Keywords_and_lines.append([df['Keyword'][i], df['Quote'][i], df['Label'][i], df['Word Before'][i], df['Word After'][i], df['Lemma'][i]])\n",
    "    except Exception:\n",
    "        fail.append(df['Keyword'][i])\n",
    "        print('Failed to append ', df['Keyword'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Keywords_and_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing Spacy Doc Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linedocs: List[Doc] = []\n",
    "keyworddocs: List[str] = []\n",
    "lemmadocs: List[str] = []\n",
    "keyword_and_lemma: dict = {}\n",
    "for i, line in enumerate(Keywords_and_lines):\n",
    "    n = i + 1\n",
    "    keyword_and_lemma[line[0]] = line[5]\n",
    "    try:\n",
    "        keyworddocs.append(line[0])\n",
    "        lemmadocs.append(line[5])\n",
    "        linedoc = nlp(line[1])\n",
    "        for token in linedoc:\n",
    "            if token.text == line[0]:\n",
    "                print(token.text, line[0], token.lemma_, line[5], \"!!!found match!!!\")\n",
    "                token.lemma_ = line[5]\n",
    "                if linedoc in linedocs:\n",
    "                    print(linedoc, \"already in linedocs\")\n",
    "                else:\n",
    "                    linedocs.append(linedoc)\n",
    "            else:\n",
    "                print(token.text, line[0], \"no match\")\n",
    "    except Exception:\n",
    "        print(\"Error on line\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554\n",
      "1533\n",
      "1533\n",
      "771\n"
     ]
    }
   ],
   "source": [
    "print(len(linedocs))\n",
    "print(len(keyworddocs))\n",
    "print(len(lemmadocs))\n",
    "print(len(keyword_and_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "class NormalizeDict:\n",
    "    def __init__(self, FILE_PATH, norm_method):\n",
    "        self.FILE_PATH = FILE_PATH\n",
    "        self.norm_method = norm_method\n",
    "        self.dendrosearch_lemma_dict = self.load_dendrosearch_lemma_dict(FILE_PATH)\n",
    "        self.dendrosearch_lemma_dict_normalized = self.normalize_dict(self.dendrosearch_lemma_dict, norm_method)\n",
    "\n",
    "    def load_dendrosearch_lemma_dict(self, FILE_PATH):\n",
    "        with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        # create dict from txt file\n",
    "        dendrosearch_lemma_dict = {}\n",
    "        for line in lines:\n",
    "            key, value = line.split('\\t')\n",
    "            # remove \\n from value\n",
    "            value = value.replace('\\n', '')\n",
    "            dendrosearch_lemma_dict[key] = value\n",
    "        return dendrosearch_lemma_dict\n",
    "    \n",
    "    def normalize_dict(self, dict_to_normalize, norm_method):\n",
    "        global dict_normalized\n",
    "        dict_normalized = {}\n",
    "        for key, value in dict_to_normalize.items():\n",
    "            try:\n",
    "                key_normalized = unicodedata.normalize(norm_method, key)\n",
    "                value_normalized = unicodedata.normalize(norm_method, value)\n",
    "                dict_normalized[key_normalized] = value_normalized\n",
    "            except TypeError:\n",
    "                print(\"TypeError for key: \", key)\n",
    "            except ValueError:\n",
    "                print(\"ValueError for key: \", key)\n",
    "            except Exception: # catch all other errors\n",
    "                print(\"Error for key: \", key)\n",
    "        print(\"Normalized dict contains\", len(dict_normalized), \"entries\")\n",
    "        #print(dict_normalized)\n",
    "        return dict_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dentest = NormalizeDict('../assets/dendrosearch_lemma_dict.txt', 'NFKC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linedocs: List[Doc] = []\n",
    "keyworddocs: List[str] = []\n",
    "lemmadocs: List[str] = []\n",
    "keyword_and_lemma: dict = {}\n",
    "for i, line in enumerate(Keywords_and_lines_normalized_NFKD):\n",
    "    n = i + 1\n",
    "    keyword_and_lemma.update({line[0]:line[5]})\n",
    "    try:\n",
    "        keyworddocs.append(line[0])\n",
    "        lemmadocs.append(line[5])\n",
    "        linedoc = nlp(line[1])\n",
    "        for token in linedoc:\n",
    "            if token.text == line[0]:\n",
    "                print(token.text, line[0], token.lemma_, line[5], \"!!!found match!!!\")\n",
    "                token.lemma_ = line[5]\n",
    "                if linedoc in linedocs:\n",
    "                    print(linedoc, \"already in linedocs\")\n",
    "                else:\n",
    "                    linedocs.append(linedoc)\n",
    "            else:\n",
    "                print(token.text, line[0], \"no match\")\n",
    "    except Exception:\n",
    "        print(\"Error on line\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linedocs: List[Doc] = []\n",
    "keyworddocs: List[str] = []\n",
    "lemmadocs: List[str] = []\n",
    "keyword_and_lemma_NFKC: dict = {}\n",
    "for i, line in enumerate(Keywords_and_lines_normalized_NFKC):\n",
    "    n = i + 1\n",
    "    keyword_and_lemma_NFKC.update({line[0]:line[5]})\n",
    "    try:\n",
    "        keyworddocs.append(line[0])\n",
    "        lemmadocs.append(line[5])\n",
    "        linedoc = nlp(line[1])\n",
    "        for token in linedoc:\n",
    "            if token.text == line[0]:\n",
    "                print(token.text, line[0], token.lemma_, line[5], \"!!!found match!!!\")\n",
    "                token.lemma_ = line[5]\n",
    "                if linedoc in linedocs:\n",
    "                    print(linedoc, \"already in linedocs\")\n",
    "                else:\n",
    "                    linedocs.append(linedoc)\n",
    "            else:\n",
    "                print(token.text, line[0], \"no match\")\n",
    "    except Exception:\n",
    "        print(\"Error on line\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linedocs: List[Doc] = []\n",
    "keyworddocs: List[str] = []\n",
    "lemmadocs: List[str] = []\n",
    "keyword_and_lemma_old: dict = {}\n",
    "for i, line in enumerate(Keywords_and_lines_normalized_NFKC):\n",
    "    n = i + 1\n",
    "    keyword_and_lemma_old.update({line[0]:line[5]})\n",
    "    try:\n",
    "        keyworddocs.append(line[0])\n",
    "        lemmadocs.append(line[5])\n",
    "        linedoc = nlp(line[1])\n",
    "        for token in linedoc:\n",
    "            if token.text == line[0]:\n",
    "                print(token.text, line[0], token.lemma_, line[5], \"!!!found match!!!\")\n",
    "                token.lemma_ = line[5]\n",
    "                if linedoc in linedocs:\n",
    "                    print(linedoc, \"already in linedocs\")\n",
    "                else:\n",
    "                    linedocs.append(linedoc)\n",
    "            else:\n",
    "                print(token.text, line[0], \"no match\")\n",
    "    except Exception:\n",
    "        print(\"Error on line\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt dict file from Assets folder\n",
    "path = '../corpus/INCEpTION_Data'\n",
    "# read txt file\n",
    "# merge all .txt files that are in the folder with keyword_and_lemma\n",
    "INCEpTION_lemma_dict = {}\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        print(file)\n",
    "        with open(path + \"/\" + file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        # create dict from txt file\n",
    "            for line in lines:\n",
    "                key, value = line.split('\\t')\n",
    "                # remove \\n from value\n",
    "                value = value.replace('\\n', '')\n",
    "                INCEpTION_lemma_dict[key] = value\n",
    "\n",
    "# check length of dict\n",
    "print(INCEpTION_lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all files into keyword_and_lemma\n",
    "full_lemma_dict = {**dendrosearch_lemma_dict, **keyword_and_lemma, **INCEpTION_lemma_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check error in dict\n",
    "clean_full_lemma_dict = {}\n",
    "for key, value in full_lemma_dict.items():\n",
    "    if not isinstance(value, str):\n",
    "        print(key, value, \"value is not a string\")\n",
    "        #replace value with key and add to dict\n",
    "        value = key\n",
    "        clean_full_lemma_dict[key] = value\n",
    "        print(key, value, \"replaced with\", key, value)\n",
    "    if not isinstance(key, str):\n",
    "        print(key, value, \"key is not a string\")\n",
    "        print(type(key), type(value))\n",
    "        #replace key with value and add to dict\n",
    "        key = value\n",
    "        clean_full_lemma_dict[key] = value\n",
    "        print(type(key), type(value))\n",
    "        print(key, value, \"replaced with\", key, value)\n",
    "    else:\n",
    "        clean_full_lemma_dict[key] = value\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = []\n",
    "lines_list = []\n",
    "labels_list = []\n",
    "word_before_list = []\n",
    "word_after_list = []\n",
    "fail = []\n",
    "lemma_list = []\n",
    "\n",
    "for i in range (0,len(df['Keyword'])):\n",
    "    keywords_list.append(df['Keyword'][i])\n",
    "    lines_list.append(df['Quote'][i])\n",
    "    labels_list.append(df['Label'][i])\n",
    "    word_before_list.append(df['Word Before'][i])\n",
    "    word_after_list.append(df['Word After'][i])\n",
    "    lemma_list.append(df['Lemma'][i])\n",
    "else:\n",
    "    fail.append(df['Keyword'][i])\n",
    "\n",
    "# Create a list of lists of keywords and lines\n",
    "Keywords_and_lines = list(zip(keywords_list, lines_list, labels_list, word_before_list, word_after_list, lemma_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print items with value \"0\" in Keywords_and_lines\n",
    "for i in Keywords_and_lines:\n",
    "    if i[5] == 0:\n",
    "        print(i[0], i[1], i[2], i[3], i[4], i[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print lines with int in word_before:\n",
    "for i in Keywords_and_lines:\n",
    "    if isinstance(i[3], int):\n",
    "        print(i[3], i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_and_lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Keywords_and_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize nfkd all items in list\n",
    "Keywords_and_lines = [(unicodedata.normalize('NFKD', keyword), unicodedata.normalize('NFKD', line), label, word_before, word_after, lemma) for keyword, line, label, word_before, word_after, lemma in Keywords_and_lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_and_lines[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize nfkc all items in list\n",
    "Keywords_and_lines = [(unicodedata.normalize('NFKC', keyword), unicodedata.normalize('NFKC'), line), label, word_before, word_after, lemma) for keyword, line, label, word_before, word_after, lemma in Keywords_and_lines]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFKC Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists from dataframe columns\n",
    "\n",
    "keywords_list = []\n",
    "lines_list = []\n",
    "labels_list = []\n",
    "word_before_list = []\n",
    "word_after_list = []\n",
    "fail = []\n",
    "lemma_list = []\n",
    "\n",
    "for i in range (0,len(df['Keyword'])):\n",
    "    keywords_list.append(df['Keyword'][i])\n",
    "    lines_list.append(df['Quote'][i])\n",
    "    labels_list.append(df['Label'][i])\n",
    "    word_before_list.append(df['Word Before'][i])\n",
    "    word_after_list.append(df['Word After'][i])\n",
    "    lemma_list.append(df['Lemma'][i])\n",
    "else:\n",
    "    fail.append(df['Keyword'][i])\n",
    "\n",
    "# Create a list of lists of keywords and lines\n",
    "Keywords_and_lines = list(zip(keywords_list, lines_list, labels_list, word_before_list, word_after_list, lemma_list))\n",
    "\n",
    "linedocs: List[Doc] = []\n",
    "keyworddocs: List[str] = []\n",
    "lemmadocs: List[str] = []\n",
    "keyword_and_lemma: dict = {}\n",
    "for i, line in enumerate(Keywords_and_lines):\n",
    "    n = i + 1\n",
    "    keyword_and_lemma.update({line[0]:line[5]})\n",
    "    try:\n",
    "        keyworddocs.append(line[0])\n",
    "        lemmadocs.append(line[5])\n",
    "        linedoc = nlp(line[1])\n",
    "        for token in linedoc:\n",
    "            if token.text == line[0]:\n",
    "                #print(token.text, line[0], token.lemma_, line[5], \"!!!found match!!!\")\n",
    "                token.lemma_ = line[5]\n",
    "                if linedoc in linedocs:\n",
    "                    print(linedoc, \"already in linedocs\")\n",
    "                else:\n",
    "                    linedocs.append(linedoc)\n",
    "            else:\n",
    "                #print(token.text, line[0], \"no match\")\n",
    "                continue\n",
    "    except Exception:\n",
    "        print(\"Error on line\", n)\n",
    "        \n",
    "        \n",
    "# merge duplicate linedocs docs that have the same text\n",
    "def merge_docs(docs: List[Doc]) -> List[Doc]:\n",
    "    \"\"\"Merge duplicate docs that have the same text.\"\"\"\n",
    "    merged_docs = []\n",
    "    seen_texts = set()\n",
    "    for doc in docs:\n",
    "        if doc.text not in seen_texts:\n",
    "            seen_texts.add(doc.text)\n",
    "            merged_docs.append(doc)\n",
    "    return merged_docs\n",
    "\n",
    "merged_linedocs_lemmas=(merge_docs(linedocs))\n",
    "\n",
    "for token in merged_linedocs_lemmas[0]:\n",
    "    print(token.text, token.lemma_)\n",
    "for key, value in keyword_and_lemma.items():\n",
    "    if not isinstance(value, str):\n",
    "        print(key, value, \"is not a string\")\n",
    "\n",
    "#for i in merged_linedocs_lemmas:\n",
    "#    for token in i:\n",
    "#        for key, value in keyword_and_lemma.items():\n",
    "#            if token.text == key:\n",
    "#                print (token.text, token.lemma_, key, \"yes\")\n",
    "            #else: \n",
    "            #    print(token.text, key, \"no\")\n",
    "            \n",
    "# load txt dict file from Assets folder\n",
    "FILE_PATH = \"../assets/dendrosearch_lemma_dict.txt\"\n",
    "# read txt file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# create dict from txt file\n",
    "dendrosearch_lemma_dict = {}\n",
    "for line in lines:\n",
    "    key, value = line.split('\\t')\n",
    "    # remove \\n from value\n",
    "    value = value.replace('\\n', '')\n",
    "    dendrosearch_lemma_dict[key] = value\n",
    "\n",
    "# check length of dict\n",
    "len(dendrosearch_lemma_dict)\n",
    "\n",
    "# normalize nfkd for dict\n",
    "unic_lemma_dict = dendrosearch_lemma_dict\n",
    "#unic_lemma_dict = {unicodedata.normalize('NFKD', key): unicodedata.normalize('NFKD', value) for key, value in unic_lemma_dict.items()}\n",
    "unic_lemma_dict = dict(unic_lemma_dict)\n",
    "#unic_lemma_dict = {key: value for key, value in unic_lemma_dict.items()}\n",
    "\n",
    "len(unic_lemma_dict)\n",
    "# merge all files into keyword_and_lemma\n",
    "#coda_lemma_dict = {#**dendrosearch_lemma_dict, **keyword_and_lemma}\n",
    "coda_lemma_dict = keyword_and_lemma\n",
    "# check error in dict\n",
    "clean_coda_lemma_dict = {}\n",
    "for key, value in coda_lemma_dict.items():\n",
    "    if not isinstance(value, str):\n",
    "        print(key, value, \"value is not a string\")\n",
    "        #replace value with key and add to dict\n",
    "        value = key\n",
    "        clean_coda_lemma_dict[key] = value\n",
    "        print(key, value, \"replaced with\", key, value)\n",
    "    if not isinstance(key, str):\n",
    "        print(key, value, \"key is not a string\")\n",
    "        print(type(key), type(value))\n",
    "        #replace key with value and add to dict\n",
    "        key = value\n",
    "        clean_coda_lemma_dict[key] = value\n",
    "        print(type(key), type(value))\n",
    "        print(key, value, \"replaced with\", key, value)\n",
    "    else:\n",
    "        clean_coda_lemma_dict[key] = value\n",
    "\n",
    "len(clean_coda_lemma_dict)\n",
    "# check bool\n",
    "for key, value in clean_coda_lemma_dict.items():\n",
    "    if not isinstance(key, str):\n",
    "        print(key, value, \"value is not a string\")\n",
    "        # remove key\n",
    "        #del full_lemma_dict[key]\n",
    "\n",
    "# save full_lemma_dict as txt file\n",
    "with open('../assets/coda_lemma_dict.txt', 'w', encoding='utf-8') as f:\n",
    "    for key, value in clean_coda_lemma_dict.items():\n",
    "        f.write(key + \"\\t\" + value + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "#load txt dict file from Assets folder\n",
    "FILE_PATH = \"../assets/coda_lemma_dict.txt\"\n",
    "# read txt file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# create dict from txt file\n",
    "full_lemma_dict1 = {}\n",
    "for line in lines:\n",
    "    key, value = line.split('\\t')\n",
    "    # remove \\n from value\n",
    "    value = value.replace('\\n', '')\n",
    "    full_lemma_dict1[key] = value\n",
    "\n",
    "db = DocBin()\n",
    "for doc in merged_linedocs_lemmas:\n",
    "    db.add(doc)\n",
    "db.to_disk(\"../corpus/merged_dataset_lemmas.spacy\")\n",
    "\n",
    "Merged_dataset = DocBin().from_disk('../corpus/merged_dataset_lemmas.spacy')\n",
    "print(len(Merged_dataset))\n",
    "\n",
    "merged_docbin = DocBin().from_disk(\"../corpus/merged_dataset_lemmas.spacy\")\n",
    "# get docs from new_docbin\n",
    "merged_docbin_docs = list(merged_docbin.get_docs(nlp.vocab))\n",
    "\n",
    "# replace lemmas in merged_linedocs with lemmas from full_lemma_dict. Count replaced lemmas\n",
    "lemma_count = 0\n",
    "for doc in merged_docbin_docs:\n",
    "    for token in doc:\n",
    "        for key, value in clean_coda_lemma_dict.items():\n",
    "            if token.text == key:\n",
    "                print(token.text, token.lemma_, key, value)\n",
    "                token.lemma_ = value\n",
    "                lemma_count += 1\n",
    "print(\"Total number of lemmas replaced:\", lemma_count)\n",
    "\n",
    "# save whole lemma dataset for evaluation\n",
    "full_lemmas_dataset = DocBin(docs=merged_docbin_docs)\n",
    "full_lemmas_dataset.to_disk(\"../corpus/full_lemmas_dataset.spacy\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFKD dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists from dataframe columns\n",
    "\n",
    "keywords_list = []\n",
    "lines_list = []\n",
    "labels_list = []\n",
    "word_before_list = []\n",
    "word_after_list = []\n",
    "fail = []\n",
    "lemma_list = []\n",
    "\n",
    "for i in range (0,len(df['Keyword'])):\n",
    "    keywords_list.append(df['Keyword'][i])\n",
    "    lines_list.append(df['Quote'][i])\n",
    "    labels_list.append(df['Label'][i])\n",
    "    word_before_list.append(df['Word Before'][i])\n",
    "    word_after_list.append(df['Word After'][i])\n",
    "    lemma_list.append(df['Lemma'][i])\n",
    "else:\n",
    "    fail.append(df['Keyword'][i])\n",
    "\n",
    "# Create a list of lists of keywords and lines\n",
    "Keywords_and_lines = list(zip(keywords_list, lines_list, labels_list, word_before_list, word_after_list, lemma_list))\n",
    "\n",
    "linedocs: List[Doc] = []\n",
    "keyworddocs: List[str] = []\n",
    "lemmadocs: List[str] = []\n",
    "keyword_and_lemma: dict = {}\n",
    "for i, line in enumerate(Keywords_and_lines):\n",
    "    n = i + 1\n",
    "    keyword_and_lemma.update({line[0]:line[5]})\n",
    "    try:\n",
    "        keyworddocs.append(line[0])\n",
    "        lemmadocs.append(line[5])\n",
    "        linedoc = nlp(line[1])\n",
    "        for token in linedoc:\n",
    "            if token.text == line[0]:\n",
    "                #print(token.text, line[0], token.lemma_, line[5], \"!!!found match!!!\")\n",
    "                token.lemma_ = line[5]\n",
    "                if linedoc in linedocs:\n",
    "                    print(linedoc, \"already in linedocs\")\n",
    "                else:\n",
    "                    linedocs.append(linedoc)\n",
    "            else:\n",
    "                #print(token.text, line[0], \"no match\")\n",
    "                continue\n",
    "    except Exception:\n",
    "        print(\"Error on line\", n)\n",
    "        \n",
    "        \n",
    "# merge duplicate linedocs docs that have the same text\n",
    "def merge_docs(docs: List[Doc]) -> List[Doc]:\n",
    "    \"\"\"Merge duplicate docs that have the same text.\"\"\"\n",
    "    merged_docs = []\n",
    "    seen_texts = set()\n",
    "    for doc in docs:\n",
    "        if doc.text not in seen_texts:\n",
    "            seen_texts.add(doc.text)\n",
    "            merged_docs.append(doc)\n",
    "    return merged_docs\n",
    "\n",
    "merged_linedocs_lemmas=(merge_docs(linedocs))\n",
    "\n",
    "for token in merged_linedocs_lemmas[0]:\n",
    "    print(token.text, token.lemma_)\n",
    "for key, value in keyword_and_lemma.items():\n",
    "    if not isinstance(value, str):\n",
    "        print(key, value, \"is not a string\")\n",
    "\n",
    "#for i in merged_linedocs_lemmas:\n",
    "#    for token in i:\n",
    "#        for key, value in keyword_and_lemma.items():\n",
    "#            if token.text == key:\n",
    "#                print (token.text, token.lemma_, key, \"yes\")\n",
    "            #else: \n",
    "            #    print(token.text, key, \"no\")\n",
    "            \n",
    "# load txt dict file from Assets folder\n",
    "FILE_PATH = \"../assets/dendrosearch_lemma_dict.txt\"\n",
    "# read txt file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# create dict from txt file\n",
    "dendrosearch_lemma_dict = {}\n",
    "for line in lines:\n",
    "    key, value = line.split('\\t')\n",
    "    # remove \\n from value\n",
    "    value = value.replace('\\n', '')\n",
    "    dendrosearch_lemma_dict[key] = value\n",
    "\n",
    "# check length of dict\n",
    "len(dendrosearch_lemma_dict)\n",
    "\n",
    "# normalize nfkd for dict\n",
    "unic_lemma_dict = dendrosearch_lemma_dict\n",
    "#unic_lemma_dict = {unicodedata.normalize('NFKD', key): unicodedata.normalize('NFKD', value) for key, value in unic_lemma_dict.items()}\n",
    "unic_lemma_dict = dict(unic_lemma_dict)\n",
    "#unic_lemma_dict = {key: value for key, value in unic_lemma_dict.items()}\n",
    "\n",
    "len(unic_lemma_dict)\n",
    "# merge all files into keyword_and_lemma\n",
    "#coda_lemma_dict = {#**dendrosearch_lemma_dict, **keyword_and_lemma}\n",
    "coda_lemma_dict = keyword_and_lemma\n",
    "# check error in dict\n",
    "clean_coda_lemma_dict = {}\n",
    "for key, value in coda_lemma_dict.items():\n",
    "    if not isinstance(value, str):\n",
    "        print(key, value, \"value is not a string\")\n",
    "        #replace value with key and add to dict\n",
    "        value = key\n",
    "        clean_coda_lemma_dict[key] = value\n",
    "        print(key, value, \"replaced with\", key, value)\n",
    "    if not isinstance(key, str):\n",
    "        print(key, value, \"key is not a string\")\n",
    "        print(type(key), type(value))\n",
    "        #replace key with value and add to dict\n",
    "        key = value\n",
    "        clean_coda_lemma_dict[key] = value\n",
    "        print(type(key), type(value))\n",
    "        print(key, value, \"replaced with\", key, value)\n",
    "    else:\n",
    "        clean_coda_lemma_dict[key] = value\n",
    "\n",
    "len(clean_coda_lemma_dict)\n",
    "# check bool\n",
    "for key, value in clean_coda_lemma_dict.items():\n",
    "    if not isinstance(key, str):\n",
    "        print(key, value, \"value is not a string\")\n",
    "        # remove key\n",
    "        #del full_lemma_dict[key]\n",
    "\n",
    "# save full_lemma_dict as txt file\n",
    "with open('../assets/coda_lemma_dict.txt', 'w', encoding='utf-8') as f:\n",
    "    for key, value in clean_coda_lemma_dict.items():\n",
    "        f.write(key + \"\\t\" + value + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "#load txt dict file from Assets folder\n",
    "FILE_PATH = \"../assets/coda_lemma_dict.txt\"\n",
    "# read txt file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# create dict from txt file\n",
    "full_lemma_dict1 = {}\n",
    "for line in lines:\n",
    "    key, value = line.split('\\t')\n",
    "    # remove \\n from value\n",
    "    value = value.replace('\\n', '')\n",
    "    full_lemma_dict1[key] = value\n",
    "\n",
    "db = DocBin()\n",
    "for doc in merged_linedocs_lemmas:\n",
    "    db.add(doc)\n",
    "db.to_disk(\"../corpus/merged_dataset_lemmas.spacy\")\n",
    "\n",
    "Merged_dataset = DocBin().from_disk('../corpus/merged_dataset_lemmas.spacy')\n",
    "print(len(Merged_dataset))\n",
    "\n",
    "merged_docbin = DocBin().from_disk(\"../corpus/merged_dataset_lemmas.spacy\")\n",
    "# get docs from new_docbin\n",
    "merged_docbin_docs = list(merged_docbin.get_docs(nlp.vocab))\n",
    "\n",
    "# replace lemmas in merged_linedocs with lemmas from full_lemma_dict. Count replaced lemmas\n",
    "lemma_count = 0\n",
    "for doc in merged_docbin_docs:\n",
    "    for token in doc:\n",
    "        for key, value in clean_coda_lemma_dict.items():\n",
    "            if token.text == key:\n",
    "                print(token.text, token.lemma_, key, value)\n",
    "                token.lemma_ = value\n",
    "                lemma_count += 1\n",
    "print(\"Total number of lemmas replaced:\", lemma_count)\n",
    "\n",
    "# save whole lemma dataset for evaluation\n",
    "full_lemmas_dataset = DocBin(docs=merged_docbin_docs)\n",
    "full_lemmas_dataset.to_disk(\"../corpus/full_lemmas_dataset.spacy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords_and_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_and_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coda_lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docbin = DocBin().from_disk(\"../corpus/full_lemmas_dataset.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lemmas_dataset_docs = list(test_docbin.get_docs(nlp.vocab))\n",
    "# count the number of lemmas in the dataset\n",
    "lemma_count = 0\n",
    "for i in full_lemmas_dataset_docs:\n",
    "    for token in i:\n",
    "        lemma_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in full_lemmas_dataset_docs:\n",
    "    for token in doc:\n",
    "        print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_full_lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bool\n",
    "for key, value in clean_full_lemma_dict.items():\n",
    "    if not isinstance(key, str):\n",
    "        print(key, value, \"value is not a string\")\n",
    "        # remove key\n",
    "        #del full_lemma_dict[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full_lemma_dict as txt file\n",
    "with open('../assets/full_lemma_dict.txt', 'w', encoding='utf-8') as f:\n",
    "    for key, value in clean_full_lemma_dict.items():\n",
    "        f.write(key + \"\\t\" + value + \"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load txt dict file from Assets folder\n",
    "FILE_PATH = \"../assets/full_lemma_dict.txt\"\n",
    "# read txt file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# create dict from txt file\n",
    "full_lemma_dict1 = {}\n",
    "for line in lines:\n",
    "    key, value = line.split('\\t')\n",
    "    # remove \\n from value\n",
    "    value = value.replace('\\n', '')\n",
    "    full_lemma_dict1[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocBin()\n",
    "for doc in merged_linedocs:\n",
    "    db.add(doc)\n",
    "db.to_disk(\"../corpus/merged_linedocs.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.from_disk(\"../corpus/merged_linedocs.spacy\")\n",
    "len(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_dataset = DocBin().from_disk('../corpus/merged_linedocs.spacy')\n",
    "print(len(Merged_dataset))\n",
    "# merge all .spacy files that are in the folder (except self) with base_db.spacy\n",
    "for file in os.listdir('../corpus/INCEpTION_Data'):\n",
    "    if file.endswith(\".spacy\") and file != 'train.spacy':\n",
    "        print(file)\n",
    "        Merged_dataset.merge(DocBin().from_disk('../corpus/INCEpTION_Data/'+file))\n",
    "print(len(Merged_dataset))\n",
    "# write the merged file to disk\n",
    "Merged_dataset.to_disk('../corpus/Merged_dataset.spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_docbin = DocBin().from_disk(\"../corpus/Merged_dataset.spacy\")\n",
    "# get docs from new_docbin\n",
    "merged_docbin_docs = list(merged_docbin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace lemmas in merged_linedocs with lemmas from full_lemma_dict. Count replaced lemmas\n",
    "lemma_count = 0\n",
    "for doc in merged_docbin_docs:\n",
    "    for token in doc:\n",
    "        for key, value in clean_full_lemma_dict.items():\n",
    "            if token.text == key:\n",
    "                print(token.text, token.lemma_, key, value)\n",
    "                token.lemma_ = value\n",
    "                lemma_count += 1\n",
    "print(\"Total number of lemmas replaced:\", lemma_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tokens in lines in linedocs\n",
    "token_count = 0\n",
    "for doc in merged_docbin_docs:\n",
    "    token_count += len(doc)\n",
    "print(\"Total number of tokens in linedocs:\", token_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a word in dendrosearch_lemma_dict\n",
    "def is_in_dict(word):\n",
    "    return word in dendrosearch_lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_in_dict(\"ἐκ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_linedocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(merged_linedocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole lemma dataset for evaluation\n",
    "full_lemmas_dataset = DocBin(docs=merged_docbin_docs)\n",
    "full_lemmas_dataset.to_disk(\"../corpus/train/lemma_train/full_lemmas_dataset.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in 60:20:20 for train:valid:test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(merged_docbin_docs, test_size=0.2, random_state=42)\n",
    "dev_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# without eval, for small datasets (it's faster)\n",
    "#train_data, test_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of train, dev and test data\n",
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(dev_data)}\")\n",
    "print(f\"Number of test examples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for duplicate lines in merged_docbin_docs\n",
    "# create list of all lines\n",
    "all_lines = []\n",
    "for doc in merged_docbin_docs:\n",
    "    all_lines.append(doc.text)\n",
    "    print(doc.text)\n",
    "# create list of unique lines\n",
    "unique_lines = []\n",
    "for line in all_lines:\n",
    "    if line not in unique_lines:\n",
    "        unique_lines.append(line)\n",
    "# check if all lines are unique\n",
    "if len(all_lines) == len(unique_lines):\n",
    "    print(\"All lines are unique\")\n",
    "else:\n",
    "    print(\"There are duplicate lines\")\n",
    "    print(\"Number of all lines:\", len(all_lines))\n",
    "    print(\"Number of unique lines:\", len(unique_lines))\n",
    "    print(\"Number of duplicate lines:\", len(all_lines)-len(unique_lines))\n",
    "    print(\"Percentage of duplicate lines:\", (len(all_lines)-len(unique_lines))/len(all_lines))\n",
    "    print(\"Percentage of unique lines:\", len(unique_lines)/len(all_lines))\n",
    "    print(\"Percentage of duplicate lines:\", (len(all_lines)-len(unique_lines))/len(all_lines)*100, \"%\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if same lines are in train, dev and test data\n",
    "for doc in train_data:\n",
    "    if doc in dev_data:\n",
    "        print(\"doc in dev_data\", train_data.index(doc), dev_data.index(doc), dev_data[dev_data.index(doc)].text, \" | \", train_data[train_data.index(doc)].text)\n",
    "    #if doc in test_data:\n",
    "    #    print(\"doc in test_data\", train_data.index(doc), test_data.index(doc), test_data[dev_data.index(doc)].text, \" | \", doc.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train, dev and test data to disk\n",
    "train_docbin = DocBin(docs=train_data)\n",
    "train_docbin.to_disk(\"../corpus/train/lemma_train/lemma_train.spacy\")\n",
    "dev_docbin = DocBin(docs=dev_data)\n",
    "dev_docbin.to_disk(\"../corpus/dev/lemma_dev/lemma_dev.spacy\")\n",
    "test_docbin = DocBin(docs=test_data)\n",
    "test_docbin.to_disk(\"../corpus/test/lemma_test/lemma_test.spacy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spacy.tokens import DocBin, Doc\n",
    "# Merge all dataset files in the train folder\n",
    "\n",
    "Merged_lemma_train_dataset = DocBin().from_disk(\"../corpus/train/lemma_train/lemma_train.spacy\")\n",
    "print(len(Merged_lemma_train_dataset))\n",
    "# merge all .spacy files that are in the folder (except self) with base_db.spacy\n",
    "for file in os.listdir(\"../corpus/train/lemma_train/\"):\n",
    "    if file.endswith(\".spacy\") and file != 'lemma_train.spacy':\n",
    "        print(file)\n",
    "        Merged_lemma_train_dataset.merge(\n",
    "            DocBin().from_disk(f\"../corpus/train/lemma_train/{file}\")\n",
    "        )\n",
    "print(len(Merged_lemma_train_dataset))\n",
    "# write the merged file to disk\n",
    "Merged_lemma_train_dataset.to_disk(\"../corpus/train/lemma_train/Merged_lemma_train_dataset.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spacy.tokens import DocBin, Doc\n",
    "# Merge all dataset files in the dev folder\n",
    "\n",
    "Merged_lemma_dev_dataset = DocBin().from_disk(\"../corpus/dev/lemma_dev/lemma_dev.spacy\")\n",
    "print(len(Merged_lemma_dev_dataset))\n",
    "# merge all .spacy files that are in the folder (except self) with base_db.spacy\n",
    "for file in os.listdir(\"../corpus/dev/lemma_dev/\"):\n",
    "    if file.endswith(\".spacy\") and file != 'lemma_dev.spacy':\n",
    "        print(file)\n",
    "        Merged_lemma_dev_dataset.merge(\n",
    "            DocBin().from_disk(f\"../corpus/dev/lemma_dev/{file}\")\n",
    "        )\n",
    "print(len(Merged_lemma_dev_dataset))\n",
    "# write the merged file to disk\n",
    "Merged_lemma_dev_dataset.to_disk(\"../corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spacy.tokens import DocBin, Doc\n",
    "# Merge all dataset files in the dev folder\n",
    "\n",
    "Merged_lemma_dev_dataset = DocBin().from_disk(\"../corpus/test/lemma_test/lemma_test.spacy\")\n",
    "print(len(Merged_lemma_dev_dataset))\n",
    "# merge all .spacy files that are in the folder (except self) with base_db.spacy\n",
    "for file in os.listdir(\"../corpus/test/lemma_test/\"):\n",
    "    if file.endswith(\".spacy\") and file != 'lemma_test.spacy':\n",
    "        print(file)\n",
    "        Merged_lemma_dev_dataset.merge(\n",
    "            DocBin().from_disk(f\"../corpus/test/lemma_test/{file}\")\n",
    "        )\n",
    "print(len(Merged_lemma_dev_dataset))\n",
    "# write the merged file to disk\n",
    "Merged_lemma_dev_dataset.to_disk(\"../corpus/test/lemma_test/Merged_lemma_test_dataset.spacy\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Weights and Biases platform to log the model training\n",
    "import wandb\n",
    "wandb.init(project=\"Lemmatizer (Ancient Greek)\", entity=\"atlomy-nlp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Training of Jacobo grc_ud_proiel_trf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/senter.cfg --output ../training/senter --gpu-id 0 --paths.train ../corpus/train/grc_perseus-ud-train.spacy --paths.dev ../corpus/dev/grc_perseus-ud-dev.spacy --nlp.lang=grc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy debug config ../configs/lemmatizer_trf.cfg  --paths.train ../corpus/train/lemma_train/Merged_lemma_train_dataset.spacy --paths.dev ../corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/lemmatizer_trf.cfg  --output ../training/transformer/lemmatizer --paths.train ../corpus/train/lemma_train/Merged_lemma_train_dataset.spacy --paths.dev ../corpus/dev/lemma_dev/Merged_lemma_dev_dataset.spacy --gpu-id 0 --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy ../training/transformer/lemmatizer/model-best/ ../corpus/test/lemma_test/Merged_lemma_test_dataset.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the transformer with attributes\n",
    "\n",
    "!python -m spacy debug config ../configs/transformer.cfg --paths.train corpus/train/grc_proiel-ud-train.spacy --paths.dev corpus/dev/grc_proiel-ud-dev.spacy --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the transformer with attributes\n",
    "!python -m spacy debug data ../configs/transformer.cfg --paths.train corpus/train/grc_proiel-ud-train.spacy --paths.dev corpus/dev/grc_proiel-ud-dev.spacy --nlp.lang=grc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS and other attributes training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on seperate dataset files\n",
    "!python -m spacy train ../configs/lemmatizer_trf.cfg --output ../training/transformer_mostfiles_clean_merged/lemmatizer --gpu-id 0 --paths.train ../corpus/train/lemma_train/clean --paths.dev ../corpus/dev/lemma_dev/clean --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on a merged dataset file\n",
    "!python -m spacy train ../configs/lemmatizer_trf.cfg --output ../training/transformer_mostfiles_clean_merged/lemmatizer --gpu-id 0 --paths.train ../corpus/train/lemma_train/clean/Merged_lemma_train_clean_dataset.spacy --paths.dev ../corpus/dev/lemma_dev/clean/Merged_lemma_dev_clean_dataset.spacy --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy ../training/transformer_mostfiles_clean/lemmatizer/model-best/ ../corpus-original-with-accents/test/lemma_test/clean/Merged_lemma_test_clean_dataset.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy ../training/Lemmatize_transformer_morefiles/lemmatizer/model-best ../corpus-original-with-accents/test/lemma_test/lemma_test.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy ../training/transformer_mostfiles_clean_merged/lemmatizer/model-best/ ../corpus/test/lemma_test/clean/Merged_lemma_test_clean_dataset.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy benchmark accuracy ../training/Lemmatize_transformer_morefiles/lemmatizer/model-best ../corpus/test/lemma_test/lemma_test.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a word in full_lemma_dict\n",
    "def is_in_dict(word):\n",
    "    return word in full_lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_nlp = spacy.load(\"./Models/grc_ud_proiel_trf/grc_ud_proiel_trf-edittree-lemmatizer/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "#test_docbin = DocBin().from_disk(\"../corpus/lemma/lemma_test.spacy\")\n",
    "test_docbin_clean = DocBin().from_disk(\"../corpus/test/lemma_test/clean/Merged_lemma_test_clean_dataset.spacy\")\n",
    "test_docbin_clean_merged = DocBin().from_disk(\"../corpus/test/lemma_test/clean/Merged_lemma_test_clean_dataset.spacy\")\n",
    "# get docs from new_docbin\n",
    "#test_docbin_docs = list(test_docbin.get_docs(lemma_only_nlp.vocab))\n",
    "\n",
    "\n",
    "#text_examples = [doc.text for doc in test_docbin_docs]\n",
    "clean_test_examples = [doc.text for doc in test_docbin_clean.get_docs(lemma_nlp_clean.vocab)]\n",
    "clean_merged_test_examples = [doc.text for doc in test_docbin_clean_merged.get_docs(lemma_nlp_clean_merged.vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_examples:\n",
    "    print(\"start:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_docbin_docs:\n",
    "    print(\"start:\", doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in test_docbin_docs[0]:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_examples:\n",
    "    doc = lemma_nlp(text)\n",
    "    for token in doc:\n",
    "        print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in text_examples:\n",
    "    doc = lemma_only_nlp(text)\n",
    "    for token in doc:\n",
    "        print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "for text in text_examples:\n",
    "    print(text)\n",
    "    doc = lemma_nlp(text)\n",
    "    doc2 = lemma_only_nlp(text)\n",
    "    for token, token2 in zip(doc, doc2):\n",
    "        #check if the lemmas are the same\n",
    "        if token.lemma_ != token2.lemma_:\n",
    "            print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text), \"||\", token2.lemma_)\n",
    "        else:\n",
    "            print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "for text in clean_merged_test_examples:\n",
    "    doc = lemma_nlp_clean(text)\n",
    "    doc2 = lemma_nlp_clean_merged(text)\n",
    "    #doc3 = lemma_nlp(text)\n",
    "    for token, token2 in zip(doc, doc2):\n",
    "        #check if the lemmas are the same\n",
    "        if token.lemma_ != token2.lemma_:\n",
    "            print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_)\n",
    "            #print(\"(\", text, \")\", token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m',  \"|\", \"3: \", token3.lemma_)\n",
    "        else:\n",
    "            #print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "for text in text_examples:\n",
    "    doc = lemma_nlp(text)\n",
    "    doc2 = lemma_only_nlp(text)\n",
    "    doc3 = lemma_nlp(text)\n",
    "    for token, token2, token3 in zip(doc, doc2, doc3):\n",
    "        #check if the lemmas are the same\n",
    "        if token.lemma_ != token2.lemma_:\n",
    "            print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_,  \"|\", \"3: \", token3.lemma_)\n",
    "            #print(\"(\", text, \")\", token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m',  \"|\", \"3: \", token3.lemma_)\n",
    "        else:\n",
    "            #print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare lemmatization of two models by comparing the number of lemmas that are not in the lemma dict\n",
    "lemma_count = 0\n",
    "lemma_only_count = 0\n",
    "nlp_count = 0\n",
    "for text in text_examples:\n",
    "    doc = lemma_nlp(text)\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_count += 1\n",
    "for text in text_examples:\n",
    "    doc = lemma_only_nlp(text)\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_only_count += 1\n",
    "for text in text_examples:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            nlp_count += 1\n",
    "print(\"Number of lemmas not in lemma dict for lemma model:\", lemma_count)\n",
    "print(\"Number of lemmas not in lemma dict for lemma only model:\", lemma_only_count)\n",
    "print(\"Number of lemmas not in lemma dict for origin model:\", nlp_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare lemmatization of two models by comparing the number of lemmas that are not in the lemma dict\n",
    "lemma_count = 0\n",
    "lemma_only_count = 0 \n",
    "nlp_count = 0\n",
    "for doc in test_docbin.get_docs(lemma_nlp.vocab):\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_count += 1\n",
    "for doc in test_docbin.get_docs(lemma_only_nlp.vocab):\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_only_count += 1\n",
    "for doc in test_docbin.get_docs(nlp.vocab):\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            nlp_count += 1\n",
    "print(\"Number of lemmas not in lemma dict for lemma model:\", lemma_count)\n",
    "print(\"Number of lemmas not in lemma dict for lemma only model:\", lemma_only_count)\n",
    "print(\"Number of lemmas not in lemma dict for origin model:\", nlp_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def clean_text(text: str) -> str:\n",
    "    #Cleans the given text by stripping accents and lowercasing.\n",
    "    try:\n",
    "        non_accent_characters = [\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "        ]\n",
    "    # str.lower() works for unicode characters\n",
    "        return ''.join(non_accent_characters).lower()\n",
    "    except TypeError:\n",
    "        return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare accented and non accented models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_nlp = spacy.load(\"../training/Lemmatize_transformer_morefiles/lemmatizer/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_no_accents_nlp = spacy.load(\"../training/transformer/lemmatizer/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "#test_docbin = DocBin().from_disk(\"../corpus/lemma/lemma_test.spacy\")\n",
    "test_docbin = DocBin().from_disk(\"../corpus_morefiles_model/test/lemma_test/lemma_test.spacy\")\n",
    "test_docbin_no_accents = DocBin().from_disk(\"../corpus/test/lemma_test/Merged_lemma_test_dataset.spacy\")\n",
    "# get docs from new_docbin\n",
    "test_docbin_docs = list(test_docbin.get_docs(lemma_nlp.vocab))\n",
    "\n",
    "\n",
    "#text_examples = [doc.text for doc in test_docbin_docs]\n",
    "test_examples = [doc.text for doc in test_docbin.get_docs(lemma_nlp.vocab)]\n",
    "test_examples_no_accents = [doc.text for doc in test_docbin_no_accents.get_docs(lemma_no_accents_nlp.vocab)]\n",
    "test_docbin_no_accents_docs = list(test_docbin.get_docs(lemma_no_accents_nlp.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in test_examples:\n",
    "    print(\"start:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in test_examples_no_accents:\n",
    "    print(\"start:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a word in full_lemma_dict\n",
    "def is_in_dict(word):\n",
    "    return word in full_lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load txt dict file from Assets folder\n",
    "FILE_PATH = \"../assets/full_lemma_dict.txt\"\n",
    "# read txt file\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# create dict from txt file\n",
    "full_lemma_dict = {}\n",
    "for line in lines:\n",
    "    key, value = line.split('\\t')\n",
    "    # remove \\n from value\n",
    "    value = value.replace('\\n', '')\n",
    "    full_lemma_dict[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in test_docbin_docs[0]:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in test_examples:\n",
    "    doc = lemma_nlp(text)\n",
    "    for token in doc:\n",
    "        print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in test_examples_no_accents:\n",
    "    doc = lemma_no_accents_nlp(text)\n",
    "    for token in doc:\n",
    "        print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "for text in test_examples:\n",
    "    print(text)\n",
    "    doc = lemma_nlp(text)\n",
    "    doc2 = lemma_no_accents_nlp(text)\n",
    "    for token, token2 in zip(doc, doc2):\n",
    "        #check if the lemmas are the same\n",
    "        if token.lemma_ != token2.lemma_:\n",
    "            print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text), \"||\", token2.lemma_)\n",
    "        else:\n",
    "            print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "for text in test_examples:\n",
    "    doc = lemma_nlp(clean_text(text))\n",
    "    doc2 = lemma_no_accents_nlp(clean_text(text))\n",
    "    #doc3 = lemma_nlp(text)\n",
    "    for token, token2 in zip(doc, doc2):\n",
    "        #check if the lemmas are the same\n",
    "        if token.lemma_ != token2.lemma_:\n",
    "            print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_)\n",
    "            #print(\"(\", text, \")\", token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m',  \"|\", \"3: \", token3.lemma_)\n",
    "        else:\n",
    "            #print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare lemmatization of two models by comparing the number of lemmas that are not in the lemma dict\n",
    "lemma_count = 0\n",
    "lemma_no_accents_count = 0\n",
    "for text in test_examples:\n",
    "    doc = lemma_nlp(text)\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_count += 1\n",
    "for text in test_examples_no_accents:\n",
    "    doc = lemma_no_accents_nlp(text)\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_no_accents_count += 1\n",
    "print(\"Number of lemmas not in lemma dict for accents model:\", lemma_count)\n",
    "print(\"Number of lemmas not in lemma dict for non accents model:\", lemma_no_accents_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare lemmatization of two models by comparing the number of lemmas that are not in the lemma dict\n",
    "lemma_count = 0\n",
    "lemma_no_accents_count = 0 \n",
    "for doc in test_docbin.get_docs(lemma_nlp.vocab):\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_count += 1\n",
    "for doc in test_docbin_no_accents.get_docs(lemma_no_accents_nlp.vocab):\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_no_accents_count += 1\n",
    "print(\"Number of lemmas not in lemma dict for lemma model:\", lemma_count)\n",
    "print(\"Number of lemmas not in lemma dict for lemma only model:\", lemma_no_accents_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents definition\n",
    "import unicodedata\n",
    "def clean_text(text: str) -> str:\n",
    "    #Cleans the given text by stripping accents and lowercasing.\n",
    "    try:\n",
    "        non_accent_characters = [\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "        #or char == '̓'  # Greek coronis\n",
    "        ]\n",
    "    # str.lower() works for unicode characters\n",
    "        return ''.join(non_accent_characters).lower()\n",
    "    except TypeError:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"καὶ γὰρ ἀναπνεῖ καὶ ἐκπνεῖ ταύτῃ καὶ ὁ πταρμὸς διὰ ταύτης γίνεται πνεύματος ἀθρόου ἔξοδος σημεῖον οἰωνιστικὸν καὶ ἱερὸν μόνον τῶν πνευμάτων\"\n",
    "text =(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι.\")\n",
    "text_no_accents = clean_text(\"Ταῦτ’ οὖν εἴς τε τὸν παρόντα καὶ τὸν ἑξῆς ἅπαντα λόγον οἷον ὑποθέσεις τινὰς τῶν ἀποδείξεων λαμβάνοντες ἐν ἑκάστῳ τῶν ὀργάνων τὴν ἐξ αὐτῶν ὠφέλειαν ἐροῦμεν ἀπὸ τῶν δακτύλων αὖθις ἀρξάμενοι.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare lemmatization of two models by comparing the number of lemmas that are not in the lemma dict\n",
    "lemma_count = 0\n",
    "lemma_no_accents_count = 0\n",
    "for text in test_examples:\n",
    "    doc = lemma_nlp(clean_text(text))\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_count += 1\n",
    "for text in test_examples_no_accents:\n",
    "    doc = lemma_no_accents_nlp(clean_text(text))\n",
    "    for token in doc:\n",
    "        if not is_in_dict(token.text):\n",
    "            lemma_no_accents_count += 1\n",
    "print(\"Number of lemmas not in lemma dict for accents model:\", lemma_count)\n",
    "print(\"Number of lemmas not in lemma dict for non accents model:\", lemma_no_accents_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "doc = lemma_nlp(text)\n",
    "doc2 = lemma_no_accents_nlp(text_no_accents)\n",
    "#doc3 = lemma_nlp(text)\n",
    "for token, token2 in zip(doc, doc2):\n",
    "    #check if the lemmas are the same\n",
    "    if token.lemma_ != token2.lemma_:\n",
    "        print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_)\n",
    "        #print(\"(\", text, \")\", token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m',  \"|\", \"3: \", token3.lemma_)\n",
    "    else:\n",
    "        #print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "        print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"==\" , \"2: \", token2.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "doc = lemma_nlp(text)\n",
    "doc2 = lemma_no_accents_nlp(text)\n",
    "#doc3 = lemma_nlp(text)\n",
    "for token, token2 in zip(doc, doc2):\n",
    "    #check if the lemmas are the same\n",
    "    if token.lemma_ != token2.lemma_:\n",
    "        print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_)\n",
    "        #print(\"(\", text, \")\", token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m',  \"|\", \"3: \", token3.lemma_)\n",
    "    else:\n",
    "        #print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "        print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"==\" , \"2: \", token2.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmas of the two models side by side\n",
    "doc = lemma_no_accents_nlp(text)\n",
    "doc2 = lemma_no_accents_nlp(text_no_accents)\n",
    "#doc3 = lemma_nlp(text)\n",
    "for token, token2 in zip(doc, doc2):\n",
    "    #check if the lemmas are the same\n",
    "    if token.lemma_ != token2.lemma_:\n",
    "        print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_)\n",
    "        #print(\"(\", text, \")\", token.text, \":\\n\", '\\033[1m', \"1: \", token.lemma_, \"!=\" , \"2: \", token2.lemma_, '\\033[0m',  \"|\", \"3: \", token3.lemma_)\n",
    "    else:\n",
    "        #print(token.text, \":\", token.lemma_, \"|\" , \"in Lemma dict:\", is_in_dict(token.text))\n",
    "        print(\"(\", text, \"):\\n\", \"|\", token.text, \"|\", \"1: \", token.lemma_, \"==\" , \"2: \", token2.lemma_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./Models/grc_ud_proiel_trf/NER_S_grc_ud_proiel_trf-new_db_3-accumulate-gradient/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    cleaned = ud.normalize('NFKD', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "#Test your text\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "#clean text\n",
    "test = clean_text(test_text)\n",
    "#predict\n",
    "doc = nlp(test)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc:\n",
    "    print(token.text, ' !!! ', token.lemma_, ' !!! ', token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing new pipe with dit tree lemmatizer\n",
    "import unicodedata as ud\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    cleaned = ud.normalize('NFKD', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "#Test your text\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "#clean text\n",
    "test = clean_text(test_text)\n",
    "#predict\n",
    "doc = nlp(test)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc:\n",
    "    print(token.text, ' !!! ', token.lemma_, ' !!! ', token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp._pipe_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_nlp = spacy.load(\"./Models/grc_ud_proiel_trf/grc_ud_proiel_trf-edittree-lemmatizer-only/model-best\")\n",
    "lemmatizer_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe('trainable_lemmatizer', source=lemmatizer_nlp, before='lemmatizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move trainable lemmatizer to after the lemmatizer\n",
    "nlp.remove_pipe('trainable_lemmatizer')\n",
    "nlp.add_pipe('trainable_lemmatizer', source=lemmatizer_nlp, after='lemmatizer')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing new pipe with edit tree lemmatizer\n",
    "import unicodedata as ud\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    cleaned = ud.normalize('NFKD', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "#Test your text\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "#clean text\n",
    "test = clean_text(test_text)\n",
    "#predict\n",
    "doc = nlp(test)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "for token in doc:\n",
    "    print(token.text, ' !!! ', token.lemma_, ' !!! ', token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package nlp pipeline\n",
    "nlp.to_disk(\"./Models/grc_ud_proiel_trf/grc_ud_proiel_trf-NER-Trainable_Lemmatizer\") # save model to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy package ./Models/grc_ud_proiel_trf/grc_ud_proiel_trf-NER-Trainable_Lemmatizer ./Models/grc_ud_proiel_trf/grc_ud_proiel_trf-NER-Trainable_Lemmatizer --name grc_ud_proiel_trf-NER-Trainable_Lemmatizer --version 0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents definition\n",
    "import unicodedata\n",
    "def clean_text(text: str) -> str:\n",
    "    #Cleans the given text by stripping accents and lowercasing.\n",
    "    try:\n",
    "        non_accent_characters = [\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "        ]\n",
    "    # str.lower() works for unicode characters\n",
    "        return ''.join(non_accent_characters).lower()\n",
    "    except TypeError:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin, Doc\n",
    "\n",
    "test_data= DocBin().from_disk(\"/root/Projects/Atlomy/git/greCy_ATLOMY/corpus/test/lemma_test/lemma_test.spacy\")\n",
    "test_docbin_docs = list(test_data.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents from text and lemmas and attributes of tokens and save to new docbin\n",
    "new_test_docbin = DocBin()\n",
    "for doc in test_docbin_docs:\n",
    "    new_doc = Doc(nlp.vocab, words=[clean_text(token.text) for token in doc], spaces=[token.whitespace_ for token in doc])\n",
    "    for new_token, token in zip(new_doc, doc):\n",
    "        new_token.lemma_ = clean_text(token.lemma_)\n",
    "        new_token.pos_ = token.pos_\n",
    "        new_token.tag_ = token.tag_\n",
    "        new_token.dep_ = token.dep_\n",
    "        #new_token.shape_ = token.shape_\n",
    "        #new_token.is_alpha = token.is_alpha\n",
    "       # new_token.is_stop = token.is_stop\n",
    "    new_test_docbin.add(new_doc)\n",
    "#new_test_docbin.to_disk(\"/root/Projects/Atlomy/git/greCy_ATLOMY/corpus/train/lemma_train/aeschylus_ii_train_clean.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_docbin_docs:\n",
    "    for token in doc:\n",
    "        print(token.text, ' !!! ', token.lemma_, ' !!! ', token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in new_test_docbin.get_docs(nlp.vocab):\n",
    "    for token in doc:\n",
    "        print(token.text, ' !!! ', token.lemma_, ' !!! ', token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare test_docbin_docs with new_test_docbin\n",
    "for doc1, doc2 in zip(test_docbin_docs, new_test_docbin.get_docs(nlp.vocab)):\n",
    "    for token1, token2 in zip(doc1, doc2):\n",
    "        if clean_text(token1.text) == token2.text:\n",
    "            #print(token1.text, ' !!! ', token2.text)\n",
    "            if clean_text(token1.lemma_) == token2.lemma_:\n",
    "                #print(token1.lemma_, ' !!! ', token2.lemma_)\n",
    "                if token1.pos_ != token2.pos_:\n",
    "                    print(token1, token2, token1.pos_, ' !!! ', token2.pos_)\n",
    "                if token1.is_alpha != token2.is_alpha:\n",
    "                    print(token1, token2, token1.is_alpha, ' !!! ', token2.is_alpha)\n",
    "                if token1.is_stop != token2.is_stop:\n",
    "                    print(token1, token2, token1.is_stop, ' !!! ', token2.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents\n",
    "def remove_accents(input_str):\n",
    "    nkfd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nkfd_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_docbin_docs:\n",
    "    for token in doc:\n",
    "        #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "        print(remove_accents(token.text), remove_accents(token.lemma_), token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents from everything in the doc and save a new docbin\n",
    "new_docbin = DocBin()\n",
    "for doc in test_docbin_docs:\n",
    "    new_doc = Doc(nlp.vocab, words=[remove_accents(token.text) for token in doc], spaces=[token.whitespace_ for token in doc])\n",
    "    for token in new_doc:\n",
    "        token.lemma_ = remove_accents(token.lemma_)\n",
    "    new_docbin.add(new_doc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving preprocess txt issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print docs in new docbin\n",
    "for doc in new_docbin.get_docs(nlp.vocab):\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving text issues\n",
    "df.loc[df['Lemma'] == 'ὁ μέν...ὁ δέ']\n",
    "# create a dictionary of the words and their lemmas\n",
    "phrase_dict = {\n",
    "    'Τὸ': 'ὁ',\n",
    "    'μὲν': 'μέν',\n",
    "    'μὲν': 'μέν',\n",
    "    'δ’': 'δέ',\n",
    "    'δ': 'δέ',\n",
    "    'δὲ': 'δέ',\n",
    "    'τὸν': 'ὁ',\n",
    "    'τὴν': 'ἡ',\n",
    "    'τὴ': 'ἡ',\n",
    "    'τὸ': 'ὁ',\n",
    "    'αἱ': 'ὁ',\n",
    "    'Αἱ': 'ὁ',\n",
    "    'Τὰ': 'ὁ',\n",
    "    'τὰ': 'ὁ',\n",
    "    'τὰς': 'ὁ',\n",
    "    'τὰν': 'ὁ',\n",
    "}\n",
    "# create wrds dictionary\n",
    "\n",
    "#find rows with lemma \"ὁ μέν...ὁ δέ\"\n",
    "#df.loc[df['Lemma'] == 'ὁ μέν...ὁ δέ']\n",
    "# if more than one word, then the lemma is \"ὁ μέν...ὁ δέ\"\n",
    "for index, row in df.iterrows():\n",
    "    if row['Lemma'] == 'ὁ μέν...ὁ δέ':\n",
    "        # if only one word, then the lemma is the lemma for the word in phrase_dict.\n",
    "        if len(row['Keyword'].split()) == 1:\n",
    "            if row['Keyword'] in phrase_dict:\n",
    "                # replace the lemma with the value in the dictionary\n",
    "                row['Lemma'] = phrase_dict[row['Keyword']]\n",
    "                #replace the category type with \"Division\"\n",
    "                df.at[index, 'Label'] = 'Division'\n",
    "            # if word not in dictionary:\n",
    "            else:\n",
    "                print('false')\n",
    "        else:\n",
    "            # split the keyword to words and add the second word to the df as a row after the first word\n",
    "            # add the second word to the df as a row after the first word\n",
    "            # copy all row values to a new row\n",
    "            df.loc[index+1.5] = [row['Keyword'].split()[1], row['Word Before'], row['Word After'], row['Quote'], row['Label'], row['Lemma'], row['Early Category Type'], row['Early Word'], row['Early Word Before'], row['Early Word After'], row['Early Quote']]\n",
    "            df.loc[index+1.5, 'Label'] = 'Division'\n",
    "            df.loc[index+1.5, 'Lemma'] = phrase_dict[df.loc[index+1.5,'Keyword']]\n",
    "    # replace the first word with the lemma \"ὁ μέν\"\n",
    "            print (index, row['Keyword'])\n",
    "            print(df.loc[index+1.5])\n",
    "            #df = df.sort_index().reset_index(drop=True)\n",
    "            #print (df.loc[index+1.5, 'Keyword', index, 'keyword'])\n",
    "            #print(\"new word: \", row['Keyword'].split()[0])\n",
    "            #print(\"new Lemma: \", phrase_dict[row['Keyword']])\n",
    "            row['Keyword'] = row['Keyword'].split()[0]\n",
    "            df.at[index, 'Lemma'] = df.at[index, 'Keyword']\n",
    "            #replace the category type with \"Division\"\n",
    "            df.at[index, 'Label'] = 'Division'\n",
    "            # replace the first word with the lemma \"ὁ δέ\"\n",
    "            #replace the category type with \"Division\"\n",
    "            #print (row['Keyword'])\n",
    "            #print (df.loc[index+1, 'Keyword'])\n",
    "            #print (df.loc[index+1, 'Lemma'])\n",
    "            #print (\"more: \", row['Keyword'])\n",
    "df = df.sort_index().reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6ChLyYk1pmVs68IB/yCSC",
   "name": "Spacy-TOPO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
