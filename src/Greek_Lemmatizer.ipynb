{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:21:50.281722400Z",
     "start_time": "2023-05-31T11:21:42.414149800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy grc model if not already installed\n",
    "nlp = spacy.load(\"grc_proiel_trf\") # Use your preferred model here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# apostrophes and correct_apostrophe are defined as follows:\n",
    "apostrophes = [\"᾽\", \"᾿\", \"'\", \"’\", \"‘\"]\n",
    "correct_apostrophe = \"ʼ\"\n",
    "\n",
    "def clean_and_remove_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing diacritics (accents), except for specific characters,\n",
    "    and converting it to lowercase.\n",
    "    \"\"\"\n",
    "    allowed_characters = [' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\", 'ʼ', '̓']  # Including the Greek apostrophe\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    try:\n",
    "        non_accent_chars = [c for c in unicodedata.normalize('NFKD', text) \n",
    "        if unicodedata.category(c) != 'Mn' or c in allowed_characters]\n",
    "        return ''.join(non_accent_chars)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # A more generic exception handling if unexpected errors occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return text\n",
    "    \n",
    "\n",
    "def normalize_text(text: str, form: str = 'NFKD', \n",
    "                   remove_accents: bool = False, \n",
    "                   lowercase: bool = False, \n",
    "                   standardize_apostrophe: bool = True, \n",
    "                   remove_brackets: bool = False, \n",
    "                   remove_trailing_numbers: bool = False, \n",
    "                   remove_extra_spaces: bool = False, \n",
    "                   debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Applies multiple text normalization and cleaning steps on the input text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be normalized.\n",
    "    - form (str): Unicode normalization form ('NFC', 'NFD', 'NFKC', 'NFKD').\n",
    "    - lowercase (bool): If True, the text is converted to lowercase.\n",
    "    - standardize_apostrophe (bool): If True, replaces all defined apostrophe characters with a standard one.\n",
    "    - remove_brackets_only (bool): If True, removes the brackets themselves.\n",
    "    - remove_trailing_numbers (bool): If True, strips leading or trailing digits from the text.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The processed text.\n",
    "    \"\"\"\n",
    "    normalized_text = text  # Initialize normalized_text with the original text\n",
    "\n",
    "    # Function to print before and after states for each operation during debugging\n",
    "    def debug_print(operation_name, before, after):\n",
    "        if debug:\n",
    "            print(f\"{operation_name} - Before: {before}\")\n",
    "            print(f\"{operation_name} - After: {after}\")\n",
    "\n",
    "    # Standardize apostrophe characters if required\n",
    "    if standardize_apostrophe:\n",
    "        before_text = normalized_text\n",
    "        for apos in apostrophes:\n",
    "            normalized_text = normalized_text.replace(apos, correct_apostrophe)\n",
    "        debug_print(\"Standardizing apostrophes\", before_text, normalized_text)\n",
    "        \n",
    "    if remove_accents:\n",
    "        before_text = normalized_text\n",
    "        try:\n",
    "            normalized_text = clean_and_remove_accents(normalized_text)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while removing accents: {e}\")\n",
    "            # Decide what to do here: return the original text, a special value, or stop the process\n",
    "            return text        \n",
    "        debug_print(\"Removing accents\", before_text, normalized_text)\n",
    "        \n",
    "    # Convert to lowercase if required\n",
    "    if lowercase:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = normalized_text.lower()\n",
    "        debug_print(\"Lowercase conversion\", before_text, normalized_text)\n",
    "\n",
    "    # Unicode normalization\n",
    "    if form:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = unicodedata.normalize(form, normalized_text)\n",
    "        debug_print(\"Unicode normalization\", before_text, normalized_text)\n",
    "            \n",
    "    # Remove brackets only if required\n",
    "    if remove_brackets:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = re.sub(r'[\\(\\)\\[\\]]', '', normalized_text)\n",
    "        debug_print(\"Removing brackets\", before_text, normalized_text)\n",
    "        \n",
    "    # Remove trailing numbers if required\n",
    "    if remove_trailing_numbers:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = re.sub(r'^\\d+|\\d+$', '', normalized_text)\n",
    "        debug_print(\"Removing trailing numbers\", before_text, normalized_text)\n",
    "\n",
    "    # Remove multiple spaces and leading/trailing spaces\n",
    "    if remove_extra_spaces:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = ' '.join(normalized_text.split()).strip()\n",
    "        debug_print(\"Removing extra spaces\", before_text, normalized_text)\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "FILE_PATH = \"../assets/NER_assets/Ancient_Words_12_5_22.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Renaming columns\n",
    "df.rename(columns={'Word': 'Keyword', 'Category Types': 'Label'}, inplace=True)\n",
    "\n",
    "# Filling NaN values more efficiently and appropriately\n",
    "for early_col, new_col in [('Early Quote', 'Quote'), ('Early Word Before', 'Word Before'), \n",
    "                           ('Early Word After', 'Word After'), ('Early Category Type', 'Label')]:\n",
    "    df[new_col].fillna(df[early_col], inplace=True)\n",
    "\n",
    "# Dropping rows with no Keyword and non-Greek Keywords\n",
    "pat = '[ء-ي]+'\n",
    "df = df.dropna(subset=['Keyword']).copy()\n",
    "df = df[~df['Keyword'].str.contains(pat, na=False)]\n",
    "\n",
    "# Cleaning data with combined regex patterns\n",
    "\n",
    "# Define a dictionary of patterns and replacements for the entire dataframe\n",
    "df_replacements = {\n",
    "    '\\d+': '',  # Numbers\n",
    "    '-': '',  # Hyphens\n",
    "    ' +': ' ',  # Multiple spaces\n",
    "}\n",
    "\n",
    "# Apply the replacements to the text columns ('Early Quote', 'Quote', 'Early Word Before', 'Word Before', 'Early Word After', 'Word After', 'Keyword')\n",
    "for col in ['Early Quote', 'Quote', 'Early Word Before', 'Word Before', 'Early Word After', 'Word After', 'Keyword']:\n",
    "    for pattern, replacement in df_replacements.items():\n",
    "        df[col].replace(pattern, replacement, regex=True, inplace=True)\n",
    "\n",
    "# Define a dictionary of patterns and replacements for the 'Keyword' column\n",
    "keyword_replacements = {\n",
    "    '\\n': '',  # New line\n",
    "    ',': '',  # Comma\n",
    "    '\\.': '',  # Period\n",
    "    '\\·': '',  # Interpunkt\n",
    "    '\\s+$': ''  # End punctuation\n",
    "}\n",
    "\n",
    "# Apply the replacements to the 'Keyword' column\n",
    "for pattern, replacement in keyword_replacements.items():\n",
    "    df['Keyword'].replace(pattern, replacement, regex=True, inplace=True)\n",
    "# Resetting the dataframe index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalizing the text in the columns\n",
    "columns_to_normalize = ['Early Quote', 'Quote', 'Early Word Before', 'Word Before', 'Early Word After', 'Word After', 'Keyword']\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    if df[col].dtype == 'object':\n",
    "        print(df[col].apply(lambda x: print(type(x))))\n",
    "        df[col] = df[col].apply(lambda x: normalize_text(x, remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_brackets=False, debug=True) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:21:50.550994900Z",
     "start_time": "2023-05-31T11:21:50.494339800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:21:50.582256500Z",
     "start_time": "2023-05-31T11:21:50.536484300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if any of the fields \"KeyWord\", \"Quote\", \"Word Before\", \"Word After\" are \"0\", print the row and drop it\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i, 0] == \"0\" or df.iloc[i, 1] == \"0\" or df.iloc[i, 2] == \"0\" or df.iloc[i, 3] == \"0\":\n",
    "        print(df.iloc[i])\n",
    "        df.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:21:50.629157200Z",
     "start_time": "2023-05-31T11:21:50.566630700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, DocBin\n",
    "from unicodedata import normalize\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create dictionaries from dendrosearch and conllu files (supplied by Jacobo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the coda dictionary for word : lemma \n",
    "\n",
    "coda_lemma_dict = df.dropna(subset=['Keyword', 'Lemma']).set_index('Keyword')['Lemma'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dendrosearch dictionary for word : lemma \n",
    "\n",
    "# punctuation to be removed\n",
    "PUNCTUATION = set(['.', \")\", \"·\", \"(\", \"[\", \"]\", \":\", \";\", \",\", \"?\", \"!\", \"،\", \"_\"])\n",
    "\n",
    "dendrosearch_lemma_dict = {} \n",
    "with open('../assets/dendrosearch_lemma_dict.txt', 'r', encoding='utf-8') as f:\n",
    "    dendrosearch_lemma_dict = {line.split()[0]: line.split()[1] for line in f if len(line.split()) > 1 and line.split()[0] not in PUNCTUATION} \n",
    "    # clean the dictionary with remove_accents_to_lowercase\n",
    "    dendrosearch_lemma_dict = {normalize_text(k, remove_accents=False, lowercase=False, standardize_apostrophe=True): normalize_text(v, remove_accents=False, lowercase=True, standardize_apostrophe=True) for k, v in dendrosearch_lemma_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrosearch_lemma_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the greCy conllu dictionary for word : lemma\n",
    "\n",
    "import os\n",
    "import glob  # Recommended for easy file pattern matching\n",
    "\n",
    "PUNCTUATION = set(['.', \")\", \"·\", \"(\", \"[\", \"]\", \":\", \";\", \",\", \"?\", \"!\", \"،\", \"_\"])  # Using a set for performance\n",
    "input_path = \"../assets/Lemmatization_training_files/Processed\"\n",
    "conllu_lemma_dict = {}\n",
    "\n",
    "def process_conllu_file(file_path, nlp, debug=False):\n",
    "    file_name = file_path.stem\n",
    "    print(f\"Processing file {file_name}...\")  # Progress indicator\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if not line.strip() or line.startswith('#'):\n",
    "                continue  # Skip empty lines and comments\n",
    "                \n",
    "            parts = line.split()\n",
    "            if len(parts) > 2 and parts[1] not in PUNCTUATION:\n",
    "                # Ensure at least id, keyword, and lemma are present and keyword isn't punctuation\n",
    "                conllu_lemma_dict[parts[1]] = parts[2]\n",
    "                \n",
    "    # return the dictionary\n",
    "    return conllu_lemma_dict    \n",
    "\n",
    "# Utilize glob for cleaner file selection\n",
    "for file_path in Path(input_path).glob(\"*.conllu\"):\n",
    "    #print(\"file name: \",file_path.name)  # Progress indicator\n",
    "    process_conllu_file(file_path, nlp, debug=False)\n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create dictionary amd sentences data from INCEpTION files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T13:08:09.090487500Z",
     "start_time": "2023-05-31T13:08:09.044413900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassis import load_typesystem, load_cas_from_xmi\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths using pathlib\n",
    "inception_files_path = Path(\"../assets/NER_assets/INCEpTION_files/\")\n",
    "tempdir_path = Path(tempfile.mkdtemp())  # Create a temporary directory and get its path\n",
    "\n",
    "try:\n",
    "    # Extract all .zip files found in the inception files path\n",
    "    for zip_file_path in inception_files_path.glob(\"*.zip\"):\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(tempdir_path)\n",
    "\n",
    "    typesystem_file_path = tempdir_path / \"TypeSystem.xml\"\n",
    "\n",
    "    # Check for the existence of TypeSystem.xml before proceeding\n",
    "    if not typesystem_file_path.exists():\n",
    "        raise FileNotFoundError(\"TypeSystem.xml not found in the extracted files.\")\n",
    "\n",
    "    with open(typesystem_file_path, 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "\n",
    "    inception_dict = {}\n",
    "    inception_sentences = []  # List of tuples (sentence, source_file)\n",
    "\n",
    "    # Process each .xmi file found in the temporary directory\n",
    "    for xmi_file_path in tempdir_path.glob(\"*.xmi\"):\n",
    "        with open(xmi_file_path, 'rb') as f:\n",
    "            cas = load_cas_from_xmi(f, typesystem=typesystem, lenient=True)\n",
    "            # Update inception_dict dictionary with lemmas\n",
    "            inception_dict.update(\n",
    "                {token.get_covered_text(): token.value for token in cas.select('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Lemma')}\n",
    "            )\n",
    "            # Extend inception_sentences list with sentences\n",
    "            inception_sentences.extend(\n",
    "                [(sentence.get_covered_text(), xmi_file_path.name) for sentence in cas.select(\"de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence\")]\n",
    "            )\n",
    "    # Clean and normalize inception_sentences\n",
    "    inception_sentences = [\n",
    "        (normalize_text(' '.join(sentence[0].replace('\\r', ' ').replace('\\n', ' ').split()), remove_accents=False, lowercase=False, standardize_apostrophe=True), sentence[1])\n",
    "        for sentence in inception_sentences\n",
    "    ] \n",
    "    \n",
    "    # Cleanup function added to ensure the temporary directory is always cleaned up\n",
    "    def cleanup_tempdir(directory):\n",
    "        for child in directory.iterdir():\n",
    "            if child.is_file():\n",
    "                child.unlink()\n",
    "        directory.rmdir()\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    cleanup_tempdir(tempdir_path)\n",
    "    # Exit or raise the error for further handling depending on the script's usage\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    cleanup_tempdir(tempdir_path)\n",
    "    # Exit or raise the error for further handling depending on the script's usage\n",
    "else:\n",
    "    # This block runs if no exceptions were raised\n",
    "    # Print or process the inception_dict and inception_sentences as needed\n",
    "    # Example: print some of the processed sentences\n",
    "        print(f\"\\nFinito!\")\n",
    "finally:\n",
    "    # print how many sentences were processed\n",
    "    print(f\"Processed {len(inception_sentences)} sentences.\")\n",
    "    # Ensure the temporary directory is always cleaned up\n",
    "    cleanup_tempdir(tempdir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create large dictionary from all sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "big_dict = {\n",
    "    'Conllu': conllu_lemma_dict,\n",
    "    'Inception': inception_dict,\n",
    "    'Coda': coda_lemma_dict,\n",
    "    'Dendrosearch': dendrosearch_lemma_dict\n",
    "}\n",
    "\n",
    "\n",
    "# Placeholder structure for the processed pairs\n",
    "processed_pairs_with_sources = {\n",
    "    'NFKD': {},\n",
    "    'NFKC': {}\n",
    "}\n",
    "\n",
    "for form in ['NFKD', 'NFKC']:\n",
    "    for source, lemma_dict in big_dict.items():\n",
    "        for word, lemma in lemma_dict.items():\n",
    "            norm_word = normalize_text(word, form, remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_brackets=True, remove_trailing_numbers=True, debug=False)\n",
    "            norm_lemma = normalize_text(lemma, form, remove_accents=False, lowercase=True, standardize_apostrophe=True, remove_brackets=True, remove_trailing_numbers=True, debug=False)\n",
    "\n",
    "            # Skip empty normalized word entries\n",
    "            if not norm_word or norm_lemma in [\"_\", \" \", \"\"]:  \n",
    "                continue\n",
    "\n",
    "            if norm_word not in processed_pairs_with_sources[form]:\n",
    "                processed_pairs_with_sources[form][norm_word] = {}\n",
    "\n",
    "            # Aggregate lemmas with their sources\n",
    "            if norm_lemma in processed_pairs_with_sources[form][norm_word]:\n",
    "                processed_pairs_with_sources[form][norm_word][norm_lemma].add(source)\n",
    "            else:\n",
    "                processed_pairs_with_sources[form][norm_word][norm_lemma] = {source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the pairs and choose the most common lemma for each word\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "processed_counter = 0  # Counter for processed words\n",
    "no_lemma_counter = 0  # Counter for words with no lemma\n",
    "deleted_counter = 0  # Counter for deleted words\n",
    "for form, word_lemmas in processed_pairs_with_sources.items():\n",
    "    keys_to_delete = []  # List to store keys of pairs to be deleted\n",
    "    for word, lemmas in word_lemmas.items():\n",
    "        most_common_lemma = None\n",
    "        most_common_count = 0\n",
    "        source_info_for_most_common = set()\n",
    "        tied_lemmas = []  # For handling ties\n",
    "        \n",
    "        for lemma, sources in lemmas.items():\n",
    "            current_count = len(sources)\n",
    "            if current_count > most_common_count:\n",
    "                most_common_lemma = lemma\n",
    "                most_common_count = current_count\n",
    "                source_info_for_most_common = sources\n",
    "                tied_lemmas = [lemma]  # Reset ties because a new max is found\n",
    "            elif current_count == most_common_count:\n",
    "                tied_lemmas.append(lemma)  # Add lemma to ties\n",
    "        \n",
    "        # Handle ties: choose the lexically first lemma if there's a tie\n",
    "        if len(tied_lemmas) > 1:\n",
    "            tied_lemmas.sort()  # Sort the list to ensure consistent processing\n",
    "            most_common_lemma = tied_lemmas[0]  # Lexically first lemma\n",
    "            # Combine sources of tied lemmas since more than one had the \"most common\" status\n",
    "            source_info_for_most_common = set().union(*(lemmas[lemma] for lemma in tied_lemmas))\n",
    "            \n",
    "        if most_common_lemma:\n",
    "            processed_pairs_with_sources[form][word] = {most_common_lemma: source_info_for_most_common}\n",
    "            processed_counter += 1\n",
    "        else:\n",
    "            keys_to_delete.append(word)\n",
    "            no_lemma_counter += 1\n",
    "\n",
    "    # Delete marked words outside the loop\n",
    "    for word in keys_to_delete:\n",
    "        del processed_pairs_with_sources[form][word]\n",
    "        deleted_counter += 1\n",
    "\n",
    "print(f\"Total processed words: {processed_counter}\")\n",
    "print(f\"Total words with no lemma: {no_lemma_counter}\")\n",
    "print(f\"Total deleted words: {deleted_counter}\")\n",
    "\n",
    "# Function to save the processed_pairs_with_sources to a json file\n",
    "#def save_processed_pairs(processed_pairs_with_sources, filename):\n",
    "    # Convert sets to lists for JSON serialization\n",
    "#    for form, word_lemmas in processed_pairs_with_sources.items():\n",
    "#        for word, lemmas in word_lemmas.items():\n",
    "#            for lemma, sources in lemmas.items():\n",
    "#                lemmas[lemma] = list(sources)\n",
    "\n",
    "    # Save to file\n",
    "    #with open(filename, 'w', encoding='utf-8') as f:\n",
    "    #    json.dump(processed_pairs_with_sources, f, ensure_ascii=False)\n",
    "\n",
    "# Example usage of save function\n",
    "#save_processed_pairs(processed_pairs_with_sources, 'processed_pairs_with_sources.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run NLP pipeline on INCEpTION and Coda files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Assuming the model is correctly installed:\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")  # Use your preferred model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'inception_sentences' is a list of tuples like (sentence_text, source_file),\n",
    "# and 'df' is your DataFrame containing Coda sentences.\n",
    "sentences = OrderedDict()\n",
    "\n",
    "\n",
    "# Add inception sentences with source file\n",
    "for sentence, source_file in inception_sentences:\n",
    "    if sentence not in sentences:\n",
    "        sentences[sentence] = ('Inception', source_file)\n",
    "    \n",
    "# source for Coda is made from the values in columns:Modern Edition, Book, Chapter, Section, Page, Line Number\n",
    "\n",
    "def format_value(col, value):\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    if col == 'Modern Edition':\n",
    "        return str(value)\n",
    "    if isinstance(value, (int, float)):\n",
    "        value = int(value)\n",
    "    return f\"{col[0]}({value})\"\n",
    "\n",
    "for row in df.index:\n",
    "    formatted_values = []\n",
    "    for col in ['Modern Edition', 'Book', 'Chapter', 'Section', 'Page', 'Line Number']:\n",
    "        value = df.loc[row, col]\n",
    "        formatted_value = format_value(col, value)\n",
    "        if formatted_value is not None:\n",
    "            formatted_values.append(formatted_value)\n",
    "\n",
    "# Add Coda sentences from DataFrame\n",
    "    sentence = df.loc[row, 'Quote']\n",
    "    \n",
    "    source = ', '.join(formatted_values)\n",
    "    cleaned_sentence = normalize_text(sentence, standardize_apostrophe=True, remove_extra_spaces=True)\n",
    "    if cleaned_sentence not in sentences:\n",
    "        sentences[cleaned_sentence] = (f\"Coda, {source}\")\n",
    "\n",
    "# Convert OrderedDict back to a list of tuples if needed for further processing (sentence, source_type, source_file)\n",
    "sentences_list = list(sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(sentences_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_pairs_with_sources['NFKD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "\n",
    "# Assuming 'processed_pairs_with_sources', 'nlp' (a SpaCy Language model), and 'sentences_list' are already defined\n",
    "\n",
    "def process_sentences(sentences_list, nlp, processed_pairs, debug=False):\n",
    "    docs_nfkd: List[Doc] = []\n",
    "    docs_nfkc: List[Doc] = []\n",
    "\n",
    "    corrections_nfkd = []\n",
    "    corrections_nfkc = []\n",
    "\n",
    "    sentences_and_metadata = [\n",
    "        (normalize_text(sentence, form, remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_brackets=False, remove_trailing_numbers=False, debug=debug), (source_info, form))\n",
    "        for sentence, source_info in sentences_list\n",
    "        for form in ['NFKD', 'NFKC']\n",
    "    ]\n",
    "\n",
    "    sentences_for_processing = [sentence for sentence, _ in sentences_and_metadata]\n",
    "    metadata = [meta for _, meta in sentences_and_metadata]\n",
    "\n",
    "    corrected_count = 0\n",
    "    not_corrected_count = 0\n",
    "    \n",
    "    for doc, meta in tqdm(zip(nlp.pipe(sentences_for_processing, batch_size=1000), metadata), total=len(sentences_and_metadata)):\n",
    "        form = meta[1]  # 'NFKD' or 'NFKC'   \n",
    "        print(f\"Processing sentence '{doc.text}'\") if debug else None\n",
    "        print(f\"Form: {form}\") if debug else None     \n",
    "        # Determine the correct list to append corrections based on the form\n",
    "        corrections_list = corrections_nfkd if form == 'NFKD' else corrections_nfkc\n",
    "        doc.user_data[\"source_info\"] = meta[0]\n",
    "        print(\"userdata: \", meta[0]) if debug else None\n",
    "        print(f\"Added source info '{meta[0]}' to the Doc object: \", doc.user_data[\"source_info\"]) if debug else None\n",
    "\n",
    "        for token in doc:\n",
    "            lemma_sources = None\n",
    "            print(f\"Processing token '{token.text}' in sentence '{doc.text}'\") if debug else None\n",
    "            if token.text in processed_pairs[form]:\n",
    "                lemma_sources = processed_pairs[form][token.text]\n",
    "                print(f\"Found token '{token.text}'\") if debug else None\n",
    "            elif token.text.lower() in processed_pairs[form]:\n",
    "                lemma_sources = processed_pairs[form][token.text.lower()]\n",
    "                print(f\"Found token '{token.text}' in lowercase\") if debug else None\n",
    "            if lemma_sources is not None:\n",
    "                for lemma, sources in lemma_sources.items():\n",
    "                    if lemma != token.lemma_:\n",
    "                        corrections_list.append({\n",
    "                            'sentence': doc.text,\n",
    "                            'source_info': meta[0],\n",
    "                            'token': token.text,\n",
    "                            'lemma': token.lemma_,\n",
    "                            'lemma_corrected': lemma,\n",
    "                            'correction_source': ', '.join(sources)\n",
    "                        })\n",
    "                        token.lemma_ = lemma  # Correct the lemma in the Doc object\n",
    "                        corrected_count += 1\n",
    "                        print(f\"Corrected token '{token.text}' to '{lemma}' in sentence '{doc.text}'\") if debug else None\n",
    "                        break\n",
    "                else:\n",
    "                    not_corrected_count += 1\n",
    "                    print(f\"Same Lemma, did not correct token '{token.text}' in sentence '{doc.text}'\") if debug else None\n",
    "            else:\n",
    "                not_corrected_count += 1\n",
    "                print(f\"Couldn't find lemma in sources, did not correct token '{token.text}', {token.lemma_} in sentence '{doc.text}'\") if debug else None\n",
    "                # change the lemma to an empty string\n",
    "                token.lemma_ = \"\"\n",
    "                print(f\"Token Lemma set to skip: {token.lemma_}\") if debug else None\n",
    "            # add source info to the token attributes\n",
    "            \n",
    "        # append the processed sentences to the appropriate list\n",
    "        if form == 'NFKD':\n",
    "            docs_nfkd.append(doc)\n",
    "        if form == 'NFKC':\n",
    "            docs_nfkc.append(doc)\n",
    "    print(f\"Corrected {corrected_count} tokens\") if debug else None\n",
    "    print(f\"Did not correct {not_corrected_count} tokens\") if debug else None\n",
    "\n",
    "    return corrections_nfkd, corrections_nfkc, docs_nfkd, docs_nfkc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corrections():\n",
    "    # Get corrections for both normalization forms\n",
    "    corrections_nfkd, corrections_nfkc, docs_nfkd, docs_nfkc = process_sentences(sentences_list, nlp, processed_pairs_with_sources, debug=True)\n",
    "\n",
    "    # Convert the corrections lists to DataFrames\n",
    "    corrections_df_nfkd = pd.DataFrame(corrections_nfkd)\n",
    "    corrections_df_nfkc = pd.DataFrame(corrections_nfkc)\n",
    "\n",
    "    print(f\"Total corrections (NFKD): {len(corrections_df_nfkd)}\")\n",
    "    print(f\"Total corrections (NFKC): {len(corrections_df_nfkc)}\")\n",
    "    \n",
    "    # find how many corrected by each dictionary\n",
    "    print(corrections_df_nfkd.groupby('correction_source').count())\n",
    "    # total number of corrections\n",
    "    print(corrections_df_nfkd.groupby('correction_source').count().sum())\n",
    "    \n",
    "        # find how many corrected by each dictionary\n",
    "    print(corrections_df_nfkc.groupby('correction_source').count())\n",
    "    # total number of corrections\n",
    "    print(corrections_df_nfkc.groupby('correction_source').count().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_corrections()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacy dataset should be exported to '../corpus/' folder.\\\n",
    "More specifically:\\\n",
    "train to '..corpus/train/lemma_train/'\\\n",
    "dev to '../corpus/dev/lemma_dev/'\\\n",
    "test to '../corpus/test/lemma_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Function to split docs into train, dev, test sets and save them\n",
    "def split_and_save_docs(sentences_list, nlp, processed_pairs_with_sources, base_path: str = \"../corpus\"):\n",
    "    # Call process_sentences to get the processed docs and corrections\n",
    "    corrections_nfkd, corrections_nfkc, docs_nfkd, docs_nfkc = process_sentences(sentences_list, nlp, processed_pairs_with_sources, debug=False)\n",
    "\n",
    "    \"\"\"\n",
    "    Splits documents into train, test, and dev sets for both 'NFKD' and 'NFKC' normalization forms.\n",
    "    Saves each set to disk in SpaCy's DocBin format.\n",
    "    \n",
    "    Args:\n",
    "    - docs_nfkd: List of SpaCy Doc objects for 'NFKD' normalized text.\n",
    "    - docs_nfkc: List of SpaCy Doc objects for 'NFKC' normalized text.\n",
    "    - base_path: Base path to save the split documents.\n",
    "    \"\"\"\n",
    "    assert docs_nfkd and docs_nfkc, \"Document lists must not be empty\"\n",
    "    \n",
    "    # Split documents for 'NFKD' normalization\n",
    "    train_docs_nfkd, temp_docs_nfkd = train_test_split(docs_nfkd, test_size=0.2, random_state=42) # 80% for training\n",
    "    test_docs_nfkd, dev_docs_nfkd = train_test_split(temp_docs_nfkd, test_size=0.5, random_state=42)  # Split the remaining 20% equally for testing and development\n",
    "\n",
    "    # Split documents for 'NFKC' normalization\n",
    "    train_docs_nfkc, temp_docs_nfkc = train_test_split(docs_nfkc, test_size=0.2, random_state=42) # 80% for training\n",
    "    test_docs_nfkc, dev_docs_nfkc = train_test_split(temp_docs_nfkc, test_size=0.5, random_state=42)  # Split the remaining 20% equally for testing and development\n",
    "\n",
    "    # Directories for saving split docs\n",
    "    directories = ['train/lemma_train', 'dev/lemma_dev', 'test/lemma_test']\n",
    "    forms = ['NFKD', 'NFKC']\n",
    "    \n",
    "    # Ensure directories exist\n",
    "    for directory in directories:\n",
    "        for form in forms:\n",
    "            Path(f\"{base_path}/{directory}/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Function to save docs to disk\n",
    "    def save_docs(docs, path):\n",
    "        doc_bin = DocBin(docs=docs, store_user_data=True)\n",
    "        doc_bin.to_disk(path)\n",
    "    \n",
    "    # Saving documents\n",
    "    save_paths = {'train': (train_docs_nfkd, 'train'), 'dev': (dev_docs_nfkd, 'dev'), 'test': (test_docs_nfkd, 'test')}\n",
    "    for split, (docs, subset) in save_paths.items():\n",
    "        save_docs(docs, f\"{base_path}/{split}/lemma_{split}/{subset}_lemma_NFKD.spacy\")\n",
    "\n",
    "    save_paths = {'train': (train_docs_nfkc, 'train'), 'dev': (dev_docs_nfkc, 'dev'), 'test': (test_docs_nfkc, 'test')}\n",
    "    for split, (docs, subset) in save_paths.items():\n",
    "        save_docs(docs, f\"{base_path}/{split}/lemma_{split}/{subset}_lemma_NFKC.spacy\")\n",
    "    \n",
    "    print(f\"Documents are successfully split and saved for 'NFKD' and 'NFKC' forms.\")\n",
    "\n",
    "split_and_save_docs(sentences_list, nlp, processed_pairs_with_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for complete dataset (non-split), uncomment the following lines\n",
    "\n",
    "#corrections_nfkd, corrections_nfkc, docs_nfkd, docs_nfkc = process_sentences(sentences_list, nlp, processed_pairs_with_sources)\n",
    "\n",
    "# save each one to DocBin\n",
    "#def save_docs(docs, path):\n",
    "#        doc_bin = DocBin(docs=docs)\n",
    "#        doc_bin.to_disk(path)\n",
    "\n",
    "#save_docs(docs_nfkd, \"../corpus/train/lemma_train/NFKD/train_lemma_NFKD_full.spacy\")\n",
    "#save_docs(docs_nfkc, \"../corpus/train/lemma_train/NFKD/train_lemma_NFKC_full.spacy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process conllu greCy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# install spacy grc model if not already installed\n",
    "nlp = spacy.load(\"grc_proiel_trf\") # Use your preferred model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_token_attributes(doc, debug=False):\n",
    "    \"\"\"\n",
    "    Modify token attributes in a doc according to specific rules.\n",
    "    \"\"\"\n",
    "\n",
    "    lemma_count = 0\n",
    "    pos_count = 0\n",
    "    dep_count = 0\n",
    "    head_dep_count = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Lemmatizer rules: Set lemma to \"\" for trainable lemmatizer\n",
    "        if token.lemma_ in ['', \"_\", '—', '-']:\n",
    "            token.lemma_ = ''\n",
    "            lemma_count += 1\n",
    "            print(f\"Adjusted lemma for token {token.text}, lemma: {token.lemma_}\") if debug else None\n",
    "            \n",
    "        # Tagger rules: Set POS tags to \"\" (empty string)\n",
    "        if token.pos_ in ['', \"_\", '—', '-', 'X', 'END', 'MID']:\n",
    "            token.pos_ = \"\"  # This won't work as spaCy does not allow direct setting of .pos_ after Doc creation\n",
    "            pos_count += 1\n",
    "            print(f\"Adjusted lemma for token {token.text}, POS: {token.pos_}\") if debug else None\n",
    "\n",
    "\n",
    "        ## Dependency parser rules: Set dep_ to \"None\" for empty strings\n",
    "        if token.dep_ in ['', \"_\", '—', '-']:\n",
    "            token.dep_ = \"None\"\n",
    "            dep_count += 1\n",
    "            print(f\"Adjusted dependency for token {token.text}, dep: {token.dep_}\") if debug else None\n",
    "            \n",
    "        if token.head.dep_ in ['', \"_\", '—', '-']:\n",
    "            token.head.dep_ = \"None\"\n",
    "            head_dep_count += 1\n",
    "            print(f\"Adjusted head dependency for token {token.head.text}, dep: {token.head.dep_}\") if debug else None\n",
    "\n",
    "    print(f\"Total lemmas adjusted: {lemma_count}\") if debug else None\n",
    "    print(f\"Total POS tags adjusted: {pos_count}\") if debug else None\n",
    "    print(f\"Total dependencies adjusted: {dep_count}\") if debug else None\n",
    "    print(f\"Total head dependencies adjusted: {head_dep_count}\") if debug else None\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def process_and_save_docs(input_dir, output_dir, nlp, debug=False):\n",
    "    \"\"\"\n",
    "    Load spaCy Docs from .spacy files, modify token attributes, and save to new .spacy files.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    doc_count = 0  # Initialize counter\n",
    "\n",
    "    for doc_file in input_path.glob(\"*.spacy\"):\n",
    "        doc_bin = DocBin().from_disk(doc_file)\n",
    "        output_bin = DocBin()\n",
    "\n",
    "        for doc in doc_bin.get_docs(nlp.vocab):\n",
    "            modified_doc = modify_token_attributes(doc, debug=debug)\n",
    "            output_bin.add(modified_doc)\n",
    "\n",
    "        output_filename = output_path / doc_file.name\n",
    "        output_bin.to_disk(output_filename)\n",
    "        doc_count += 1  # Increment counter\n",
    "        print(f\"Processed and saved {output_filename}\")\n",
    "\n",
    "    print(f\"Total number of documents processed: {doc_count}\")\n",
    "\n",
    "# Example usage:\n",
    "#nlp = spacy.blank(\"en\")  # Use the appropriate language model\n",
    "#process_and_save_docs(\"path/to/input/folder\", \"path/to/output/folder\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_docs(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/\", \"../corpus/train/lemma_train\", nlp, debug=True)\n",
    "process_and_save_docs(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/\", \"../corpus/train/lemma_train\", nlp, debug=True)\n",
    "\n",
    "process_and_save_docs(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/\", \"../corpus/train/lemma_train\", nlp, debug=True)\n",
    "process_and_save_docs(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/\", \"../corpus/train/lemma_train\", nlp, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_docs(\"../assets/Lemmatization_training_files/Processed/lemma_train/spaCy\", \"../corpus/train/lemma_train\", nlp, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a docbin\n",
    "test_doc_bin = DocBin().from_disk(\"../corpus/train/lemma_train/grc_proiel-ud-train_NFKD.spacy\")\n",
    "# get the docs\n",
    "test_docs = list(test_doc_bin.get_docs(nlp.vocab))\n",
    "# check the first doc\n",
    "print(test_docs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  list all unique dep attributes in docbin\n",
    "dep_attributes = set()\n",
    "for doc in test_docs:\n",
    "    for token in doc:\n",
    "        dep_attributes.add(token.pos_)\n",
    "\n",
    "        \n",
    "dep_attributes\n",
    "\n",
    "\n",
    "\n",
    "# for each word in the found doc, make a datafarame with attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ner: use None as the IOB tag, as explained in Training NER on Incomplete Annotations #11114\n",
    "tagger: use \"\" as the tag, e.g. [\"\", \"V\", \"S\", \"J\", \"\"]\n",
    "parser: use None both for heads and deps e.g. [1, 1, 1, None] and [\"nsubj\", \"ROOT\", \"dobj\", None]\n",
    "trainable_lemmatizer: set the lemma to \"\", e.g. [\"\", \"like\", \"green\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a docbin\n",
    "test_doc_bin = DocBin().from_disk(\"../corpus/train/lemma_train/train_lemma_NFKD.spacy\")\n",
    "# get the docs\n",
    "test_docs = list(test_doc_bin.get_docs(nlp.vocab))\n",
    "# check the first doc\n",
    "for doc in test_docs:\n",
    "    print(doc.text)\n",
    "    print(doc.user_data[\"source_info\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "# install spacy grc model if not already installed\n",
    "nlp = spacy.load(\"grc_proiel_trf\") # Use your preferred model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_spacy_objects(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".spacy\"):  # Check if the file is a spacy object\n",
    "            tested_doc = DocBin().from_disk(os.path.join(folder_path, filename))  # Load the spacy object\n",
    "            docs = list(tested_doc.get_docs(nlp.vocab))  # Convert generator to list to reuse it\n",
    "            num_docs = len(docs)  # Calculate the number of documents\n",
    "            total_length = sum(len(doc.text) for doc in docs)  # Calculate the total length\n",
    "            avg_length = total_length / num_docs if num_docs > 0 else 0  # Calculate the average length\n",
    "            print(f\"Spacy object: {filename}\")\n",
    "            print(f\"Number of documents: {num_docs}\")\n",
    "            print(f\"Total length: {total_length}\")\n",
    "            print(f\"Average length: {avg_length}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the path to the folder containing the spacy objects\n",
    "analyze_spacy_objects(\"../corpus/dev/lemma_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "def analyze_spacy_objects(folder_path):\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"grc_proiel_trf\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".spacy\"):\n",
    "            # Load the Spacy object\n",
    "            doc_bin = nlp.from_bytes(open(file_path, \"rb\").read())\n",
    "\n",
    "            # Calculate the number of documents\n",
    "            num_docs = len(list(doc_bin.sents))\n",
    "\n",
    "            # Calculate the total length (in characters)\n",
    "            total_length = sum(len(str(sent)) for sent in doc_bin.sents)\n",
    "\n",
    "            # Calculate the average length of a document\n",
    "            avg_length = total_length / num_docs\n",
    "\n",
    "            print(f\"File: {filename}\")\n",
    "            print(f\"Number of documents: {num_docs}\")\n",
    "            print(f\"Total length (in characters): {total_length}\")\n",
    "            print(f\"Average length of a document: {avg_length:.2f}\")\n",
    "            print()\n",
    "\n",
    "# Example usage\n",
    "analyze_spacy_objects(\"../corpus/dev/lemma_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlomy_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
