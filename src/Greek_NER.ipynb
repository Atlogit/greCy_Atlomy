{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:09:18.200289400Z",
     "start_time": "2023-06-13T07:09:05.302556800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from unicodedata import normalize\n",
    "import regex\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and process data from Coda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:10:17.144471Z",
     "start_time": "2023-06-13T07:10:16.982123100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_PATH = \"../assets/NER_assets/Ancient_Words.csv\"\n",
    "# read csv file\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# rename columns to fit code\n",
    "df.rename(columns = {'Word':'Keyword', 'Category Types':'Label'}, inplace = True)\n",
    "# If a cell is empty (NaN), Fill it with the value in its parallel \"Early\" column\n",
    "for row in df:\n",
    "    df['Quote'].fillna(df['Early Quote'], inplace=True)\n",
    "    df['Word Before'].fillna(df['Early Word Before'], inplace=True)\n",
    "    df['Word After'].fillna(df['Early Word After'], inplace=True)\n",
    "    df['Label'].fillna(df['Early Category Type'], inplace=True)\n",
    "# remove rows with no Keyword\n",
    "df = df.dropna(subset=['Keyword'])\n",
    "# Remove any row that isn't Greek\n",
    "pat = '[ء-ي]+'\n",
    "#df.Keyword.str.contains(pat)\n",
    "df = df[~df.Keyword.str.contains(pat, na=False)]\n",
    "#replace new line in df column\n",
    "df['Keyword'].replace('\\n', '', regex=True, inplace=True)\n",
    "#replace numbers in df\n",
    "df.replace('\\d+', '', regex=True, inplace=True)\n",
    "#replace hyphens in df column\n",
    "df.replace('-', '', regex=True, inplace=True)\n",
    "# replace comma in df column\n",
    "df['Keyword'].replace(',', '', regex=True, inplace=True)\n",
    "#replace period in df column\n",
    "df['Keyword'].replace('\\.', '', regex=True, inplace=True)\n",
    "#replace interpunkt in df column\n",
    "df['Keyword'].replace('\\·', '', regex=True, inplace=True)\n",
    "# replace multiple spaces in df column\n",
    "df.replace(' +', ' ', regex=True, inplace=True)\n",
    "# replace end punctuation in df column\n",
    "df['Keyword'].replace('\\s+$', '', regex=True, inplace=True)\n",
    "\n",
    "df.fillna(0)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:10:17.583084Z",
     "start_time": "2023-06-13T07:10:17.504959800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize table\n",
    "FORMAT = 'NFKC'\n",
    "for col in ['Keyword', 'Quote', 'Word Before', 'Word After']:\n",
    "    df[col] = df[col].apply(lambda x: normalize(FORMAT, str(x)) if not isinstance(x, float) else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fix similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.DataFrame(columns=['Quote', 'entities'])\n",
    "\n",
    "# group all similar sentences together, and for each one append to entities: [label, word, word before, word after]\n",
    "entities_list = []\n",
    "for name, group in df.groupby('Quote'):\n",
    "    entities = [\n",
    "        [row['Label'], row[\"Keyword\"], row['Word Before'], row['Word After']]\n",
    "        for i, row in group.iterrows()\n",
    "    ]\n",
    "    # append to entities_list\n",
    "    entities_list.append({'Quote': name, 'entities': entities})\n",
    "\n",
    "# create new DataFrame from entities_list\n",
    "df_grouped = pd.DataFrame(entities_list)\n",
    "\n",
    "# sample random from df_grouped\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:42.125237700Z",
     "start_time": "2023-06-13T07:11:38.282223200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after naively grouping similar sentences, we need to check if they are actually similar\n",
    "# we do this by using regex with difference up to 10 chars\n",
    "checked = []\n",
    "similar_indices = []\n",
    "\n",
    "for i, row in df_grouped.iterrows():\n",
    "    # if we haven't checked this sentence yet\n",
    "    if i not in checked:\n",
    "        checked.append(i)\n",
    "        similar_indices.append([i])\n",
    "        # loop over all other sentences\n",
    "        pattern = fr\"{re.escape(row['Quote'])}\"\n",
    "        # add fuzzy matching with up to 10 mistakes\n",
    "        pattern = fr\"(?:{pattern}){{e<=10}}\"\n",
    "\n",
    "        for j, row2 in df_grouped.iterrows():\n",
    "                # if the sentences are similar\n",
    "            if regex.search(row['Quote'], row2['Quote']):\n",
    "                if j not in checked:\n",
    "                    # append to similar_indices\n",
    "                    similar_indices[-1].append(j)\n",
    "                    # append to checked\n",
    "                    checked.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:42.204175800Z",
     "start_time": "2023-06-13T07:11:42.141587200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (similar_indices)\n",
    "# choose a random group from similar_indices where the length of the group is more than 1\n",
    "group = random.choice([group for group in similar_indices if len(group) > 1])\n",
    "print (group)\n",
    "# print the sentences in the group\n",
    "for index in group:\n",
    "    print (df_grouped.loc[index, 'Quote'])\n",
    "    # print entities of each sentence in the group\n",
    "    for entity in df_grouped.loc[index, 'entities']:\n",
    "        print (entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:44.552974200Z",
     "start_time": "2023-06-13T07:11:44.155882500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the longest sentence in each group of similar sentences\n",
    "df_grouped_regexed = pd.DataFrame(columns=['Quote', 'entities'])\n",
    "for indices in similar_indices:\n",
    "    # if there is only one sentence in the group add it to df_grouped_regexed\n",
    "    if len(indices) == 1:\n",
    "        df_grouped_regexed = pd.concat([df_grouped_regexed, df_grouped.loc[indices[0]].to_frame().T], ignore_index=True)\n",
    "        continue\n",
    "    # find the longest sentence in the group\n",
    "    longest = max(indices, key=lambda x: len(df_grouped.loc[x, 'Quote']))\n",
    "    entities = []\n",
    "    # concatenate all entities from the other sentences in the group\n",
    "    for index in indices:\n",
    "        entities.extend(df_grouped.loc[index, 'entities'])\n",
    "    # create a new row with the longest sentence and the concatenated entities\n",
    "    new_row = {'Quote': df_grouped.loc[longest, 'Quote'], 'entities': entities}\n",
    "    # add the new row to df_grouped_regexed\n",
    "    df_grouped_regexed = pd.concat([df_grouped_regexed, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:45.152808200Z",
     "start_time": "2023-06-13T07:11:45.121920400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from df_grouped_regexed\n",
    "df_grouped_regexed.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:45.987165200Z",
     "start_time": "2023-06-13T07:11:45.976802700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:46.488164500Z",
     "start_time": "2023-06-13T07:11:46.441070900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped=df_grouped_regexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:46.911786Z",
     "start_time": "2023-06-13T07:11:46.864904600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from df_grouped\n",
    "df_grouped.sample(10)\n",
    "len(df_grouped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fix similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:48.153679200Z",
     "start_time": "2023-06-13T07:11:48.122410Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {ord('\\N{COMBINING ACUTE ACCENT}'):None, ord('\\N{COMBINING COMMA ABOVE}'):None, ord('\\N{COMBINING REVERSED COMMA ABOVE}'):None}\n",
    "\n",
    "def find_word_index(sentence, word, word_before, word_after, clean_chars=True):\n",
    "    \"\"\"Find the index of a word in a sentence, it can appear multiple times but we return by word_before and word_after\n",
    "    :returns start and end index of word in sentence\"\"\"\n",
    "\n",
    "\n",
    "    # find the index of the word in the sentence (return index of beginning of word)\n",
    "    word_index = sentence.find(word)\n",
    "\n",
    "    # if \"clean_chars\" is True, remove all accents from the sentence\n",
    "    if clean_chars: \n",
    "        # return all chars of word before: [),.,·] (meaning none of these chars will be included) so we get only data contained within the sentence\n",
    "        for char in [\")\", \".\", \"·\", ',']:\n",
    "            if word_before.find(char) != -1:\n",
    "                word_before = word_before[word_before.find(char)+1:]\n",
    "                print (\"word before: \", word_before)\n",
    "            if word_after.find(char) != -1:\n",
    "                word_after = word_after[:word_after.find(char)]\n",
    "                print (\"word after: \", word_after)\n",
    "    else:\n",
    "        #do nothing\n",
    "        pass\n",
    "\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)}){re.escape(word_after)}\"\n",
    "    # add fuzzy matching with up to 3 mistakes\n",
    "    pattern = fr\"(?:{pattern}){{e<=3}}\"\n",
    "    if match := regex.search(pattern, sentence):\n",
    "        print (\"match 3: \", match.span(1))\n",
    "        return match.span(1)\n",
    "\n",
    "    # try matching only with word_before\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)})\"\n",
    "    # add fuzzy matching with up to 3 mistakes\n",
    "    pattern = fr\"(?:{pattern}){{e<=2}}\"\n",
    "    if match := regex.search(pattern, sentence):\n",
    "        print (\"match with before: \", match.span(1))\n",
    "        return match.span(1)\n",
    "\n",
    "    # try matching only with word_after\n",
    "    pattern = fr\"({(word)}){re.escape(word_after)}\"\n",
    "    # add fuzzy matching with up to 3 mistakes\n",
    "    pattern = fr\"(?:{pattern}){{e<=2}}\"\n",
    "    if match := regex.search(pattern, sentence):\n",
    "        print (\"match with after: \", match.span(1))\n",
    "        return match.span(1)\n",
    "\n",
    "    # if no match found, return None\n",
    "    print (\"\\nNo match found for\", word, \"in\", sentence, \"\\nwith word_before\", word_before, \"and word_after\", word_after)\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index_test(sentence, word, word_before, word_after):\n",
    "    \"\"\"Find the index of a word in a sentence, it can appear multiple times but we return by word_before and word_after\n",
    "    :returns start and end index of word in sentence\"\"\"\n",
    "\n",
    "    # Remove unwanted characters from word_before and word_after\n",
    "    word_before = remove_unwanted_chars(word_before)\n",
    "    word_after = remove_unwanted_chars(word_after)\n",
    "\n",
    "    # Try matching with word_before and word_after\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)}){re.escape(word_after)}\"\n",
    "    match = fuzzy_search(pattern, sentence, max_mistakes=3)\n",
    "    if match:\n",
    "        return match.span(1)\n",
    "\n",
    "    # Try matching with word_before only\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)})\"\n",
    "    match = fuzzy_search(pattern, sentence, max_mistakes=2)\n",
    "    if match:\n",
    "        return match.span(1)\n",
    "\n",
    "    # Try matching with word_after only\n",
    "    pattern = fr\"({(word)}){re.escape(word_after)}\"\n",
    "    match = fuzzy_search(pattern, sentence, max_mistakes=2)\n",
    "    if match:\n",
    "        return match.span(1)\n",
    "\n",
    "    # If no match found, return None\n",
    "    print(f\"No match found for '{word}' in '{sentence}' with word_before '{word_before}' and word_after '{word_after}'\")\n",
    "    return None, None\n",
    "\n",
    "def remove_unwanted_chars(text):\n",
    "    \"\"\"Remove unwanted characters from a string\"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for char in [\")\", \".\", \"·\", \",\", \"’\", \"ανωθεν\"]:\n",
    "        if char in text:\n",
    "            print(f\"Removing character '{char}' from text\")\n",
    "            text = text\n",
    "    text = text\n",
    "    return text\n",
    "\n",
    "def fuzzy_search(pattern, text, max_mistakes):\n",
    "    \"\"\"Perform a fuzzy search for a regular expression pattern in a string\"\"\"\n",
    "    pattern = fr\"(?:{pattern}){{e<={max_mistakes}}}\"\n",
    "    return regex.search(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:31.077447500Z",
     "start_time": "2023-06-13T07:12:26.589458Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\") # I use small model for speed but you should use trf (transformer) model for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.114901400Z",
     "start_time": "2023-06-13T07:12:31.077447500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = [] # list of tuples connecting sentences and all entities in that sentence\n",
    "\n",
    "\n",
    "# create train data: loop over all quotes, and all entities in each quote, and find the index of the word in the sentence so now it entities was [[label, word before, word after], [label, word before, word after]] and now it is [[start, end, label], [start, end, label]]\n",
    "not_found_list = []\n",
    "not_found = 0\n",
    "for i, row in df_grouped.iterrows():\n",
    "    # loop over all entities\n",
    "    #print (row['Quote'])\n",
    "    new_entities = []\n",
    "    print(row['Quote'])\n",
    "    for entity in row['entities']:\n",
    "        # find the index of the word in the sentence\n",
    "        #print (entity)\n",
    "        print (entity[1])\n",
    "        start, end = find_word_index(row['Quote'], entity[1], entity[2], entity[3], clean_chars=True)\n",
    "        print (start, end, entity[1])\n",
    "        # append to TRAIN_DATA\n",
    "        if start is None:\n",
    "            not_found += 1\n",
    "            print (\"not found: \", start, end)\n",
    "            # add word and sentence to not_found list\n",
    "            not_found_list.append([entity[1], row['Quote']])\n",
    "        else:\n",
    "            new_entities.append([start, end, entity[0]])\n",
    "    TRAIN_DATA.append((row['Quote'], {'entities': new_entities}))\n",
    "    # print length of TRAIN_DATA and sum of all entities in TRAIN_DATA\n",
    "    print (len(TRAIN_DATA), sum([len(x[1]['entities']) for x in TRAIN_DATA]))\n",
    "    #print not_found words and their sentences\n",
    "    print (not_found_list)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [] # list of tuples connecting sentences and all entities in that sentence\n",
    "TRAIN_DATA_TEST = [] # list of tuples connecting sentences and all entities in that sentence\n",
    "TRAIN_DATA_TEST_CLEAN = [] # list of tuples connecting sentences and all entities in that sentence\n",
    "\n",
    "# create train data: loop over all quotes, and all entities in each quote, and find the index of the word in the sentence so now it entities was [[label, word before, word after], [label, word before, word after]] and now it is [[start, end, label], [start, end, label]]\n",
    "not_found_list = []\n",
    "not_found = 0\n",
    "not_found_test = 0\n",
    "not_found_clean = 0\n",
    "for i, row in df_grouped.iterrows():\n",
    "    # loop over all entities\n",
    "    new_entities = []\n",
    "    new_entities_test = []\n",
    "    new_entities_clean = []\n",
    "    for entity in row['entities']:\n",
    "        # find the index of the word in the sentence using find_word_index with clean_chars=False\n",
    "        start, end = find_word_index(row['Quote'], entity[1], entity[2], entity[3], clean_chars=False)\n",
    "        if start is None:\n",
    "            not_found += 1\n",
    "            # add word and sentence to not_found list\n",
    "            not_found_list.append([entity[1], row['Quote'], 'find_word_index'])\n",
    "        else:\n",
    "            new_entities.append([start, end, entity[0]])\n",
    "        # find the index of the word in the sentence using find_word_index_test\n",
    "        start_test, end_test = find_word_index_test(row['Quote'], entity[1], entity[2], entity[3])\n",
    "        if start_test is None:\n",
    "            not_found_test += 1\n",
    "            # add word and sentence to not_found list\n",
    "            not_found_list.append([entity[1], row['Quote'], 'find_word_index_test'])\n",
    "        else:\n",
    "            new_entities_test.append([start_test, end_test, entity[0]])\n",
    "        # find the index of the word in the sentence using find_word_index with clean_chars=True\n",
    "        start_clean, end_clean = find_word_index(row['Quote'], entity[1], entity[2], entity[3], clean_chars=True)\n",
    "        if start_clean is None:\n",
    "            not_found_clean += 1\n",
    "        else:\n",
    "            new_entities_clean.append([start_clean, end_clean, entity[0]])\n",
    "    TRAIN_DATA.append((row['Quote'], {'entities': new_entities}))\n",
    "    TRAIN_DATA_TEST.append((row['Quote'], {'entities': new_entities_test}))\n",
    "    TRAIN_DATA_TEST_CLEAN.append((row['Quote'], {'entities': new_entities_clean}))\n",
    "    # print length of TRAIN_DATA and sum of all entities in TRAIN_DATA\n",
    "    print(f\"TRAIN_DATA length: {len(TRAIN_DATA)}, sum of all entities: {sum([len(x[1]['entities']) for x in TRAIN_DATA])}\")\n",
    "    print(f\"TRAIN_DATA_TEST length: {len(TRAIN_DATA_TEST)}, sum of all entities: {sum([len(x[1]['entities']) for x in TRAIN_DATA_TEST])}\")\n",
    "    print(f\"TRAIN_DATA_TEST_CLEAN length: {len(TRAIN_DATA_TEST_CLEAN)}, sum of all entities: {sum([len(x[1]['entities']) for x in TRAIN_DATA_TEST_CLEAN])}\")\n",
    "    # print not_found words and their sentences for all methods\n",
    "    not_found_set = set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index']) - set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index_test']) - set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index_clean'])\n",
    "    if not_found_set:\n",
    "        print(f\"Words not found by find_word_index but found by other methods: {not_found_set}\")\n",
    "    not_found_set_test = set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index_test']) - set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index']) - set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index_clean'])\n",
    "    if not_found_set_test:\n",
    "        print(f\"Words not found by find_word_index_test but found by other methods: {not_found_set_test}\")\n",
    "    not_found_set_clean = set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index_clean']) - set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index']) - set([(x[0], x[1]) for x in not_found_list if x[2] == 'find_word_index_test'])\n",
    "    if not_found_set_clean:\n",
    "        print(f\"Words not found by find_word_index with clean_chars=True but found by other methods: {not_found_set_clean}\")\n",
    "    # print good results for all methods\n",
    "    print(f\"Good results for find_word_index: {len(new_entities)}\")\n",
    "    print(f\"Good results for find_word_index_test: {len(new_entities_test)}\")\n",
    "    print(f\"Good results for find_word_index with clean_chars=True: {len(new_entities_clean)}\")\n",
    "    \n",
    "print(f\"Words not found by any method: {not_found}\")\n",
    "print(f\"Words not found by find_word_index: {not_found}\")\n",
    "print(f\"Words not found by find_word_index_test: {not_found_test}\")\n",
    "print(f\"Words not found by find_word_index with clean_chars=True: {not_found_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.114901400Z",
     "start_time": "2023-06-13T07:12:32.095759300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print TRAIN_DATA row where column value is a specific value\n",
    "for row in TRAIN_DATA:\n",
    "    #if \" Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸς καὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\" in row[0]:\n",
    "    if \" Ἔχει δὲ διαφορὰς πολλάς, καθάπερ ἡ κοιλία, καὶ τοῦτο τὸ μόριον\" in row[0]:\n",
    "        print (row)\n",
    "        #print characters in index locations\n",
    "        print (row[0][row[1]['entities'][0][0]:row[1]['entities'][0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print chracters in index location in a text row. the text is \" Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸς καὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\"ArithmeticError\n",
    "for row in TRAIN_DATA:\n",
    "    if \"Ἐξήρτηται δ' ἐκ τῆς μεγάλης φλεβὸς καὶ τῆς ἀορτῆς, καὶ δι' αὐτοῦ φλέβες πολλαὶ καὶ πυκναί, κατατείνουσαι πρὸς τὴν τῶν ἐντέρων θέσιν, ἄνωθεν ἀρξάμεναι μέχρι κάτω\" in row[0]:\n",
    "    #if \"Φυσωμένης δὲ τῆς ἀρτηρίας ἐν ἐνίοις μὲν οὐ κατάδηλον ποιεῖ, ἐν δὲ τοῖς μείζοσι τῶν ζῴων δῆλον ὅτι εἰσέρχεται τὸ πνεῦμα εἰς αὐτήν.\" in row[0]:\n",
    "        print (row[0][row[1]['entities'][0][0]:row[1]['entities'][0][1]])\n",
    "        # print all entities in the row and the characters in the index locations\n",
    "        for entity in row[1]['entities']:\n",
    "            print (entity, row[0][entity[0]:entity[1]])\n",
    "        # print how many entities in the row\n",
    "        print (len(row[1]['entities']))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_chars(text):\n",
    "    \"\"\"Remove unwanted characters from a string\"\"\"\n",
    "    cleaned_text = \"\"\n",
    "    for char in [\")\", \".\", \"·\", \",\", \"’\"]:\n",
    "        if char in text:\n",
    "            print(f\"Removing character '{char}' from text\")\n",
    "            text = text.replace(char, \"\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test, ανωθεν.\"\n",
    "cleaned_text = remove_unwanted_chars(text)\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"Cleaned text: '{cleaned_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.258653600Z",
     "start_time": "2023-06-13T07:12:32.114901400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from TRAIN_DATA\n",
    "random.sample(TRAIN_DATA, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Add annotations from INCEpTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.320825600Z",
     "start_time": "2023-06-13T07:12:32.146255100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract all files in inception folder to temp folder\n",
    "from cassis import *\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from spacy.training import Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:30:48.392674600Z",
     "start_time": "2023-06-13T09:30:48.350703100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_inception_tags(sentence, begin, end, label):\n",
    "    \"\"\"Problem: INCEpTION texts are in different formats, we want a uniform format defined by the constant FORMAT\"\"\"\n",
    "    \"\"\"When we normalize a sentence we lose the correct offsets of the entities, so we need to fix them\"\"\"\n",
    "    \"\"\"We use the above function find_word_index to find the correct offsets\"\"\"\n",
    "\n",
    "    word = sentence[begin:end]\n",
    "    # word before is the word before the entity (take 5 chars before if exists, else take maximum available)\n",
    "    word_before = sentence[max(0, begin-5):begin]\n",
    "    # word after is the word after the entity (take 5 chars after if exists, else take maximum available)\n",
    "    word_after = sentence[end:min(len(sentence), end+5)]\n",
    "\n",
    "    # normalize all variables\n",
    "    sentence = normalize(FORMAT, sentence)\n",
    "    print (\"sentence: \", sentence)\n",
    "    word = normalize(FORMAT, word)\n",
    "    print (\"word: \", word)\n",
    "    word_before = normalize(FORMAT, word_before)\n",
    "    print (\"word before: \", word_before)\n",
    "    word_after = normalize(FORMAT, word_after)\n",
    "    print (\"word after: \", word_after)\n",
    "\n",
    "    return find_word_index(sentence, word, word_before, word_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:36:57.108804300Z",
     "start_time": "2023-06-13T09:36:56.471197800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INCEPTION_TRAIN_DATA = []\n",
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "    for f in os.listdir(\"../assets/NER_assets/INCEpTION_files/test/\"):\n",
    "        if f.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(os.path.join(\"../assets/NER_assets/INCEpTION_files/test/\", f), 'r') as zip_ref:\n",
    "                zip_ref.extractall(tempdir)\n",
    "    # open typesystem and print content\n",
    "    with open('{0}/{1}'.format(tempdir, \"TypeSystem.xml\"), 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "\n",
    "    # iterate over all files in temp folder\n",
    "    for f in os.listdir(tempdir):\n",
    "        # if file is a xmi file\n",
    "        if f.endswith(\".xmi\"):\n",
    "            # load xmi file\n",
    "            with open(os.path.join(tempdir, f), 'rb') as f:\n",
    "                print(f.name)\n",
    "                # load typesystem from temp folder\n",
    "                cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "                # get all entities from cas\n",
    "                for sentence in cas.select((\"de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence\")):\n",
    "                    ents = []\n",
    "                    for token in cas.select_covered('webanno.custom.CategoryType', sentence):\n",
    "                        print(token.get_covered_text(), token.value(\"Value\"))\n",
    "                        # create a span with the token start and end and the label\n",
    "                        # find begin and end position of the token relative to the sentence\n",
    "                        start = token.begin - sentence.begin\n",
    "                        end = token.end - sentence.begin\n",
    "\n",
    "                        # normalize the sentence, word, word_before and word_after\n",
    "                        begin, end = normalize_inception_tags(sentence.get_covered_text(), start, end, token.value(\"Value\"))\n",
    "                        ents.append((begin, end, token.value(\"Value\")))\n",
    "                    sentence = normalize(FORMAT, sentence.get_covered_text())\n",
    "                    INCEPTION_TRAIN_DATA.append((sentence, {'entities': ents}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print cas content\n",
    "for token in cas.select_covered('webanno.custom.CategoryType', sentence):\n",
    "    print(token.get_covered_text(), token.value(\"Value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in INCEPTION_TRAIN_DATA:\n",
    "    print (t[0]+ \"\\n\")\n",
    "    for entity in t[1]['entities']:\n",
    "        print (t[0][entity[0]:entity[1]], entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:12.251962100Z",
     "start_time": "2023-06-13T09:37:12.219743300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from INCEPTION_TRAIN_DATA and print for each entity text[start:end] and label\n",
    "t = random.choice(INCEPTION_TRAIN_DATA)\n",
    "print (t[0]+ \"\\n\")\n",
    "for entity in t[1]['entities']:\n",
    "    print (t[0][entity[0]:entity[1]], entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:20.622829200Z",
     "start_time": "2023-06-13T09:37:20.607098800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MERGED_TRAIN_DATA = TRAIN_DATA + INCEPTION_TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:20.832999700Z",
     "start_time": "2023-06-13T09:37:20.801576500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(\"Τελευταία γὰρ ἡ κύστις κεῖται, \\r\\nτὴν μὲν ἐξάρτησιν ἔχουσα τοῖς ἀπὸ τῶν νεφρῶν τεταμένοις \\r\\nπόροις παρὰ τὸν καυλὸν τὸν ἐπὶ τὴν οὐρήθραν τείνοντα, \\r\\nκαὶ σχεδὸν πάντῃ κύκλῳ λεπτοῖς καὶ ἰνώδεσιν ὑμενίοις \\r\\nἐστὶ προσειλημμένη, παραπλησίοις οὖσι τρόπον τινὰ τῷ \\r\\nδιαζώματι τοῦ θώρακος.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:21.405090700Z",
     "start_time": "2023-06-13T09:37:20.949788500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "losses = {}\n",
    "for text, annot in TRAIN_DATA:\n",
    "    # train model\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        # first check char span by text[start:end] and compare with annot\n",
    "        print (text[start:end], label)\n",
    "        # if label isnt float NaN\n",
    "        if label == label and label != None:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "            print (span)\n",
    "            ents.append(span)\n",
    "    # check overlapping entities (each spacy token should only be part of one entity)\n",
    "    ents = spacy.util.filter_spans(ents)\n",
    "\n",
    "    doc.ents = ents\n",
    "    docs.append(doc)\n",
    "    print(len(docs), sum([len(doc.ents) for doc in docs]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:23.302560900Z",
     "start_time": "2023-06-13T09:37:23.265829100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if charspan correctly found the entities, and all of them.\n",
    "doc = random.choice(docs)\n",
    "\n",
    "print (\"Spacy entities, for line:\")\n",
    "print (doc.text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# locate doc in MERGED_TRAIN_DATA\n",
    "line = None\n",
    "for i, row in enumerate(MERGED_TRAIN_DATA):\n",
    "    if row[0] == doc.text:\n",
    "        line = i\n",
    "        #print (row[1])\n",
    "        break\n",
    "\n",
    "print (\"\\nTable entities, for same line:\")\n",
    "for entity in MERGED_TRAIN_DATA[line][1]['entities']:\n",
    "    print (MERGED_TRAIN_DATA[line][0][entity[0]:entity[1]], entity[2])\n",
    "\n",
    "# check if there are entities in TRAIN_DATA that are not in spacy doc\n",
    "for entity in MERGED_TRAIN_DATA[line][1]['entities']:\n",
    "    if entity[2] not in [ent.label_ for ent in doc.ents]:\n",
    "        print (\"Entity not found in spacy doc: \", MERGED_TRAIN_DATA[line][0][entity[0]:entity[1]], entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "NER_DocBin = DocBin(docs=docs)\n",
    "NER_DocBin.to_disk('../corpus/Merged_NER_dataset_{0}.spacy'.format(FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split docs into train test and dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "\n",
    "train_docs, test_docs = train_test_split(docs, test_size=0.2)\n",
    "train_docs, dev_docs = train_test_split(train_docs, test_size=0.2)\n",
    "\n",
    "Path(\"../corpus/train\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../corpus/dev\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../corpus/test\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train = DocBin(docs=train_docs)\n",
    "train.to_disk(\"../corpus/train/train_ner_{0}.spacy\".format(FORMAT))\n",
    "test = DocBin(docs=test_docs)\n",
    "test.to_disk(\"../corpus/test/test_ner_{0}.spacy\".format(FORMAT))\n",
    "dev = DocBin(docs=dev_docs)\n",
    "dev.to_disk(\"../corpus/dev/dev_ner_{0}.spacy\".format(FORMAT))\n",
    "\n",
    "print (\"Train: \", len(train_docs))\n",
    "print (\"Test: \", len(test_docs))\n",
    "print (\"Dev: \", len(dev_docs))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equally distribute examples between train, dev and test data objects.\n",
    "We need to properly represent all named entities in each set. We will use a stratification algorithm twice: 1st we split the data to train and test sets, and then we split the test set to dev and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "nlp = spacy.load(\"grc_proiel_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = 'NFKC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_docbin = DocBin().from_disk(\"../corpus/Merged_NER_dataset_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "merged_docbin_docs = list(merged_docbin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to split the multilabel data into two sets\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.utils import indexable, _safe_indexing\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from sklearn.model_selection._split import _validate_shuffle_split\n",
    "from itertools import chain\n",
    "\n",
    "def multilabel_train_test_split(*arrays,\n",
    "                                test_size=None,\n",
    "                                train_size=None,\n",
    "                                random_state=None,\n",
    "                                shuffle=True,\n",
    "                                stratify=None):\n",
    "    \"\"\"\n",
    "    Train test split for multilabel classification. Uses the algorithm from: \n",
    "    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.\n",
    "    \"\"\"\n",
    "    if stratify is None:\n",
    "        return train_test_split(*arrays, test_size=test_size,train_size=train_size,\n",
    "                                random_state=random_state, stratify=None, shuffle=shuffle)\n",
    "    \n",
    "    assert shuffle, \"Stratified train/test split is not implemented for shuffle=False\"\n",
    "    \n",
    "    n_arrays = len(arrays)\n",
    "    arrays = indexable(*arrays)\n",
    "    n_samples = _num_samples(arrays[0])\n",
    "    n_train, n_test = _validate_shuffle_split(\n",
    "        n_samples, test_size, train_size, default_test_size=0.25\n",
    "    )\n",
    "    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=123)\n",
    "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
    "\n",
    "    return list(\n",
    "        chain.from_iterable(\n",
    "            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Split: train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# List of all labels as integers. \n",
    "# IMPORTANT: Make sure that the labels integers are indeed item[7].\n",
    "\n",
    "labels_ints = [item[7] for item in merged_docbin.tokens for item in item]\n",
    "#construct an ndarray with shape len(new_docbin), buffer is item[7] for item in new_docbin.tokens for item in item\n",
    "labels_nd_array = np.ndarray(shape=(len(merged_docbin), len(np.unique(labels_ints))), buffer=np.array(labels_ints), dtype=int)\n",
    "labels_nd_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = multilabel_train_test_split(merged_docbin_docs ,stratify=labels_nd_array, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train to X_train.spacy\n",
    "X_train_docbin = DocBin(docs=X_train)\n",
    "X_test_docbin = DocBin(docs=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docbin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd split: dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all labels as integers\n",
    "X_labels_ints = [item[7] for item in X_test_docbin.tokens for item in item]\n",
    "#construct an ndarray with shape len(new_docbin), buffer is item[7] for item in new_docbin.tokens for item in item\n",
    "X_labels_nd_array = np.ndarray(shape=(len(X_test_docbin), len(np.unique(X_labels_ints))), buffer=np.array(X_labels_ints), dtype=int)\n",
    "X_labels_nd_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test = multilabel_train_test_split(X_test ,stratify=X_labels_nd_array, test_size=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_docbin = DocBin(docs=X_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new sets and validate the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test data to disk\n",
    "train_docbin = DocBin(docs=X_train)\n",
    "train_docbin.to_disk(\"../corpus/train/ner_train/ner_train_{0}.spacy\".format(FORMAT))\n",
    "dev_docbin = DocBin(docs=X_dev)\n",
    "dev_docbin.to_disk(\"../corpus/dev/ner_dev/ner_dev_{0}.spacy\".format(FORMAT))\n",
    "test_docbin = DocBin(docs=X_test)\n",
    "test_docbin.to_disk(\"../corpus/test/ner_test/ner_test_{0}.spacy\".format(FORMAT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "test_data_labels = ([ent.label_ for docs in X_test for ent in docs.ents])\n",
    "dev_data_labels = ([ent.label_ for docs in X_dev for ent in docs.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the labels in the train, test and dev data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(20,10))\n",
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "test_data_labels = ([ent.label_ for docs in X_test for ent in docs.ents])\n",
    "dev_data_labels = ([ent.label_ for docs in X_dev for ent in docs.ents])\n",
    "train_data_labels = pd.DataFrame(train_data_labels, columns=['labels'])\n",
    "test_data_labels = pd.DataFrame(test_data_labels, columns=['labels'])\n",
    "dev_data_labels = pd.DataFrame(dev_data_labels, columns=['labels'])\n",
    "sns.countplot(data=train_data_labels, x='labels')\n",
    "plt.title(\"Distribution of labels in the train data\")\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=test_data_labels, x='labels')\n",
    "plt.title(\"Distribution of labels in the test data\")\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=dev_data_labels, x='labels')\n",
    "plt.title(\"Distribution of labels in the dev data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of classes in train and test set with respect to each other\n",
    "import pandas as pd\n",
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "test_data_labels = ([ent.label_ for docs in X_test for ent in docs.ents])\n",
    "dev_data_labels = ([ent.label_ for docs in X_dev for ent in docs.ents])\n",
    "train_data_labels = pd.DataFrame(train_data_labels, columns=['labels'])\n",
    "test_data_labels = pd.DataFrame(test_data_labels, columns=['labels'])\n",
    "dev_data_labels = pd.DataFrame(dev_data_labels, columns=['labels'])\n",
    "train_data_labels['dataset'] = 'train'\n",
    "test_data_labels['dataset'] = 'test'\n",
    "dev_data_labels['dataset'] = 'dev'\n",
    "all_data_labels = pd.concat([train_data_labels, test_data_labels, dev_data_labels], ignore_index=True)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=all_data_labels, x='labels', hue='dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize frequency of annotation labels in X_train set\n",
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "train_data_labels = pd.DataFrame(train_data_labels, columns=['labels'])\n",
    "train_data_labels = train_data_labels['labels'].value_counts()\n",
    "train_data_labels = pd.DataFrame(train_data_labels)\n",
    "train_data_labels = train_data_labels.reset_index()\n",
    "train_data_labels = train_data_labels.rename(columns={'index': 'label', 'labels': 'frequency'})\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data=train_data_labels, x='label', y='frequency')\n",
    "plt.title(\"Frequency of labels in the train data\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test:\", len(X_test), \"dev:\", len(X_dev), \"train:\", len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution in numbers\n",
    "grouped_labels = all_data_labels.groupby(['labels', 'dataset']).size()\n",
    "print(grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
