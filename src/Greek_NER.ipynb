{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:09:18.200289400Z",
     "start_time": "2023-06-13T07:09:05.302556800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# import requirements for converting the dataframe to Spacy Docs\n",
    "from collections import defaultdict\n",
    "from unicodedata import normalize\n",
    "import regex\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and process data from Coda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:10:17.144471Z",
     "start_time": "2023-06-13T07:10:16.982123100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_PATH = \"../assets/NER_assets/Ancient_Words.csv\"\n",
    "# read csv file\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# rename columns to fit code\n",
    "df.rename(columns = {'Word':'Keyword', 'Category Types':'Label'}, inplace = True)\n",
    "# If a cell is empty (NaN), Fill it with the value in its parallel \"Early\" column\n",
    "for row in df:\n",
    "    df['Quote'].fillna(df['Early Quote'], inplace=True)\n",
    "    df['Word Before'].fillna(df['Early Word Before'], inplace=True)\n",
    "    df['Word After'].fillna(df['Early Word After'], inplace=True)\n",
    "    df['Label'].fillna(df['Early Category Type'], inplace=True)\n",
    "# remove rows with no Keyword\n",
    "df = df.dropna(subset=['Keyword'])\n",
    "# Remove any row that isn't Greek\n",
    "pat = '[ء-ي]+'\n",
    "#df.Keyword.str.contains(pat)\n",
    "df = df[~df.Keyword.str.contains(pat, na=False)]\n",
    "#replace new line in df column\n",
    "df['Keyword'].replace('\\n', '', regex=True, inplace=True)\n",
    "#replace numbers in df\n",
    "df.replace('\\d+', '', regex=True, inplace=True)\n",
    "#replace hyphens in df column\n",
    "df.replace('-', '', regex=True, inplace=True)\n",
    "# replace comma in df column\n",
    "df['Keyword'].replace(',', '', regex=True, inplace=True)\n",
    "#replace period in df column\n",
    "df['Keyword'].replace('\\.', '', regex=True, inplace=True)\n",
    "#replace interpunkt in df column\n",
    "df['Keyword'].replace('\\·', '', regex=True, inplace=True)\n",
    "# replace multiple spaces in df column\n",
    "df.replace(' +', ' ', regex=True, inplace=True)\n",
    "# replace end punctuation in df column\n",
    "df['Keyword'].replace('\\s+$', '', regex=True, inplace=True)\n",
    "\n",
    "df.fillna(0)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:10:17.583084Z",
     "start_time": "2023-06-13T07:10:17.504959800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize table\n",
    "FORMAT = 'NFKD'\n",
    "for col in ['Keyword', 'Quote', 'Word Before', 'Word After']:\n",
    "    df[col] = df[col].apply(lambda x: normalize(FORMAT, str(x)) if not isinstance(x, float) else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fix similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = pd.DataFrame(columns=['Quote', 'entities'])\n",
    "\n",
    "# group all similar sentences together, and for each one append to entities: [label, word, word before, word after]\n",
    "entities_list = []\n",
    "for name, group in df.groupby('Quote'):\n",
    "    entities = [\n",
    "        [row['Label'], row[\"Keyword\"], row['Word Before'], row['Word After']]\n",
    "        for i, row in group.iterrows()\n",
    "    ]\n",
    "    # append to entities_list\n",
    "    entities_list.append({'Quote': name, 'entities': entities})\n",
    "\n",
    "# create new DataFrame from entities_list\n",
    "df_grouped = pd.DataFrame(entities_list)\n",
    "\n",
    "# sample random from df_grouped\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:42.125237700Z",
     "start_time": "2023-06-13T07:11:38.282223200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after naively grouping similar sentences, we need to check if they are actually similar\n",
    "# we do this by using regex with difference up to 10 chars\n",
    "checked = []\n",
    "similar_indices = []\n",
    "\n",
    "for i, row in df_grouped.iterrows():\n",
    "    # if we haven't checked this sentence yet\n",
    "    if i not in checked:\n",
    "        checked.append(i)\n",
    "        similar_indices.append([i])\n",
    "        # loop over all other sentences\n",
    "        pattern = fr\"{re.escape(row['Quote'])}\"\n",
    "        # add fuzzy matching with up to 10 mistakes\n",
    "        pattern = fr\"(?:{pattern}){{e<=10}}\"\n",
    "\n",
    "        for j, row2 in df_grouped.iterrows():\n",
    "                # if the sentences are similar\n",
    "            if regex.search(row['Quote'], row2['Quote']):\n",
    "                if j not in checked:\n",
    "                    # append to similar_indices\n",
    "                    similar_indices[-1].append(j)\n",
    "                    # append to checked\n",
    "                    checked.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:42.204175800Z",
     "start_time": "2023-06-13T07:11:42.141587200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (similar_indices)\n",
    "# choose a random group from similar_indices where the length of the group is more than 1\n",
    "group = random.choice([group for group in similar_indices if len(group) > 1])\n",
    "print (group)\n",
    "# print the sentences in the group\n",
    "for index in group:\n",
    "    print (df_grouped.loc[index, 'Quote'])\n",
    "    # print entities of each sentence in the group\n",
    "    for entity in df_grouped.loc[index, 'entities']:\n",
    "        print (entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:44.552974200Z",
     "start_time": "2023-06-13T07:11:44.155882500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the longest sentence in each group of similar sentences\n",
    "df_grouped_regexed = pd.DataFrame(columns=['Quote', 'entities'])\n",
    "for indices in similar_indices:\n",
    "    # if there is only one sentence in the group add it to df_grouped_regexed\n",
    "    if len(indices) == 1:\n",
    "        df_grouped_regexed = pd.concat([df_grouped_regexed, df_grouped.loc[indices[0]].to_frame().T], ignore_index=True)\n",
    "        continue\n",
    "    # find the longest sentence in the group\n",
    "    longest = max(indices, key=lambda x: len(df_grouped.loc[x, 'Quote']))\n",
    "    entities = []\n",
    "    # concatenate all entities from the other sentences in the group\n",
    "    for index in indices:\n",
    "        entities.extend(df_grouped.loc[index, 'entities'])\n",
    "    # create a new row with the longest sentence and the concatenated entities\n",
    "    new_row = {'Quote': df_grouped.loc[longest, 'Quote'], 'entities': entities}\n",
    "    # add the new row to df_grouped_regexed\n",
    "    df_grouped_regexed = pd.concat([df_grouped_regexed, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:45.152808200Z",
     "start_time": "2023-06-13T07:11:45.121920400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from df_grouped_regexed\n",
    "df_grouped_regexed.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:45.987165200Z",
     "start_time": "2023-06-13T07:11:45.976802700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:46.488164500Z",
     "start_time": "2023-06-13T07:11:46.441070900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped=df_grouped_regexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:46.911786Z",
     "start_time": "2023-06-13T07:11:46.864904600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from df_grouped\n",
    "df_grouped.sample(10)\n",
    "len(df_grouped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fix similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:11:48.153679200Z",
     "start_time": "2023-06-13T07:11:48.122410Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {ord('\\N{COMBINING ACUTE ACCENT}'):None, ord('\\N{COMBINING COMMA ABOVE}'):None, ord('\\N{COMBINING REVERSED COMMA ABOVE}'):None}\n",
    "\n",
    "def find_word_index(sentence, word, word_before, word_after, clean_chars=True):\n",
    "    \"\"\"Find the index of a word in a sentence, it can appear multiple times but we return by word_before and word_after\n",
    "    :returns start and end index of word in sentence\"\"\"\n",
    "\n",
    "\n",
    "    # find the index of the word in the sentence (return index of beginning of word)\n",
    "    word_index = sentence.find(word)\n",
    "\n",
    "    # if \"clean_chars\" is True, remove all accents from the sentence\n",
    "    if clean_chars: \n",
    "        # return all chars of word before: [),.,·] (meaning none of these chars will be included) so we get only data contained within the sentence\n",
    "        for char in [\")\", \".\", \"·\", ',']:\n",
    "            if word_before.find(char) != -1:\n",
    "                word_before = word_before[word_before.find(char)+1:]\n",
    "                print (\"word before: \", word_before)\n",
    "            if word_after.find(char) != -1:\n",
    "                word_after = word_after[:word_after.find(char)]\n",
    "                print (\"word after: \", word_after)\n",
    "    else:\n",
    "        #do nothing\n",
    "        pass\n",
    "\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)}){re.escape(word_after)}\"\n",
    "    # add fuzzy matching with up to 3 mistakes\n",
    "    pattern = fr\"(?:{pattern}){{e<=3}}\"\n",
    "    if match := regex.search(pattern, sentence):\n",
    "        print (\"match 3: \", match.span(1))\n",
    "        return match.span(1)\n",
    "\n",
    "    # try matching only with word_before\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)})\"\n",
    "    # add fuzzy matching with up to 3 mistakes\n",
    "    pattern = fr\"(?:{pattern}){{e<=2}}\"\n",
    "    if match := regex.search(pattern, sentence):\n",
    "        print (\"match with before: \", match.span(1))\n",
    "        return match.span(1)\n",
    "\n",
    "    # try matching only with word_after\n",
    "    pattern = fr\"({(word)}){re.escape(word_after)}\"\n",
    "    # add fuzzy matching with up to 3 mistakes\n",
    "    pattern = fr\"(?:{pattern}){{e<=2}}\"\n",
    "    if match := regex.search(pattern, sentence):\n",
    "        print (\"match with after: \", match.span(1))\n",
    "        return match.span(1)\n",
    "\n",
    "    # if no match found, return None\n",
    "    print (\"\\nNo match found for\", word, \"in\", sentence, \"\\nwith word_before\", word_before, \"and word_after\", word_after)\n",
    "    return None, None\n",
    "\n",
    "#test new functions\n",
    "\n",
    "def remove_unwanted_chars(text):\n",
    "    \"\"\"Remove unwanted characters from a string\"\"\"\n",
    "    for char in [\")\", \".\", \"·\", \",\", \"’\"]:\n",
    "        if char in text:\n",
    "            print(f\"Removing character '{char}' from text\")\n",
    "            #text = text\n",
    "            text = text.replace(char, \"\")\n",
    "    text = text\n",
    "    return text\n",
    "\n",
    "def remove_line_breaks(text):\n",
    "    \"\"\"Remove line breaks from a string\"\"\"\n",
    "    for char in [\"\\n\", \"\\r\"]:\n",
    "        if char in text:\n",
    "            print(f\"Removing line break '{char}' from text\")\n",
    "            text = text.replace(char, \" \")\n",
    "    return ' '.join(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index_test(sentence, word, word_before, word_after):\n",
    "    \"\"\"Find the index of a word in a sentence, it can appear multiple times but we return by word_before and word_after\n",
    "    :returns start and end index of word in sentence\"\"\"\n",
    "\n",
    "    # Remove unwanted characters from word_before and word_after\n",
    "    word_before = remove_unwanted_chars(word_before)\n",
    "    word_before = remove_line_breaks(word_before)\n",
    "    word_after = remove_unwanted_chars(word_after)\n",
    "    word_after = remove_line_breaks(word_after)\n",
    "\n",
    "    # Try matching with word_before and word_after\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)}){re.escape(word_after)}\"\n",
    "    match = fuzzy_search(pattern, sentence, max_mistakes=3)\n",
    "    if match:\n",
    "        return match.span(1)\n",
    "\n",
    "    # Try matching with word_before only\n",
    "    pattern = fr\"{re.escape(word_before)}({(word)})\"\n",
    "    match = fuzzy_search(pattern, sentence, max_mistakes=2)\n",
    "    if match:\n",
    "        return match.span(1)\n",
    "\n",
    "    # Try matching with word_after only\n",
    "    pattern = fr\"({(word)}){re.escape(word_after)}\"\n",
    "    match = fuzzy_search(pattern, sentence, max_mistakes=2)\n",
    "    if match:\n",
    "        return match.span(1)\n",
    "\n",
    "    # If no match found, return None\n",
    "    print(f\"No match found for '{word}' in '{sentence}' with word_before '{word_before}' and word_after '{word_after}'\")\n",
    "    return None, None\n",
    "\n",
    "def remove_unwanted_chars(text):\n",
    "    \"\"\"Remove unwanted characters from a string\"\"\"\n",
    "    for char in [\")\", \".\", \"·\", \",\", \"’\"]:\n",
    "        if char in text:\n",
    "            print(f\"Removing character '{char}' from text\")\n",
    "            #text = text\n",
    "            text = text.replace(char, \"\")\n",
    "    text = text\n",
    "    return text\n",
    "\n",
    "def remove_line_breaks(text):\n",
    "    \"\"\"Remove line breaks from a string\"\"\"\n",
    "    for char in [\"\\n\", \"\\r\"]:\n",
    "        if char in text:\n",
    "            print(f\"Removing line break '{char}' from text\")\n",
    "            text = text.replace(char, \" \")\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fuzzy_search(pattern, text, max_mistakes):\n",
    "    \"\"\"Perform a fuzzy search for a regular expression pattern in a string\"\"\"\n",
    "    pattern = fr\"(?:{pattern}){{e<={max_mistakes}}}\"\n",
    "    return regex.search(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:31.077447500Z",
     "start_time": "2023-06-13T07:12:26.589458Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\") # I use small model for speed but you should use trf (transformer) model for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.114901400Z",
     "start_time": "2023-06-13T07:12:31.077447500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = [] # list of tuples connecting sentences and all entities in that sentence\n",
    "\n",
    "\n",
    "# create train data: loop over all quotes, and all entities in each quote, and find the index of the word in the sentence so now it entities was [[label, word before, word after], [label, word before, word after]] and now it is [[start, end, label], [start, end, label]]\n",
    "not_found_list = []\n",
    "not_found = 0\n",
    "for i, row in df_grouped.iterrows():\n",
    "    # loop over all entities\n",
    "    #print (row['Quote'])\n",
    "    new_entities = []\n",
    "    print(row['Quote'])\n",
    "    for entity in row['entities']:\n",
    "        # find the index of the word in the sentence\n",
    "        #print (entity)\n",
    "        print (entity[1])\n",
    "        start, end = find_word_index(row['Quote'], entity[1], entity[2], entity[3], clean_chars=True)\n",
    "        print (start, end, entity[1])\n",
    "        # append to TRAIN_DATA\n",
    "        if start is None:\n",
    "            not_found += 1\n",
    "            print (\"not found: \", start, end)\n",
    "            # add word and sentence to not_found list\n",
    "            not_found_list.append([entity[1], row['Quote']])\n",
    "        else:\n",
    "            new_entities.append([start, end, entity[0]])\n",
    "    TRAIN_DATA.append((row['Quote'], {'entities': new_entities}))\n",
    "    # print length of TRAIN_DATA and sum of all entities in TRAIN_DATA\n",
    "    print(len(TRAIN_DATA), sum(len(x[1]['entities']) for x in TRAIN_DATA))\n",
    "    #print not_found words and their sentences\n",
    "    print (not_found_list)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.114901400Z",
     "start_time": "2023-06-13T07:12:32.095759300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_found_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.258653600Z",
     "start_time": "2023-06-13T07:12:32.114901400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from TRAIN_DATA\n",
    "random.sample(TRAIN_DATA, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Add annotations from INCEpTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:12:32.320825600Z",
     "start_time": "2023-06-13T07:12:32.146255100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract all files in inception folder to temp folder\n",
    "from cassis import *\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCEPTION_TRAIN_DATA = []\n",
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "    for f in os.listdir(\"../assets/NER_assets/INCEpTION_files/\"):\n",
    "        if f.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(os.path.join(\"../assets/NER_assets/INCEpTION_files/\", f), 'r') as zip_ref:\n",
    "                zip_ref.extractall(tempdir)\n",
    "    # open typesystem and print content\n",
    "    with open('{0}/{1}'.format(tempdir, \"TypeSystem.xml\"), 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "\n",
    "    # iterate over all files in temp folder\n",
    "    for f in os.listdir(tempdir):\n",
    "        # if file is a xmi file\n",
    "        if f.endswith(\".xmi\"):\n",
    "            # load xmi file\n",
    "            with open(os.path.join(tempdir, f), 'rb') as f:\n",
    "                print(f.name)\n",
    "                # load typesystem from temp folder\n",
    "                cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "                # get all entities from cas\n",
    "                for sentence in cas.select((\"de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence\")):\n",
    "                    ents = []\n",
    "                    # print original sentence\n",
    "                    print(\"Original sentence: \", sentence.get_covered_text())\n",
    "                    print(\"Original sentence index: \", sentence.begin, \"-\", sentence.end)\n",
    "                    print(\"Original sentence length: \", len(sentence.get_covered_text()))\n",
    "                    # remove line breaks from sentence text\n",
    "                    #sentence_text = sentence.get_covered_text().split()\n",
    "                    print(\"SSSSSSSSSSSSSSSSSSSSSentence text: \", sentence.get_covered_text())\n",
    "                    #replace ' with ᾿\n",
    "                    \n",
    "                    # normalize items in sentence_text\n",
    "                    #sentence_text = [normalize(FORMAT, x) for x in sentence_text]\n",
    "                    #sentence_text = sentence_text.replace('\\r', '').replace('\\n', '')\n",
    "                    # remove extra white spaces from sentence text without touching punctuation\n",
    "                    #sentence_text = ' '.join(sentence_text.split())\n",
    "                    # join sentence text back together\n",
    "                    #sentence_text = ' '.join(sentence.get_covered_text().replace('\\r', ' ').replace('\\n', ' ').split())\n",
    "                    #sentence_text = [x.replace(\"᾿\", \"’\") for x in sentence_text]\n",
    "                    sentence_text = ' '.join(sentence.get_covered_text().replace('\\r', ' ').replace('\\n', ' ').replace(\"᾿\", \"’\").split())\n",
    "                    #sentence_text = ' '.join(sentence_text)\n",
    "        # FIRST VERSION:::::: sentence_text = ' '.join(sentence.get_covered_text().replace('\\r', ' ').replace('\\n', ' ').split())\n",
    "                    #sentence_text = re.split(r\"(?<!\\w)\\s+|\\s+(?!\\w)\", sentence_text)\n",
    "                    print('rrrrrrrrrrrrrrrrrrrr: ', sentence_text)\n",
    "                    sentence_begin = sentence.begin\n",
    "                    sentence_end = sentence_begin + len(sentence_text)\n",
    "                    # print sentence text\n",
    "                    print(\"Cleaned Sentence text: \", sentence_text)\n",
    "                    #print clean sentence length:\n",
    "                    print(\"Cleaned sentence length: \", len(sentence_text))\n",
    "                    \n",
    "                    for token in cas.select_covered('webanno.custom.CategoryType', sentence):\n",
    "                        # create a span with the token start and end and the label\n",
    "                        # find begin and end position of the token relative to the sentence\n",
    "                        print(\"TOKEN: \", token.get_covered_text(), \"| Index: \", token.begin, \"-\", token.end)\n",
    "                        # print the begin and end index numbers of sentence.get_covered_text()\n",
    "                        #print(\"Sentence index: \", sentence.begin, \"-\", sentence.end, \"| Sentence: \", sentence.get_covered_text()[(token.begin - sentence.begin):(token.end - sentence.begin)])\n",
    "                        # get the index of text before the token\n",
    "                        sentence_text_before = sentence.get_covered_text()[:(token.begin - sentence_begin)].replace(\"᾿\", \"’\")\n",
    "                        print(\"1\",sentence_text_before)\n",
    "                        sentence_text_before = ' '.join(sentence_text_before.split()) + ' ' if sentence_text_before else ''\n",
    "                        print(sentence_text_before.split())\n",
    "                        print(\"2\",sentence_text_before)\n",
    "                        print(\"sentence_text_before calculation: \", token.begin, '-', sentence_begin)\n",
    "                        sentence_text_before = normalize(FORMAT, sentence_text_before)\n",
    "                        \n",
    "                        sentence_text_after = sentence.get_covered_text()[:(token.end - sentence_begin)].replace(\"᾿\", \"’\")\n",
    "                        print(\"sentence_text_after calculation: \", token.end, '-', sentence_begin)\n",
    "                        sentence_text_after = ' '.join(sentence_text_after.split())\n",
    "                        sentence_text_after = normalize(FORMAT, sentence_text_after)\n",
    "                        print(\"Sentence text before token begin: \", sentence_text_before, len(sentence_text_before))\n",
    "                        print(\"Sentence text before token end: \", sentence_text_after, len(sentence_text_after))\n",
    "                        token_begin = len(sentence_text_before)\n",
    "                        token_end = len(sentence_text_after)\n",
    "                        \n",
    "                        #print(\"original sentence Index: \", sentence.begin, \"-\", sentence.end)\n",
    "                        print(\"New Begin: \", sentence.begin, \"+\", len(sentence_text_before), \"=\", token_begin)\n",
    "                        print(\"New end: \", sentence.begin, \"+\", len(sentence_text_after), \"=\", token_end)\n",
    "                        #print(\"New Begin index: \", begin, \"New End index: \", end)\n",
    "                        #sentence.begin = sentence.begin\n",
    "                        # print sentence length\n",
    "                        #print(\"Sentence length: \", len(sentence.get_covered_text()))\n",
    "                        # sentence.end is the end of the sentence_text\n",
    "                        #sentence.end = sentence_begin + len(sentence_text)\n",
    "                        print(sentence.begin, \"+\", len(sentence_text))\n",
    "                        print(\"New sentence index: \", sentence.begin, \"-\", sentence_end)\n",
    "                        #print string in location token_begin:token_end in sentence_text\n",
    "                        #print(\"New token: \", sentence_begin, sentence_end, sentence_text[token_begin:token_end])\n",
    "                        print(\"New token: \", sentence_text[token_begin:token_end])\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #print(\"New token: \", sentence_text[token_begin:token_end])\n",
    "                        # normalize the sentence, word, word_before and word_after\n",
    "                        ents.append((token_begin, token_end, token.value(\"Value\")))\n",
    "                        print(\"ents: \", ents)\n",
    "                    #print sentence text length\n",
    "                    print(\"Sentence length: \", len(sentence_text))\n",
    "                    sentence_text = normalize(FORMAT, sentence_text)\n",
    "                    #print sentencvce text length after normalization\n",
    "                    print(\"Sentence length after normalization: \", len(sentence_text))\n",
    "                    print(\"Final sentence: \", sentence_text)\n",
    "                    INCEPTION_TRAIN_DATA.append((sentence_text, {'entities': ents}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save INCEPTION_TRAIN_DATA to csv file\n",
    "df_INCEPTION_TRAIN_DATA = pd.DataFrame(INCEPTION_TRAIN_DATA)\n",
    "df_INCEPTION_TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCEPTION_TRAIN_DATA\n",
    "# save TRAIN_DATA to spacy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in INCEPTION_TRAIN_DATA:\n",
    "    print (t[0]+ \"\\n\")\n",
    "    for entity in t[1]['entities']:\n",
    "        print (entity, t[0][entity[0]:entity[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in INCEPTION_TRAIN_DATA:\n",
    "    print (t[0]+ \"\\n\")\n",
    "    for entity in t[1]['entities']:\n",
    "        print (t[0][entity[0]:entity[1]], entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:12.251962100Z",
     "start_time": "2023-06-13T09:37:12.219743300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample random from INCEPTION_TRAIN_DATA and print for each entity text[start:end] and label\n",
    "t = random.choice(INCEPTION_TRAIN_DATA)\n",
    "print (t[0]+ \"\\n\")\n",
    "for entity in t[1]['entities']:\n",
    "    print (t[0][entity[0]:entity[1]], entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:20.622829200Z",
     "start_time": "2023-06-13T09:37:20.607098800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MERGED_TRAIN_DATA = TRAIN_DATA + INCEPTION_TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:21.405090700Z",
     "start_time": "2023-06-13T09:37:20.949788500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "losses = {}\n",
    "for text, annot in MERGED_TRAIN_DATA:\n",
    "    # train model\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        # first check char span by text[start:end] and compare with annot\n",
    "        print (text[start:end], label)\n",
    "        # if label isnt float NaN\n",
    "        if label == label and label != None:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "            print (span)\n",
    "            ents.append(span)\n",
    "    # check overlapping entities (each spacy token should only be part of one entity)\n",
    "    ents = spacy.util.filter_spans(ents)\n",
    "\n",
    "    doc.ents = ents\n",
    "    docs.append(doc)\n",
    "    print(len(docs), sum(len(doc.ents) for doc in docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T09:37:23.302560900Z",
     "start_time": "2023-06-13T09:37:23.265829100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if charspan correctly found the entities, and all of them.\n",
    "doc = random.choice(docs)\n",
    "\n",
    "print (\"Spacy entities, for line:\")\n",
    "print (doc.text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# locate doc in MERGED_TRAIN_DATA\n",
    "line = None\n",
    "for i, row in enumerate(MERGED_TRAIN_DATA):\n",
    "    if row[0] == doc.text:\n",
    "        line = i\n",
    "        #print (row[1])\n",
    "        break\n",
    "\n",
    "print (\"\\nTable entities, for same line:\")\n",
    "for entity in MERGED_TRAIN_DATA[line][1]['entities']:\n",
    "    print (MERGED_TRAIN_DATA[line][0][entity[0]:entity[1]], entity[2])\n",
    "\n",
    "# check if there are entities in TRAIN_DATA that are not in spacy doc\n",
    "for entity in MERGED_TRAIN_DATA[line][1]['entities']:\n",
    "    if entity[2] not in [ent.label_ for ent in doc.ents]:\n",
    "        print (\"Entity not found in spacy doc: \", MERGED_TRAIN_DATA[line][0][entity[0]:entity[1]], entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "NER_DocBin = DocBin(docs=docs)\n",
    "NER_DocBin.to_disk('../corpus/Merged_NER_dataset_{0}.spacy'.format(FORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "\n",
    "def split_docs(docs, format):\n",
    "    # Split the docs into train, test, and dev sets\n",
    "    train_docs, test_docs = train_test_split(docs, test_size=0.2)\n",
    "    train_docs, dev_docs = train_test_split(train_docs, test_size=0.2)\n",
    "\n",
    "    # Create directories for the train, test, and dev sets\n",
    "    Path(\"../corpus/train\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../corpus/dev\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../corpus/test\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the train, test, and dev sets to disk\n",
    "    train = DocBin(docs=train_docs)\n",
    "    train.to_disk(\"../corpus/train/train_ner_{0}.spacy\".format(format))\n",
    "    test = DocBin(docs=test_docs)\n",
    "    test.to_disk(\"../corpus/test/test_ner_{0}.spacy\".format(format))\n",
    "    dev = DocBin(docs=dev_docs)\n",
    "    dev.to_disk(\"../corpus/dev/dev_ner_{0}.spacy\".format(format))\n",
    "\n",
    "    # Print the number of docs in each set\n",
    "    print(\"Train: \", len(train_docs))\n",
    "    print(\"Test: \", len(test_docs))\n",
    "    print(\"Dev: \", len(dev_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equally distribute examples between train, dev and test data objects.\n",
    "We need to properly represent all named entities in each set. We will use a stratification algorithm twice: 1st we split the data to train and test sets, and then we split the test set to dev and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "nlp = spacy.load(\"grc_proiel_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = 'NFKC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_docbin = DocBin().from_disk(\"../corpus/Merged_NER_dataset_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "merged_docbin_docs = list(merged_docbin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in merged_docbin_docs:\n",
    "    for ents in doc.ents:\n",
    "        print (doc, ents.text, ents.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to split the multilabel data into two sets\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.utils import indexable, _safe_indexing\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from sklearn.model_selection._split import _validate_shuffle_split\n",
    "from itertools import chain\n",
    "\n",
    "def multilabel_train_test_split(*arrays,\n",
    "                                test_size=None,\n",
    "                                train_size=None,\n",
    "                                random_state=None,\n",
    "                                shuffle=True,\n",
    "                                stratify=None):\n",
    "    \"\"\"\n",
    "    Train test split for multilabel classification. Uses the algorithm from: \n",
    "    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.\n",
    "    \"\"\"\n",
    "    if stratify is None:\n",
    "        return train_test_split(*arrays, test_size=test_size,train_size=train_size,\n",
    "                                random_state=random_state, stratify=None, shuffle=shuffle)\n",
    "    \n",
    "    assert shuffle, \"Stratified train/test split is not implemented for shuffle=False\"\n",
    "    \n",
    "    n_arrays = len(arrays)\n",
    "    arrays = indexable(*arrays)\n",
    "    n_samples = _num_samples(arrays[0])\n",
    "    n_train, n_test = _validate_shuffle_split(\n",
    "        n_samples, test_size, train_size, default_test_size=0.25\n",
    "    )\n",
    "    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=123)\n",
    "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
    "\n",
    "    return list(\n",
    "        chain.from_iterable(\n",
    "            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Split: train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# List of all labels as integers. \n",
    "# IMPORTANT: Make sure that the labels integers are indeed item[7].\n",
    "\n",
    "labels_ints = [item[7] for item in merged_docbin.tokens for item in item]\n",
    "#construct an ndarray with shape len(new_docbin), buffer is item[7] for item in new_docbin.tokens for item in item\n",
    "labels_nd_array = np.ndarray(shape=(len(merged_docbin), len(np.unique(labels_ints))), buffer=np.array(labels_ints), dtype=int)\n",
    "labels_nd_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = multilabel_train_test_split(merged_docbin_docs ,stratify=labels_nd_array, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train to X_train.spacy\n",
    "X_train_docbin = DocBin(docs=X_train)\n",
    "X_test_docbin = DocBin(docs=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docbin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd split: dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all labels as integers\n",
    "X_labels_ints = [item[7] for item in X_test_docbin.tokens for item in item]\n",
    "#construct an ndarray with shape len(new_docbin), buffer is item[7] for item in new_docbin.tokens for item in item\n",
    "X_labels_nd_array = np.ndarray(shape=(len(X_test_docbin), len(np.unique(X_labels_ints))), buffer=np.array(X_labels_ints), dtype=int)\n",
    "X_labels_nd_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test = multilabel_train_test_split(X_test ,stratify=X_labels_nd_array, test_size=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_docbin = DocBin(docs=X_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new sets and validate the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test data to disk\n",
    "train_docbin = DocBin(docs=X_train)\n",
    "train_docbin.to_disk(\"../corpus/train/ner_train/ner_train_{0}.spacy\".format(FORMAT))\n",
    "dev_docbin = DocBin(docs=X_dev)\n",
    "dev_docbin.to_disk(\"../corpus/dev/ner_dev/ner_dev_{0}.spacy\".format(FORMAT))\n",
    "test_docbin = DocBin(docs=X_test)\n",
    "test_docbin.to_disk(\"../corpus/test/ner_test/ner_test_{0}.spacy\".format(FORMAT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "test_data_labels = ([ent.label_ for docs in X_test for ent in docs.ents])\n",
    "dev_data_labels = ([ent.label_ for docs in X_dev for ent in docs.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the labels in the train, test and dev data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(20,10))\n",
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "test_data_labels = ([ent.label_ for docs in X_test for ent in docs.ents])\n",
    "dev_data_labels = ([ent.label_ for docs in X_dev for ent in docs.ents])\n",
    "train_data_labels = pd.DataFrame(train_data_labels, columns=['labels'])\n",
    "test_data_labels = pd.DataFrame(test_data_labels, columns=['labels'])\n",
    "dev_data_labels = pd.DataFrame(dev_data_labels, columns=['labels'])\n",
    "sns.countplot(data=train_data_labels, x='labels')\n",
    "plt.title(\"Distribution of labels in the train data\")\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=test_data_labels, x='labels')\n",
    "plt.title(\"Distribution of labels in the test data\")\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=dev_data_labels, x='labels')\n",
    "plt.title(\"Distribution of labels in the dev data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of classes in train and test set with respect to each other\n",
    "import pandas as pd\n",
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "test_data_labels = ([ent.label_ for docs in X_test for ent in docs.ents])\n",
    "dev_data_labels = ([ent.label_ for docs in X_dev for ent in docs.ents])\n",
    "train_data_labels = pd.DataFrame(train_data_labels, columns=['labels'])\n",
    "test_data_labels = pd.DataFrame(test_data_labels, columns=['labels'])\n",
    "dev_data_labels = pd.DataFrame(dev_data_labels, columns=['labels'])\n",
    "train_data_labels['dataset'] = 'train'\n",
    "test_data_labels['dataset'] = 'test'\n",
    "dev_data_labels['dataset'] = 'dev'\n",
    "all_data_labels = pd.concat([train_data_labels, test_data_labels, dev_data_labels], ignore_index=True)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=all_data_labels, x='labels', hue='dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize frequency of annotation labels in X_train set\n",
    "train_data_labels = ([ent.label_ for docs in X_train for ent in docs.ents])\n",
    "train_data_labels = pd.DataFrame(train_data_labels, columns=['labels'])\n",
    "train_data_labels = train_data_labels['labels'].value_counts()\n",
    "train_data_labels = pd.DataFrame(train_data_labels)\n",
    "train_data_labels = train_data_labels.reset_index()\n",
    "train_data_labels = train_data_labels.rename(columns={'index': 'label', 'labels': 'frequency'})\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data=train_data_labels, x='label', y='frequency')\n",
    "plt.title(\"Frequency of labels in the train data\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test:\", len(X_test), \"dev:\", len(X_dev), \"train:\", len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution in numbers\n",
    "grouped_labels = all_data_labels.groupby(['labels', 'dataset']).size()\n",
    "print(grouped_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train, test and dev data\n",
    "#import spacy\n",
    "#from spacy.tokens import DocBin\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")\n",
    "FORMAT = 'NFKC'\n",
    "train_docbin = DocBin().from_disk(\"../corpus/train/ner_train/ner_train_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "train_docs = list(train_docbin.get_docs(nlp.vocab))\n",
    "dev_docbin = DocBin().from_disk(\"../corpus/dev/ner_dev/ner_dev_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "dev_docs = list(dev_docbin.get_docs(nlp.vocab))\n",
    "test_docbin = DocBin().from_disk(\"../corpus/test/ner_test/ner_test_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "test_docs = list(test_docbin.get_docs(nlp.vocab))\n",
    "# count sentences in train, test and dev data\n",
    "print (\"train:\", len(train_docs), \"dev:\", len(dev_docs), \"test:\", len(test_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train, test and dev data\n",
    "#import spacy\n",
    "#from spacy.tokens import DocBin\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")\n",
    "FORMAT = 'NFKD'\n",
    "train_docbin = DocBin().from_disk(\"../corpus/train/ner_train/ner_train_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "train_docs = list(train_docbin.get_docs(nlp.vocab))\n",
    "dev_docbin = DocBin().from_disk(\"../corpus/dev/ner_dev/ner_dev_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "dev_docs = list(dev_docbin.get_docs(nlp.vocab))\n",
    "test_docbin = DocBin().from_disk(\"../corpus/test/ner_test/ner_test_{0}.spacy\".format(FORMAT))\n",
    "# get docs from new_docbin\n",
    "test_docs = list(test_docbin.get_docs(nlp.vocab))\n",
    "# count sentences in train, test and dev data\n",
    "print (\"train:\", len(train_docs), \"dev:\", len(dev_docs), \"test:\", len(test_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
