{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import conllu\n",
    "#from conllu import parse\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using vectors\n",
    "#import spacy\n",
    "#from spacy.language import Language\n",
    "#spacy.prefer_gpu()\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")\n",
    "#!python -m spacy init vectors grc ../assets/grc_floret_cbow_nn2_xn10_b200k_dim300.floret ../vectors/large --mode floret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# apostrophes and correct_apostrophe are defined as follows:\n",
    "apostrophes = [\"᾽\", \"᾿\", \"'\", \"’\", \"‘\"]\n",
    "correct_apostrophe = \"ʼ\"\n",
    "\n",
    "def clean_and_remove_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing diacritics (accents), except for specific characters,\n",
    "    and converting it to lowercase.\n",
    "    \"\"\"\n",
    "    allowed_characters = [' ̓', \"᾿\", \"᾽\", \"'\", \"’\", \"‘\", 'ʼ', '̓']  # Including the Greek apostrophe\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    try:\n",
    "        non_accent_chars = [c for c in unicodedata.normalize('NFKD', text) \n",
    "        if unicodedata.category(c) != 'Mn' or c in allowed_characters]\n",
    "        return ''.join(non_accent_chars)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # A more generic exception handling if unexpected errors occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return text\n",
    "    \n",
    "\n",
    "def normalize_text(text: str, form: str = 'NFKD', \n",
    "                   remove_accents: bool = False, \n",
    "                   lowercase: bool = False, \n",
    "                   standardize_apostrophe: bool = True, \n",
    "                   remove_brackets: bool = False, \n",
    "                   remove_trailing_numbers: bool = False, \n",
    "                   remove_extra_spaces: bool = False, \n",
    "                   debug: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Applies multiple text normalization and cleaning steps on the input text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be normalized.\n",
    "    - form (str): Unicode normalization form ('NFC', 'NFD', 'NFKC', 'NFKD').\n",
    "    - lowercase (bool): If True, the text is converted to lowercase.\n",
    "    - standardize_apostrophe (bool): If True, replaces all defined apostrophe characters with a standard one.\n",
    "    - remove_brackets_only (bool): If True, removes the brackets themselves.\n",
    "    - remove_trailing_numbers (bool): If True, strips leading or trailing digits from the text.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The processed text.\n",
    "    \"\"\"\n",
    "    normalized_text = text  # Initialize normalized_text with the original text\n",
    "\n",
    "    # Function to print before and after states for each operation during debugging\n",
    "    def debug_print(operation_name, before, after):\n",
    "        if debug:\n",
    "            print(f\"{operation_name} - Before: {before}\")\n",
    "            print(f\"{operation_name} - After: {after}\")\n",
    "\n",
    "    # Standardize apostrophe characters if required\n",
    "    if standardize_apostrophe:\n",
    "        before_text = normalized_text\n",
    "        for apos in apostrophes:\n",
    "            normalized_text = normalized_text.replace(apos, correct_apostrophe)\n",
    "        debug_print(\"Standardizing apostrophes\", before_text, normalized_text)\n",
    "        \n",
    "    if remove_accents:\n",
    "        before_text = normalized_text\n",
    "        try:\n",
    "            normalized_text = clean_and_remove_accents(normalized_text)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while removing accents: {e}\")\n",
    "            # Decide what to do here: return the original text, a special value, or stop the process\n",
    "            return text        \n",
    "        debug_print(\"Removing accents\", before_text, normalized_text)\n",
    "        \n",
    "    # Convert to lowercase if required\n",
    "    if lowercase:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = normalized_text.lower()\n",
    "        debug_print(\"Lowercase conversion\", before_text, normalized_text)\n",
    "\n",
    "    # Unicode normalization\n",
    "    if form:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = unicodedata.normalize(form, normalized_text)\n",
    "        debug_print(\"Unicode normalization\", before_text, normalized_text)\n",
    "            \n",
    "    # Remove brackets only if required\n",
    "    if remove_brackets:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = re.sub(r'[\\(\\)\\[\\]]', '', normalized_text)\n",
    "        debug_print(\"Removing brackets\", before_text, normalized_text)\n",
    "        \n",
    "    # Remove trailing numbers if required\n",
    "    if remove_trailing_numbers:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = re.sub(r'^\\d+|\\d+$', '', normalized_text)\n",
    "        debug_print(\"Removing trailing numbers\", before_text, normalized_text)\n",
    "\n",
    "    # Remove multiple spaces and leading/trailing spaces\n",
    "    if remove_extra_spaces:\n",
    "        before_text = normalized_text\n",
    "        normalized_text = ' '.join(normalized_text.split()).strip()\n",
    "        debug_print(\"Removing extra spaces\", before_text, normalized_text)\n",
    "\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import os\n",
    "\n",
    "def process_sentences(input_files, output_file, combine=False):\n",
    "    \"\"\"\n",
    "    Processes .conllu files: cleans text, separates sentences based on punctuation,\n",
    "    and optionally combines multiple .conllu files.\n",
    "\n",
    "    Args:\n",
    "        input_files (list): List of paths to input .conllu files.\n",
    "        output_file (str): Path to the output .conllu file.\n",
    "        combine (bool, optional): Whether to combine input files. Defaults to False.\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "\n",
    "    # Loop through each input file, whether combining or not\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        sentences = conllu.parse(text)\n",
    "        if combine:\n",
    "            all_sentences.extend(sentences)  # Combine sentences from all files\n",
    "        else:\n",
    "            all_sentences = sentences  # Use sentences from the current file only\n",
    "            break  # Exit loop after the first file if not combining\n",
    "\n",
    "    rebuilt_sentences = []\n",
    "    sent_id = 1\n",
    "    current_sentence_tokens = []\n",
    "    token_id = 1  # Initialize token id for running numbers throughout each sentence\n",
    "\n",
    "    for sentence in all_sentences:\n",
    "        for token in sentence:\n",
    "            # Clean the text for 'form' and 'lemma'\n",
    "            token['form'] = normalize_text(token['form'], remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_extra_spaces=True, debug=False)\n",
    "            token['lemma'] = normalize_text(token['lemma'], remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_extra_spaces=True, debug=False)\n",
    "            # Update current token's id to ensure running numbers\n",
    "            token['id'] = token_id\n",
    "            \n",
    "            # Add the token to the current sentence tokens and increment token_id\n",
    "            current_sentence_tokens.append(token)\n",
    "            token_id += 1            \n",
    "            \n",
    "            # Check if the current token is punctuation that indicates end of a sentence\n",
    "            if token[\"form\"] in [\".\", \"·\"]:\n",
    "                # Append the current sentence tokens as a new TokenList to rebuilt_sentences\n",
    "                metadata = {\"sent_id\": str(sent_id), \"text\": \"NA\"}\n",
    "                rebuilt_sentences.append(conllu.TokenList(tokens=current_sentence_tokens, metadata=metadata))\n",
    "                current_sentence_tokens = []  # Reset for the next sentence\n",
    "                sent_id += 1\n",
    "                token_id = 1  # Reset token id for the new sentence\n",
    "                \n",
    "    # Finalize the last sentence if it doesn't end with specified punctuation\n",
    "    if current_sentence_tokens:\n",
    "        metadata = {\"sent_id\": str(sent_id), \"text\": \"NA\"}\n",
    "        rebuilt_sentences.append(conllu.TokenList(tokens=current_sentence_tokens, metadata=metadata))\n",
    "        \n",
    "    # Write rebuilt sentences to the output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for sentence in rebuilt_sentences:\n",
    "            out_f.write(sentence.serialize())\n",
    "            out_f.write(\"\\n\\n\")  # Ensure correct .conllu formatting with blank lines\n",
    "\n",
    "    # Print summary\n",
    "    summary = \"Combined\" if combine else \"Original\"\n",
    "    print(f\"{summary} number of sentences from input files: {len(all_sentences)}\")\n",
    "    print(f\"Rebuilt and separated {len(rebuilt_sentences)} sentences.\\n\")\n",
    "\n",
    "\n",
    "# Example usage for processing a single file\n",
    "# process_sentences([\"input_file.conllu\"], \"output_file.conllu\", combine=False)\n",
    "\n",
    "# Example usage for combining multiple files into one output file\n",
    "# process_sentences([\"input_file1.conllu\", \"input_file2.conllu\"], \"combined_output.conllu\", combine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../assets/Lemmatization_training_files\"\n",
    "for entry in os.listdir(directory):\n",
    "    full_path = os.path.join(directory, entry)\n",
    "    if os.path.isfile(full_path) and entry.endswith(\".conllu\"):\n",
    "        process_sentences([full_path], \"../assets/Lemmatization_training_files/Processed/\" + entry[:-7] + \"_NFKD.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import unicodedata\n",
    "import conllu\n",
    "from pathlib import Path\n",
    "def adjust_tokens_for_spacy(sentences, debug=False):\n",
    "    \"\"\"\n",
    "    Adjusts tokens in parsed sentences for spaCy's trainable lemmatizer requirements\n",
    "    and handles specific token conditions, setting appropriate defaults.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            # Adjustments for forms and lemmas\n",
    "            if token[\"form\"] in ['', \"_\", '—', '-']:\n",
    "                token[\"form\"] = token[\"lemma\"] if token[\"lemma\"] not in ['', \"_\", '—', '-'] else \"_\"\n",
    "                print(f\"Adjusted form for token {token['id']}, form: {token['form']}\") if debug else None\n",
    "                \n",
    "            if token[\"lemma\"] in ['', \"_\", '—', '-']:\n",
    "                token[\"lemma\"] = \"_\"\n",
    "                print(f\"Adjusted lemma for token {token['id']}, lemma: {token['lemma']}\") if debug else None\n",
    "            \n",
    "            # ID and UPOS adjustments\n",
    "            if token[\"id\"] == '':\n",
    "                token[\"id\"] = \"UNK\"  # Example arbitrary value for unknown IDs\n",
    "                print(f\"Adjusted ID for token {token['form']}, ID: {token['id']}\") if debug else None\n",
    "                \n",
    "            if token[\"upos\"] in ['', \"_\", '—', '-']:\n",
    "                token[\"upos\"] = \"_\"  # Use '' for as per spaCy standard or 'X' for unknown UPOS as per CoNLL-U standard\n",
    "                print(f\"Adjusted UPOS for token {token['form']}, UPOS: {token['upos']}\") if debug else None\n",
    "                \n",
    "            if token[\"upos\"] in ['END', 'MID']:\n",
    "                token[\"upos\"] = \"NOUN\"  # Correcting specific UPOS conditions\n",
    "                print(f\"Adjusted UPOS for token {token['form']}, UPOS: {token['upos']}\") if debug else None\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def process_and_normalize_files(input_directory, output_directory, normalization_form='NFKD', debug=False):\n",
    "    \"\"\"\n",
    "    Process .conllu files in the given directory, normalize text according\n",
    "    to the specified normalization form, and split data into training and\n",
    "    development sets, with conditions adjusted for spaCy's requirements.\n",
    "    \"\"\"\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    # Check if input directory exists\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"Error: The input directory '{input_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Check if output directory exists, create if not\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\") if debug else None\n",
    "    \n",
    "    # Process and normalize each .conllu file in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".conllu\"):\n",
    "            print(\"\\n\", file_name) if debug else None\n",
    "            \n",
    "            # Read file\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            sentences = conllu.parse(open(file_path, \"r\", encoding=\"utf-8\").read())\n",
    "            \n",
    "            # Adjust tokens before normalization and spaCy conversion\n",
    "            sentences = adjust_tokens_for_spacy(sentences)\n",
    "            \n",
    "            # Parse sentences\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    # Apply normalization to token form and lemma\n",
    "                    token[\"form\"] = normalize_text(token[\"form\"], form=normalization_form)\n",
    "                    token[\"lemma\"] = normalize_text(token[\"lemma\"], form=normalization_form)\n",
    "            \n",
    "            # Set the seed for reproducibility\n",
    "            random.seed(42)\n",
    "            # Shuffle the sentences randomly\n",
    "            random.shuffle(sentences)\n",
    "\n",
    "            # Split the sentences into training and development data\n",
    "            split_index = int(len(sentences) * 0.9)\n",
    "            train_data, dev_data = sentences[:split_index], sentences[split_index:]\n",
    "            \n",
    "            # Write training and development data\n",
    "            train_output_file = os.path.join(output_directory, f\"{file_name[:-7]}_{normalization_form}_train.conllu\")\n",
    "            dev_output_file = os.path.join(output_directory, f\"{file_name[:-7]}_{normalization_form}_dev.conllu\")\n",
    "            with open(train_output_file, \"w\", encoding=\"utf-8\") as train_file:\n",
    "                for sentence in train_data:\n",
    "                    train_file.write(sentence.serialize())\n",
    "            with open(dev_output_file, \"w\", encoding=\"utf-8\") as dev_file:\n",
    "                for sentence in dev_data:\n",
    "                    dev_file.write(sentence.serialize())\n",
    "            \n",
    "            print(f\"Processed and normalized {file_name}. Train and dev data saved.\") if debug else None\n",
    "\n",
    "# Example usage:\n",
    "# Make sure to specify your actual paths for the input directory and output directory\n",
    "# process_and_normalize_files(\"../assets/Lemmatization_training_files/test\", \"../assets/Lemmatization_training_files/lemma_train\", \"NFKD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_normalize_files(\"../assets/Lemmatization_training_files/Processed\", \"../assets/Lemmatization_training_files/Processed/lemma_train\", \"NFKD\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_normalize_files(\"../assets/Lemmatization_training_files/Processed\", \"../assets/Lemmatization_training_files/Processed/lemma_train\", \"NFKC\", debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing spaCy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import conllu\n",
    "import subprocess\n",
    "\n",
    "def validate_head_indices(sentences, debug=False):\n",
    "    \"\"\"\n",
    "    Validates that all head indices in the tokens of the sentences are within the valid range.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[TokenList]): List of sentences parsed from a .conllu file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all head indices are valid, False otherwise.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        token_ids = {token[\"id\"] for token in sentence}  # Set of valid token IDs for reference\n",
    "        for token in sentence:\n",
    "            # Assuming head is directly accessible in token and is an int\n",
    "            head = token.get(\"head\")  # Use .get() to safely handle missing 'head' entries\n",
    "            \n",
    "            # Check if head exists or is set to None\n",
    "            if head is None:\n",
    "                print(f\"Missing head for token '{token['form']}' in sentence: {sentence.metadata.get('text', 'NA')}\") if debug else None\n",
    "                return False\n",
    "\n",
    "            # Check if head index is within the valid range or is a root (0)\n",
    "            if head not in token_ids and head != 0:\n",
    "                print(f\"Invalid head index {head} for token '{token['form']}' in sentence: {sentence.metadata.get('text', 'NA')}\") if debug else None\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def read_and_parse_conllu(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Reads and parses a .conllu file from the given path.\n",
    "    \"\"\"\n",
    "    \n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    # Check if input directory exists\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"Error: The input directory '{input_directory}' does not exist.\") if debug else None\n",
    "        return\n",
    "\n",
    "    # Check if output directory exists, create if not\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\") if debug else None\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as data:\n",
    "        annotations = data.read()\n",
    "    return conllu.parse(annotations)\n",
    "\n",
    "def convert_to_spacy(file_path, output_directory, sentences):\n",
    "    \"\"\"\n",
    "    Converts the .conllu file to spaCy format if head indices are valid.\n",
    "    \"\"\"\n",
    "    extra_args = \"--n-sents 10\" if len(sentences) >= 10 else \"\"\n",
    "    convert_command = f\"python -m spacy convert {file_path} {output_directory} -c conllu -m --merge-subtokens {extra_args}\"\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(convert_command.split(), check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error converting file '{os.path.basename(file_path)}':\", e) # Print error message\n",
    "\n",
    "def process_conllu_files(input_directory, output_directory, debug=False):\n",
    "    \"\"\"\n",
    "    Processes all .conllu files in the input directory, validating and converting them.\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".conllu\"):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            \n",
    "            print(f\"\\nProcessing file: {file_name}\") # Print current file being processed\n",
    "            sentences = read_and_parse_conllu(file_path)\n",
    "            \n",
    "            #if validate_head_indices(sentences):\n",
    "            #    print(f\"{file_name}: {len(sentences)} sentences - Head indices valid.\")\n",
    "            #    convert_to_spacy(file_path, output_directory, sentences)\n",
    "            #else:\n",
    "            #    print(f\"{file_name}: Head indices validation failed. Conversion skipped.\")\n",
    "            #convert_to_spacy(file_path, output_directory, sentences)\n",
    "            convert_to_spacy(file_path, output_directory, sentences)\n",
    "# Example usage\n",
    "#input_directory = \"../assets/Lemmatization_training_files/test/\"\n",
    "#output_directory = \"../assets/Lemmatization_training_files/test/\"\n",
    "#process_conllu_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"../assets/Lemmatization_training_files/Processed/lemma_train/\"\n",
    "output_directory = \"../assets/Lemmatization_training_files/Processed/lemma_train/spaCy/\"\n",
    "process_conllu_files(input_directory, output_directory, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process Proiel and Perseus files\n",
    "\n",
    "def process_corpus_conllu_files(input_directory, output_directory, normalization_form):\n",
    "    \"\"\"\n",
    "    Processes .conllu files in the given directory, normalizes token forms and lemmas \n",
    "    using the specified normalization form, and writes them to a new directory.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".conllu\"):\n",
    "            input_file_path = os.path.join(input_directory, file_name)\n",
    "            with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                annotations = file.read()\n",
    "\n",
    "            sentences = conllu.parse(annotations)\n",
    "            output_file_path = os.path.join(output_directory, f\"{file_name[:-7]}_{normalization_form}.conllu\")\n",
    "            \n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for sentence in sentences:\n",
    "                    for token in sentence:\n",
    "                        token[\"form\"] = normalize_text((token[\"form\"]), normalization_form, remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_extra_spaces=True, debug=False)\n",
    "                        token[\"lemma\"] = normalize_text(token[\"lemma\"], normalization_form, remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_extra_spaces=True, debug=False)\n",
    "                    file.write(sentence.serialize())\n",
    "            \n",
    "            print(f\"Processed file: {file_name}\")\n",
    "            token['form'] = normalize_text(token['form'], remove_accents=False, lowercase=False, standardize_apostrophe=True, remove_extra_spaces=True, debug=False)\n",
    "\n",
    "# Example usage:\n",
    "#input_directory = \"../assets/UD_Ancient_Greek-Perseus\"\n",
    "#output_directory_nfkd = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD\"\n",
    "#process_conllu_files(input_directory, output_directory_nfkd, 'NFKD')\n",
    "\n",
    "#output_directory_nfkc = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC\"\n",
    "#process_conllu_files(input_directory, output_directory_nfkc, 'NFKC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"../assets/UD_Ancient_Greek-Perseus\"\n",
    "output_directory_nfkd = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD\"\n",
    "process_corpus_conllu_files(input_directory, output_directory_nfkd, normalization_form='NFKD')\n",
    "\n",
    "output_directory_nfkc = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC\"\n",
    "process_corpus_conllu_files(input_directory, output_directory_nfkc, normalization_form='NFKC')\n",
    "\n",
    "input_directory = \"../assets/UD_Ancient_Greek-PROIEL\"\n",
    "output_directory_nfkd = \"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD\"\n",
    "process_corpus_conllu_files(input_directory, output_directory_nfkd, normalization_form='NFKD')\n",
    "\n",
    "output_directory_nfkc = \"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC\"\n",
    "process_corpus_conllu_files(input_directory, output_directory_nfkc, normalization_form='NFKC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert conllu to spacy UD_Ancient_Greek and UD_Ancient_Greek-PROIEL\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# install spacy grc model if not already installed\n",
    "nlp = spacy.load(\"grc_proiel_trf\") # Use your preferred model here\n",
    "\n",
    "lemma_train= DocBin().from_disk('../corpus/train/lemma_train/train_lemma_NFKC.spacy')\n",
    "lemma_train_docs = list(lemma_train.get_docs(nlp.vocab))\n",
    "\n",
    "PROIEL_NFKD= DocBin().from_disk('../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/grc_proiel-ud-train_NFKD.spacy')\n",
    "PROIEL_NFKD_docs = list(PROIEL_NFKD.get_docs(nlp.vocab))\n",
    "\n",
    "PROIEL_NFKC= DocBin().from_disk('../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/grc_proiel-ud-train_NFKC.spacy')\n",
    "PROIEL_NFKC_docs = list(PROIEL_NFKC.get_docs(nlp.vocab))\n",
    "\n",
    "Perseus_NFKD= DocBin().from_disk('../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/grc_perseus-ud-train_NFKD.spacy')\n",
    "Perseus_NFKD_docs = list(Perseus_NFKD.get_docs(nlp.vocab))\n",
    "\n",
    "Perseus_NFKC= DocBin().from_disk('../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/grc_perseus-ud-train_NFKC.spacy')\n",
    "Perseus_NFKC_docs = list(Perseus_NFKC.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through sentences in lemma_train. If a sentence is in any of the other files, print the sentence and the file it is in\n",
    "for doc in lemma_train_docs:\n",
    "    if doc in PROIEL_NFKD_docs:\n",
    "        print(\"PROIEL_NFKD\")\n",
    "        print(doc)\n",
    "    if doc in PROIEL_NFKC_docs:\n",
    "        print(\"PROIEL_NFKC\")\n",
    "        print(doc)\n",
    "    if doc in Perseus_NFKD_docs:\n",
    "        print(\"Perseus_NFKD\")\n",
    "        print(doc)\n",
    "    if doc in Perseus_NFKC_docs:\n",
    "        print(\"Perseus_NFKC\")\n",
    "        print(doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sentence in lemma_train contain \"XLV.\" if so, then print sentence\n",
    "for doc in lemma_train_docs:\n",
    "    if \"XLV.\" in doc.text:\n",
    "        print(doc.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.5"
=======
   "version": "3.12.1"
>>>>>>> 1dae430 (Upated scripts)
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
