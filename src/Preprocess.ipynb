{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import conllu\n",
    "#from conllu import parse\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using vectors\n",
    "#import spacy\n",
    "#from spacy.language import Language\n",
    "#spacy.prefer_gpu()\n",
    "#nlp = spacy.load(\"grc_proiel_trf\")\n",
    "#!python -m spacy init vectors grc ../assets/grc_floret_cbow_nn2_xn10_b200k_dim300.floret ../vectors/large --mode floret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the given text by removing diacritics (accents), except for specific characters,\n",
    "    and converting it to lowercase.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the input is not a string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    \n",
    "    try:\n",
    "        non_accent_characters = [\n",
    "            char for char in unicodedata.normalize('NFKD', text)\n",
    "            if unicodedata.category(char) != 'Mn' or char == '̓'  # Keep Greek coronis\n",
    "        ]\n",
    "        # Use str.lower() for converting to lowercase, which works for Unicode characters\n",
    "        return ''.join(non_accent_characters).lower()\n",
    "    except Exception as e:\n",
    "        # A more generic exception handling if unexpected errors occur\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import os\n",
    "\n",
    "def process_sentences(input_files, output_file, combine=False):\n",
    "    \"\"\"\n",
    "    Processes .conllu files: cleans text, separates sentences based on punctuation,\n",
    "    and optionally combines multiple .conllu files.\n",
    "\n",
    "    Args:\n",
    "        input_files (list): List of paths to input .conllu files.\n",
    "        output_file (str): Path to the output .conllu file.\n",
    "        combine (bool, optional): Whether to combine input files. Defaults to False.\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "\n",
    "    # Loop through each input file, whether combining or not\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        sentences = conllu.parse(text)\n",
    "        if combine:\n",
    "            all_sentences.extend(sentences)  # Combine sentences from all files\n",
    "        else:\n",
    "            all_sentences = sentences  # Use sentences from the current file only\n",
    "            break  # Exit loop after the first file if not combining\n",
    "\n",
    "    rebuilt_sentences = []\n",
    "    sent_id = 1\n",
    "    current_sentence_tokens = []\n",
    "    token_id = 1  # Initialize token id for running numbers throughout each sentence\n",
    "\n",
    "    for sentence in all_sentences:\n",
    "        for token in sentence:\n",
    "            # Clean the text for 'form' and 'lemma'\n",
    "            token['form'] = clean_text(token['form'])\n",
    "            token['lemma'] = clean_text(token['lemma'])\n",
    "            # Update current token's id to ensure running numbers\n",
    "            token['id'] = token_id\n",
    "            \n",
    "            # Add the token to the current sentence tokens and increment token_id\n",
    "            current_sentence_tokens.append(token)\n",
    "            token_id += 1            \n",
    "            \n",
    "            # Check if the current token is punctuation that indicates end of a sentence\n",
    "            if token[\"form\"] in [\".\", \"·\"]:\n",
    "                # Append the current sentence tokens as a new TokenList to rebuilt_sentences\n",
    "                metadata = {\"sent_id\": str(sent_id), \"text\": \"NA\"}\n",
    "                rebuilt_sentences.append(conllu.TokenList(tokens=current_sentence_tokens, metadata=metadata))\n",
    "                current_sentence_tokens = []  # Reset for the next sentence\n",
    "                sent_id += 1\n",
    "                token_id = 1  # Reset token id for the new sentence\n",
    "                \n",
    "    # Finalize the last sentence if it doesn't end with specified punctuation\n",
    "    if current_sentence_tokens:\n",
    "        metadata = {\"sent_id\": str(sent_id), \"text\": \"NA\"}\n",
    "        rebuilt_sentences.append(conllu.TokenList(tokens=current_sentence_tokens, metadata=metadata))\n",
    "        \n",
    "    # Write rebuilt sentences to the output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for sentence in rebuilt_sentences:\n",
    "            out_f.write(sentence.serialize())\n",
    "            out_f.write(\"\\n\\n\")  # Ensure correct .conllu formatting with blank lines\n",
    "\n",
    "    # Print summary\n",
    "    summary = \"Combined\" if combine else \"Original\"\n",
    "    print(f\"{summary} number of sentences from input files: {len(all_sentences)}\")\n",
    "    print(f\"Rebuilt and separated {len(rebuilt_sentences)} sentences.\\n\")\n",
    "\n",
    "\n",
    "# Example usage for processing a single file\n",
    "# process_sentences([\"input_file.conllu\"], \"output_file.conllu\", combine=False)\n",
    "\n",
    "# Example usage for combining multiple files into one output file\n",
    "# process_sentences([\"input_file1.conllu\", \"input_file2.conllu\"], \"combined_output.conllu\", combine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of sentences from input files: 3180\n",
      "Rebuilt and separated 11680 sentences.\n",
      "\n",
      "Original number of sentences from input files: 3577\n",
      "Rebuilt and separated 6055 sentences.\n",
      "\n",
      "Original number of sentences from input files: 2763\n",
      "Rebuilt and separated 5860 sentences.\n",
      "\n",
      "Original number of sentences from input files: 437\n",
      "Rebuilt and separated 2695 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1\n",
      "Rebuilt and separated 305 sentences.\n",
      "\n",
      "Original number of sentences from input files: 4338\n",
      "Rebuilt and separated 10022 sentences.\n",
      "\n",
      "Original number of sentences from input files: 2896\n",
      "Rebuilt and separated 1292 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1417\n",
      "Rebuilt and separated 4615 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1476\n",
      "Rebuilt and separated 3732 sentences.\n",
      "\n",
      "Original number of sentences from input files: 3363\n",
      "Rebuilt and separated 3363 sentences.\n",
      "\n",
      "Original number of sentences from input files: 5834\n",
      "Rebuilt and separated 3154 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1368\n",
      "Rebuilt and separated 1609 sentences.\n",
      "\n",
      "Original number of sentences from input files: 9577\n",
      "Rebuilt and separated 3917 sentences.\n",
      "\n",
      "Original number of sentences from input files: 27789\n",
      "Rebuilt and separated 14682 sentences.\n",
      "\n",
      "Original number of sentences from input files: 12018\n",
      "Rebuilt and separated 5417 sentences.\n",
      "\n",
      "Original number of sentences from input files: 2392\n",
      "Rebuilt and separated 6552 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1406\n",
      "Rebuilt and separated 3361 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1355\n",
      "Rebuilt and separated 4578 sentences.\n",
      "\n",
      "Original number of sentences from input files: 275\n",
      "Rebuilt and separated 5550 sentences.\n",
      "\n",
      "Original number of sentences from input files: 2248\n",
      "Rebuilt and separated 4593 sentences.\n",
      "\n",
      "Original number of sentences from input files: 376\n",
      "Rebuilt and separated 8342 sentences.\n",
      "\n",
      "Original number of sentences from input files: 7760\n",
      "Rebuilt and separated 4207 sentences.\n",
      "\n",
      "Original number of sentences from input files: 1014\n",
      "Rebuilt and separated 2329 sentences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"../assets/Lemmatization_training_files\"\n",
    "for entry in os.listdir(directory):\n",
    "    full_path = os.path.join(directory, entry)\n",
    "    if os.path.isfile(full_path) and entry.endswith(\".conllu\"):\n",
    "        process_sentences([full_path], \"../assets/Lemmatization_training_files/test/\" + entry[:-7] + \"_no_accents_NFKD.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import unicodedata\n",
    "import conllu\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_text(text, normalization_form='NFKD'):\n",
    "    \"\"\"\n",
    "    Normalize the given text using specified Unicode normalization form.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(normalization_form, text)\n",
    "\n",
    "def adjust_tokens_for_spacy(sentences):\n",
    "    \"\"\"\n",
    "    Adjusts tokens in parsed sentences for spaCy's trainable lemmatizer requirements\n",
    "    and handles specific token conditions, setting appropriate defaults.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            # Adjustments for forms and lemmas\n",
    "            if token[\"form\"] in ['', \"_\", '—', '-']:\n",
    "                token[\"form\"] = token[\"lemma\"] if token[\"lemma\"] not in ['', \"_\", '—', '-'] else \"_\"\n",
    "            if token[\"lemma\"] in ['', \"_\", '—', '-']:\n",
    "                token[\"lemma\"] = \"_\"\n",
    "            \n",
    "            # ID and UPOS adjustments\n",
    "            if token[\"id\"] == '':\n",
    "                token[\"id\"] = \"UNK\"  # Example arbitrary value for unknown IDs\n",
    "            if token[\"upos\"] in ['', \"_\", '—', '-']:\n",
    "                token[\"upos\"] = \"_\"  # Use '' for as per spaCy standard or 'X' for unknown UPOS as per CoNLL-U standard\n",
    "            if token[\"upos\"] in ['END', 'MID']:\n",
    "                token[\"upos\"] = \"NOUN\"  # Correcting specific UPOS conditions\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def process_and_normalize_files(input_directory, output_directory, normalization_form='NFKD'):\n",
    "    \"\"\"\n",
    "    Process .conllu files in the given directory, normalize text according\n",
    "    to the specified normalization form, and split data into training and\n",
    "    development sets, with conditions adjusted for spaCy's requirements.\n",
    "    \"\"\"\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    # Check if input directory exists\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"Error: The input directory '{input_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Check if output directory exists, create if not\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\")\n",
    "    \n",
    "    # Process and normalize each .conllu file in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".conllu\"):\n",
    "            print(\"\\n\", file_name)\n",
    "            \n",
    "            # Read file\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            sentences = conllu.parse(open(file_path, \"r\", encoding=\"utf-8\").read())\n",
    "            \n",
    "            # Adjust tokens before normalization and spaCy conversion\n",
    "            sentences = adjust_tokens_for_spacy(sentences)\n",
    "            \n",
    "            # Parse sentences\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    # Apply normalization to token form and lemma\n",
    "                    token[\"form\"] = normalize_text(token[\"form\"], normalization_form)\n",
    "                    token[\"lemma\"] = normalize_text(token[\"lemma\"], normalization_form)\n",
    "            \n",
    "            # Set the seed for reproducibility\n",
    "            random.seed(42)\n",
    "            # Shuffle the sentences randomly\n",
    "            random.shuffle(sentences)\n",
    "\n",
    "            # Split the sentences into training and development data\n",
    "            split_index = int(len(sentences) * 0.8)\n",
    "            train_data, dev_data = sentences[:split_index], sentences[split_index:]\n",
    "            \n",
    "            # Write training and development data\n",
    "            train_output_file = os.path.join(output_directory, f\"{file_name[:-7]}_{normalization_form}_train.conllu\")\n",
    "            dev_output_file = os.path.join(output_directory, f\"{file_name[:-7]}_{normalization_form}_dev.conllu\")\n",
    "            with open(train_output_file, \"w\", encoding=\"utf-8\") as train_file:\n",
    "                for sentence in train_data:\n",
    "                    train_file.write(sentence.serialize())\n",
    "            with open(dev_output_file, \"w\", encoding=\"utf-8\") as dev_file:\n",
    "                for sentence in dev_data:\n",
    "                    dev_file.write(sentence.serialize())\n",
    "            \n",
    "            print(f\"Processed and normalized {file_name}. Train and dev data saved.\")\n",
    "\n",
    "# Example usage:\n",
    "# Make sure to specify your actual paths for the input directory and output directory\n",
    "# process_and_normalize_files(\"../assets/Lemmatization_training_files/test\", \"../assets/Lemmatization_training_files/lemma_train\", \"NFKD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " euripides_no_accents_NFKD.conllu\n",
      "Processed and normalized euripides_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plato_ii_no_accents_NFKD.conllu\n",
      "Processed and normalized plato_ii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " lucian_no_accents_NFKD.conllu\n",
      "Processed and normalized lucian_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plato_iii_no_accents_NFKD.conllu\n",
      "Processed and normalized plato_iii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " orators_no_accents_NFKD.conllu\n",
      "Processed and normalized orators_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " homer_no_accents_NFKD.conllu\n",
      "Processed and normalized homer_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plutarch_no_accents_NFKD.conllu\n",
      "Processed and normalized plutarch_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " thucydides_no_accents_NFKD.conllu\n",
      "Processed and normalized thucydides_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " aristotle_no_accents_NFKD.conllu\n",
      "Processed and normalized aristotle_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_iv_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_iv_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " sophocles_no_accents_NFKD.conllu\n",
      "Processed and normalized sophocles_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " herodotus_no_accents_NFKD.conllu\n",
      "Processed and normalized herodotus_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plato_iv_no_accents_NFKD.conllu\n",
      "Processed and normalized plato_iv_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_iii_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_iii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " pausanias_no_accents_NFKD.conllu\n",
      "Processed and normalized pausanias_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " aeschylus_no_accents_NFKD.conllu\n",
      "Processed and normalized aeschylus_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " apollonius_no_accents_NFKD.conllu\n",
      "Processed and normalized apollonius_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_i_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_i_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " galen_no_accents_NFKD.conllu\n",
      "Processed and normalized galen_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " aristophanes_no_accents_NFKD.conllu\n",
      "Processed and normalized aristophanes_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_ii_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_ii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " pindar_no_accents_NFKD.conllu\n",
      "Processed and normalized pindar_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " arrian_no_accents_NFKD.conllu\n",
      "Processed and normalized arrian_no_accents_NFKD.conllu. Train and dev data saved.\n"
     ]
    }
   ],
   "source": [
    "process_and_normalize_files(\"../assets/Lemmatization_training_files/test\", \"../assets/Lemmatization_training_files/test/lemma_train\", \"NFKD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " euripides_no_accents_NFKD.conllu\n",
      "Processed and normalized euripides_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plato_ii_no_accents_NFKD.conllu\n",
      "Processed and normalized plato_ii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " lucian_no_accents_NFKD.conllu\n",
      "Processed and normalized lucian_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plato_iii_no_accents_NFKD.conllu\n",
      "Processed and normalized plato_iii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " orators_no_accents_NFKD.conllu\n",
      "Processed and normalized orators_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " homer_no_accents_NFKD.conllu\n",
      "Processed and normalized homer_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plutarch_no_accents_NFKD.conllu\n",
      "Processed and normalized plutarch_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " thucydides_no_accents_NFKD.conllu\n",
      "Processed and normalized thucydides_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " aristotle_no_accents_NFKD.conllu\n",
      "Processed and normalized aristotle_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_iv_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_iv_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " sophocles_no_accents_NFKD.conllu\n",
      "Processed and normalized sophocles_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " herodotus_no_accents_NFKD.conllu\n",
      "Processed and normalized herodotus_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " plato_iv_no_accents_NFKD.conllu\n",
      "Processed and normalized plato_iv_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_iii_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_iii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " pausanias_no_accents_NFKD.conllu\n",
      "Processed and normalized pausanias_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " aeschylus_no_accents_NFKD.conllu\n",
      "Processed and normalized aeschylus_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " apollonius_no_accents_NFKD.conllu\n",
      "Processed and normalized apollonius_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_i_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_i_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " galen_no_accents_NFKD.conllu\n",
      "Processed and normalized galen_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " aristophanes_no_accents_NFKD.conllu\n",
      "Processed and normalized aristophanes_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " xenophon_ii_no_accents_NFKD.conllu\n",
      "Processed and normalized xenophon_ii_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " pindar_no_accents_NFKD.conllu\n",
      "Processed and normalized pindar_no_accents_NFKD.conllu. Train and dev data saved.\n",
      "\n",
      " arrian_no_accents_NFKD.conllu\n",
      "Processed and normalized arrian_no_accents_NFKD.conllu. Train and dev data saved.\n"
     ]
    }
   ],
   "source": [
    "process_and_normalize_files(\"../assets/Lemmatization_training_files/test\", \"../assets/Lemmatization_training_files/test/lemma_train\", \"NFKC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing spaCy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import conllu\n",
    "import subprocess\n",
    "\n",
    "def validate_head_indices(sentences):\n",
    "    \"\"\"\n",
    "    Validates that all head indices in the tokens of the sentences are within the valid range.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[TokenList]): List of sentences parsed from a .conllu file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all head indices are valid, False otherwise.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        token_ids = {token[\"id\"] for token in sentence}  # Set of valid token IDs for reference\n",
    "        for token in sentence:\n",
    "            # Assuming head is directly accessible in token and is an int\n",
    "            head = token.get(\"head\")  # Use .get() to safely handle missing 'head' entries\n",
    "            \n",
    "            # Check if head exists or is set to None\n",
    "            if head is None:\n",
    "                print(f\"Missing head for token '{token['form']}' in sentence: {sentence.metadata.get('text', 'NA')}\")\n",
    "                return False\n",
    "\n",
    "            # Check if head index is within the valid range or is a root (0)\n",
    "            if head not in token_ids and head != 0:\n",
    "                print(f\"Invalid head index {head} for token '{token['form']}' in sentence: {sentence.metadata.get('text', 'NA')}\")\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def read_and_parse_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Reads and parses a .conllu file from the given path.\n",
    "    \"\"\"\n",
    "    \n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    # Check if input directory exists\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"Error: The input directory '{input_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Check if output directory exists, create if not\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as data:\n",
    "        annotations = data.read()\n",
    "    return conllu.parse(annotations)\n",
    "\n",
    "def convert_to_spacy(file_path, output_directory, sentences):\n",
    "    \"\"\"\n",
    "    Converts the .conllu file to spaCy format if head indices are valid.\n",
    "    \"\"\"\n",
    "    extra_args = \"--n-sents 10\" if len(sentences) >= 10 else \"\"\n",
    "    convert_command = f\"python -m spacy convert {file_path} {output_directory} -c conllu -m --merge-subtokens {extra_args}\"\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(convert_command.split(), check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error converting file '{os.path.basename(file_path)}':\", e)\n",
    "\n",
    "def process_conllu_files(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Processes all .conllu files in the input directory, validating and converting them.\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".conllu\"):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            \n",
    "            print(f\"\\nProcessing file: {file_name}\")\n",
    "            sentences = read_and_parse_conllu(file_path)\n",
    "            \n",
    "            #if validate_head_indices(sentences):\n",
    "            #    print(f\"{file_name}: {len(sentences)} sentences - Head indices valid.\")\n",
    "            #    convert_to_spacy(file_path, output_directory, sentences)\n",
    "            #else:\n",
    "            #    print(f\"{file_name}: Head indices validation failed. Conversion skipped.\")\n",
    "            #convert_to_spacy(file_path, output_directory, sentences)\n",
    "            convert_to_spacy(file_path, output_directory, sentences)\n",
    "# Example usage\n",
    "#input_directory = \"../assets/Lemmatization_training_files/test/\"\n",
    "#output_directory = \"../assets/Lemmatization_training_files/test/\"\n",
    "#process_conllu_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: aristophanes_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (85 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristophanes_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iv_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (25 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iv_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristophanes_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (337 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristophanes_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: lucian_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (216 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/lucian_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iv_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iv_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iii_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (92 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iii_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_ii_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (111 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_ii_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_i_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (525 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_i_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: thucydides_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (485 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/thucydides_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristotle_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (167 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristotle_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_ii_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (111 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_ii_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: orators_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (118 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/orators_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: sophocles_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (109 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/sophocles_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iv_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (25 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iv_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristophanes_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (85 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristophanes_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iii_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (93 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iii_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: galen_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (68 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/galen_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plutarch_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (368 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plutarch_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_ii_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (75 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_ii_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iii_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (367 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iii_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: arrian_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (68 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/arrian_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: thucydides_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (122 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/thucydides_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_ii_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (444 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_ii_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: sophocles_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (434 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/sophocles_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_i_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (132 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_i_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plutarch_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (92 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plutarch_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: lucian_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (54 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/lucian_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_ii_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (299 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_ii_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iv_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iv_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: sophocles_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (109 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/sophocles_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_i_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (132 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_i_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristotle_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (668 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristotle_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: pindar_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (129 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pindar_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: pausanias_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (935 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pausanias_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: homer_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (294 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/homer_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: sophocles_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (434 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/sophocles_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_i_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (525 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_i_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: pindar_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (33 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pindar_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iii_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (93 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iii_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: apollonius_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (253 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/apollonius_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: thucydides_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (485 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/thucydides_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aeschylus_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (79 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aeschylus_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: pausanias_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (234 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pausanias_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: arrian_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (269 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/arrian_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: herodotus_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (201 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/herodotus_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: thucydides_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (122 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/thucydides_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: aeschylus_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (79 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aeschylus_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: aeschylus_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (314 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aeschylus_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: homer_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (294 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/homer_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: arrian_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (269 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/arrian_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: euripides_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (26 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/euripides_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iv_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (187 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iv_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: plutarch_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (368 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plutarch_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: orators_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (118 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/orators_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_ii_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (75 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_ii_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: orators_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (469 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/orators_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iii_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (370 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iii_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: lucian_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (216 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/lucian_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: pindar_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (33 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pindar_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: euripides_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (104 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/euripides_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: pausanias_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (234 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pausanias_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristotle_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (167 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristotle_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: galen_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (68 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/galen_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: pindar_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (129 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pindar_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: homer_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1175 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/homer_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: herodotus_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (201 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/herodotus_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: herodotus_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (802 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/herodotus_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristotle_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (668 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristotle_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iv_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (47 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iv_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: lucian_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (54 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/lucian_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: arrian_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (68 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/arrian_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plutarch_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (92 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plutarch_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: homer_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1175 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/homer_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iii_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (367 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iii_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iv_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (47 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iv_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: apollonius_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (64 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/apollonius_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: apollonius_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (253 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/apollonius_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: apollonius_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (64 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/apollonius_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: euripides_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (104 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/euripides_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: pausanias_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (935 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/pausanias_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iii_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (370 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iii_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_ii_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (299 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_ii_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: herodotus_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (802 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/herodotus_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: galen_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (269 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/galen_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: euripides_no_accents_NFKD_NFKD_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (26 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/euripides_no_accents_NFKD_NFKD_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_iii_no_accents_NFKD_NFKC_dev.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (92 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_iii_no_accents_NFKD_NFKC_dev.spacy\u001b[0m\n",
      "\n",
      "Processing file: galen_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (269 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/galen_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: plato_ii_no_accents_NFKD_NFKD_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (444 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/plato_ii_no_accents_NFKD_NFKD_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aristophanes_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (337 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aristophanes_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: aeschylus_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (314 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/aeschylus_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: xenophon_iv_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (187 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/xenophon_iv_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n",
      "\n",
      "Processing file: orators_no_accents_NFKD_NFKC_train.conllu\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (469 documents):\n",
      "../assets/Lemmatization_training_files/test/lemma_train/spaCy/orators_no_accents_NFKD_NFKC_train.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"../assets/Lemmatization_training_files/test/lemma_train/\"\n",
    "output_directory = \"../assets/Lemmatization_training_files/test/lemma_train/spaCy/\"\n",
    "process_conllu_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import conllu\n",
    "import unicodedata\n",
    "\n",
    "def process_conllu_files(input_directory, output_directory, normalization_form):\n",
    "    \"\"\"\n",
    "    Processes .conllu files in the given directory, normalizes token forms and lemmas \n",
    "    using the specified normalization form, and writes them to a new directory.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".conllu\"):\n",
    "            input_file_path = os.path.join(input_directory, file_name)\n",
    "            with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                annotations = file.read()\n",
    "\n",
    "            sentences = conllu.parse(annotations)\n",
    "            output_file_path = os.path.join(output_directory, f\"{file_name[:-7]}_{normalization_form}.conllu\")\n",
    "            \n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                for sentence in sentences:\n",
    "                    for token in sentence:\n",
    "                        token[\"form\"] = normalize_text(clean_text(token[\"form\"]), normalization_form)\n",
    "                        token[\"lemma\"] = normalize_text(clean_text(token[\"lemma\"]), normalization_form)\n",
    "                    file.write(sentence.serialize())\n",
    "            \n",
    "            print(f\"Processed file: {file_name}\")\n",
    "\n",
    "# Example usage:\n",
    "#input_directory = \"../assets/UD_Ancient_Greek-Perseus\"\n",
    "#output_directory_nfkd = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD\"\n",
    "#process_conllu_files(input_directory, output_directory_nfkd, 'NFKD')\n",
    "\n",
    "#output_directory_nfkc = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC\"\n",
    "#process_conllu_files(input_directory, output_directory_nfkc, 'NFKC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"../assets/UD_Ancient_Greek-Perseus\"\n",
    "output_directory_nfkd = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD\"\n",
    "process_conllu_files(input_directory, output_directory_nfkd, 'NFKD')\n",
    "\n",
    "output_directory_nfkc = \"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC\"\n",
    "process_conllu_files(input_directory, output_directory_nfkc, 'NFKC')\n",
    "\n",
    "input_directory = \"../assets/UD_Ancient_Greek-PROIEL\"\n",
    "output_directory_nfkd = \"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFK\"\n",
    "process_conllu_files(input_directory, output_directory_nfkd, 'NFKD')\n",
    "\n",
    "output_directory_nfkc = \"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC\"\n",
    "process_conllu_files(input_directory, output_directory_nfkc, 'NFKC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert conllu to spacy UD_Ancient_Greek and UD_Ancient_Greek-PROIEL\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_train= DocBin().from_disk('../corpus/train/lemma_train/lemma_train.spacy')\n",
    "lemma_train_docs = list(lemma_train.get_docs(nlp.vocab))\n",
    "\n",
    "PROIEL_NFKD= DocBin().from_disk('../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/grc_proiel-ud-train_NFKD.spacy')\n",
    "PROIEL_NFKD_docs = list(PROIEL_NFKD.get_docs(nlp.vocab))\n",
    "\n",
    "PROIEL_NFKC= DocBin().from_disk('../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/grc_proiel-ud-train_NKFC.spacy')\n",
    "PROIEL_NFKC_docs = list(PROIEL_NFKC.get_docs(nlp.vocab))\n",
    "\n",
    "Perseus_NFKD= DocBin().from_disk('../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/grc_perseus-ud-train_NFKD.spacy')\n",
    "Perseus_NFKD_docs = list(Perseus_NFKD.get_docs(nlp.vocab))\n",
    "\n",
    "Perseus_NFKC= DocBin().from_disk('../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/grc_perseus-ud-train_NFKC.spacy')\n",
    "Perseus_NFKC_docs = list(Perseus_NFKC.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through sentences in lemma_train. If a sentence is in any of the other files, print the sentence and the file it is in\n",
    "for doc in lemma_train_docs:\n",
    "    if doc not in PROIEL_NFKD_docs:\n",
    "        print(\"PROIEL_NFKD\")\n",
    "        print(doc)\n",
    "    if doc in PROIEL_NFKC_docs:\n",
    "        print(\"PROIEL_NFKC\")\n",
    "        print(doc)\n",
    "    if doc in Perseus_NFKD_docs:\n",
    "        print(\"Perseus_NFKD\")\n",
    "        print(doc)\n",
    "    if doc in Perseus_NFKC_docs:\n",
    "        print(\"Perseus_NFKC\")\n",
    "        print(doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sentence in lemma_train contain \"XLV.\" if so, then print sentence\n",
    "for doc in lemma_train_docs:\n",
    "    if \"XLV.\" in doc.text:\n",
    "        print(doc.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
