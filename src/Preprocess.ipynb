{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import conllu\n",
    "#from conllu import parse\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using vectors\n",
    "!python -m spacy init vectors grc ../assets/grc_floret_cbow_nn2_xn10_b200k_dim300.floret ../vectors/large --mode floret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if making senter\n",
    "!python -m spacy train ../configs/senter_vec.cfg --output ../training/senter/large --gpu-id 0 --paths.train ../corpus/train/grc_perseus-ud-train.spacy --paths.dev ../corpus/dev/grc_perseus-ud-dev.spacy --paths.vectors ../vectors/large --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/lemmatizer_vec.cfg --output ../training/lemmatizer/large --gpu-id 0 --paths.train ../corpus/train/lemma_train --paths.dev ../corpus/dev/lemma_dev --paths.vectors ../vectors/large --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ../configs/transformer.cfg --output ../training/transformer/proiel/assembled --gpu-id 0 --paths.train ../corpus/train/grc_proiel-ud-train.spacy --paths.dev ../corpus/dev/grc_proiel-ud-dev.spacy --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents definition\n",
    "import unicodedata\n",
    "def clean_text(text: str) -> str:\n",
    "    #Cleans the given text by stripping accents and lowercasing.\n",
    "    try:\n",
    "        non_accent_characters = [\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "        or char == '̓'  # Greek coronis\n",
    "        ]\n",
    "    # str.lower() works for unicode characters\n",
    "        return ''.join(non_accent_characters).lower()\n",
    "    except TypeError:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "from conllu import TokenList\n",
    "from collections import OrderedDict\n",
    "\n",
    "def separate_sentences(input_file, output_file):\n",
    "    # Load the input file\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Parse the input file using the conllu library\n",
    "    sentences = conllu.parse(text)\n",
    "\n",
    "    # Separate the sentences based on the punctuation\n",
    "    new_sentences = TokenList()\n",
    "    print(new_sentences)\n",
    "    current_sentence = []\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            current_sentence.append(token)\n",
    "            print(current_sentence)\n",
    "            if token[\"form\"] == \".\":\n",
    "                new_sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "        if current_sentence:\n",
    "            new_sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    print(\"Number of sentences:\", len(new_sentences))\n",
    "    print(new_sentences)\n",
    "    print(type(new_sentences))\n",
    "    # create a conllu file with the new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "from conllu import TokenList\n",
    "from collections import OrderedDict\n",
    "\n",
    "class MyToken(OrderedDict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self[\"new_field_1\"] = \"_\"\n",
    "        self[\"new_field_2\"] = \"_\"\n",
    "        self[\"new_field_3\"] = \"_\"\n",
    "        self[\"new_field_4\"] = \"_\"\n",
    "        self[\"new_field_5\"] = \"_\"\n",
    "        self[\"new_field_6\"] = \"_\"\n",
    "        self[\"new_field_7\"] = \"_\"\n",
    "        self[\"new_field_8\"] = \"_\"\n",
    "        self[\"new_field_9\"] = \"_\"\n",
    "        self[\"new_field_10\"] = \"_\"\n",
    "\n",
    "\n",
    "\n",
    "def separate_sentences(input_file, output_file):\n",
    "    # Load the input file\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Parse the input file using the conllu library\n",
    "    sentences = conllu.parse(text, token_class=MyToken)\n",
    "    print(sentences)\n",
    "    # Separate the sentences based on the punctuation\n",
    "    new_sentences = TokenList()\n",
    "    current_sentence = []\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if not token:\n",
    "                continue\n",
    "            if token[\"form\"] == \"\" or token[\"form\"].startswith(\"# newdoc id =\") or token[\"form\"].startswith(\"# sent_id =\") or token[\"form\"].startswith(\"# text = NA\"):\n",
    "                continue\n",
    "            current_sentence.append(token)\n",
    "            if token[\"form\"] == \".\":\n",
    "                new_sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "        if current_sentence and len(current_sentence) > 0:\n",
    "            new_sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    print(\"Number of sentences:\", len(new_sentences))\n",
    "    print(new_sentences)\n",
    "    print(type(new_sentences))\n",
    "\n",
    "    # Save the output to a file\n",
    "    fields = [\"id\", \"form\", \"lemma\", \"upos\", \"xpos\", \"feats\", \"head\", \"deprel\", \"deps\", \"misc\", \"new_field_1\", \"new_field_2\"]\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(conllu.serialize(new_sentences, fields=fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    data.write(sentence.serialize())\n",
    "        data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_sentences(\"../assets/Lemmatization_training_files/plato_iv.conllu\", \"../assets/Lemmatization_training_files/plato_iv_new.conllu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_sentences(\"../assets/Lemmatization_training_files/aeschylus.conllu\", \"../assets/Lemmatization_training_files/aeschylus_new.conllu\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"../assets/Lemmatization_training_files/lemma_train_NFKD\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../assets/Lemmatization_training_files/lemma_train_NFKC\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ʼ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization to NFKD\n",
    "import random\n",
    "import unicodedata\n",
    "def normalize(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "# # load files from folder\n",
    "\n",
    "#input_file = \"../assets/Lemmatization_training_files/lemma_train_NFKD/aristotle_NFKD.spacy\"\n",
    "#train_output_file = \"../assets/Lemmatization_training_files/lemma_train_NFKD/aristotle_NFKD_train.spacy\"\n",
    "#dev_output_file = \"../assets/Lemmatization_training_files/lemma_train_NFKD/aristotle_NFKD_dev.spacy\"\n",
    "\n",
    "for file in os.listdir(\"../assets/Lemmatization_training_files\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\n",
    "            f\"../assets/Lemmatization_training_files/{file}\", \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        for sentence in sentences:\n",
    "            for token in sentence:\n",
    "                if token[\"form\"] in ['', \"_\", '—', '-']:\n",
    "                    #print ('Empty form:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"form\"] = token[\"lemma\"]\n",
    "                    #print(clean_text(sentence))\n",
    "                if token[\"lemma\"] in ['', \"_\", '—', '-']:\n",
    "                    #print ('Empty lemma:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"lemma\"] = \"\"                     \n",
    "                if token[\"id\"] == '':\n",
    "                    print ('Empty id:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"id\"] = \"UNK\"\n",
    "                if token[\"upos\"] == '':\n",
    "                    print ('Empty upos:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"upos\"] = \"\"\n",
    "                if token[\"upos\"] in ['END', 'MID']:\n",
    "                    print ('Wrong upos:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"upos\"] = \"NOUN\"\n",
    "                token[\"form\"] = normalize(token[\"form\"])\n",
    "                token[\"lemma\"] = normalize(token[\"lemma\"])\n",
    "        # Shuffle the sentences randomly\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        # Split the sentences into train and dev data\n",
    "        train_data = sentences[:int(len(sentences) * 0.8)]\n",
    "        dev_data = sentences[int(len(sentences) * 0.8):]\n",
    "        with open(\"../assets/Lemmatization_training_files/lemma_train_NFKD/\" + \"{0}_NFKD_train.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in train_data:\n",
    "                    data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        with open(\"../assets/Lemmatization_training_files/lemma_train_NFKD/\" + \"{0}_NFKD_dev.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in dev_data:\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of connlu file\n",
    "for file in os.listdir(\"../assets/Lemmatization_training_files/lemma_train_NFKD\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        #print(file)\n",
    "        data = open(\n",
    "            f\"../assets/Lemmatization_training_files/lemma_train_NFKD/{file}\", \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        if len(sentences) >= 10:\n",
    "            print(file, \": \", len(sentences), \" sentences\")\n",
    "            # convert conllu to spacy file\n",
    "            !python -m spacy convert \"../assets/Lemmatization_training_files/lemma_train_NFKD/{file}\" ../assets/Lemmatization_training_files/lemma_train_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "        else:\n",
    "            print(file, \": \", len(sentences), \" sentences\")\n",
    "            !python -m spacy convert \"../assets/Lemmatization_training_files/lemma_train_NFKD/{file}\" ../assets/Lemmatization_training_files/lemma_train_NFKD/ -c conllu -m --merge-subtokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of connlu file xenophon_ii_NFKD_train.conllu\n",
    "data = open(\n",
    "            f\"../assets/Lemmatization_training_files/xenophon_ii.conllu\", \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "annotations = data.read()\n",
    "sentences = conllu.parse(annotations)\n",
    "print(len(sentences))\n",
    "print(sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization to NFKC\n",
    "import unicodedata\n",
    "def normalize(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "# # load files from folder\n",
    "\n",
    "#input_file = \"../assets/Lemmatization_training_files/lemma_train_NFKC/aristotle_NFKC.spacy\"\n",
    "train_output_file = \"../assets/Lemmatization_training_files/lemma_train_NFKC/aristotle_NFKC_train.spacy\"\n",
    "dev_output_file = \"../assets/Lemmatization_training_files/lemma_train_NFKC/aristotle_NFKC_dev.spacy\"\n",
    "\n",
    "for file in os.listdir(\"../assets/Lemmatization_training_files\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\n",
    "            f\"../assets/Lemmatization_training_files/{file}\", \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        for sentence in sentences:\n",
    "            for token in sentence:\n",
    "                if token[\"form\"] in ['', \"_\", '—', '-']:\n",
    "                    #print ('Empty form:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"form\"] = token[\"lemma\"]\n",
    "                    #token[\"form\"] = token[\"lemma\"]\n",
    "                    #print(clean_text(sentence))\n",
    "                if token[\"lemma\"] in ['', \"_\", '—', '-']:\n",
    "                    #print ('Empty lemma:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"lemma\"] = \"\"                     \n",
    "                    #token[\"lemma\"] = token[\"form\"]\n",
    "                if token[\"id\"] == '':\n",
    "                    print ('Empty id:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"id\"] = \"UNK\"\n",
    "                if token[\"upos\"] == '':\n",
    "                    print ('Empty upos:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"upos\"] = \"\"\n",
    "                if token[\"upos\"] in ['END', 'MID']:\n",
    "                    print ('Wrong upos:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                    token[\"upos\"] = \"NOUN\"\n",
    "                token[\"form\"] = normalize(token[\"form\"])\n",
    "                token[\"lemma\"] = normalize(token[\"lemma\"])\n",
    "        # Shuffle the sentences randomly\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        # Split the sentences into train and dev data\n",
    "        train_data = sentences[:int(len(sentences) * 0.8)]\n",
    "        dev_data = sentences[int(len(sentences) * 0.8):]\n",
    "        with open(\"../assets/Lemmatization_training_files/lemma_train_NFKC/\" + \"{0}_NFKC_train.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in train_data:\n",
    "                    data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        with open(\"../assets/Lemmatization_training_files/lemma_train_NFKC/\" + \"{0}_NFKC_dev.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in dev_data:\n",
    "                    data.write(sentence.serialize())\n",
    "        data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of connlu file\n",
    "for file in os.listdir(\"../assets/Lemmatization_training_files/lemma_train_NFKC\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        #print(file)\n",
    "        data = open(\n",
    "            f\"../assets/Lemmatization_training_files/lemma_train_NFKC/{file}\", \"r\", encoding=\"utf-8\"\n",
    "        )\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        if len(sentences) >= 10:\n",
    "            print(file, \": \", len(sentences), \" sentences\")\n",
    "            # convert conllu to spacy file\n",
    "            !python -m spacy convert \"../assets/Lemmatization_training_files/lemma_train_NFKC/{file}\" ../assets/Lemmatization_training_files/lemma_train_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "        else:\n",
    "            print(file, \": \", len(sentences), \" sentences\")\n",
    "            !python -m spacy convert \"../assets/Lemmatization_training_files/lemma_train_NFKC/{file}\" ../assets/Lemmatization_training_files/lemma_train_NFKC/ -c conllu -m --merge-subtokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize and convert Perseus dataset\n",
    "\n",
    "import unicodedata\n",
    "def normalize_NFKD(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-Perseus\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-Perseus/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/\" + \"{0}_NFKD.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKD(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKD(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        \n",
    "import unicodedata\n",
    "def normalize_NFKC(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-Perseus\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-Perseus/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/\" + \"{0}_NFKC.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKC(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKC(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Normalize and convert PROEIL dataset\n",
    "\n",
    "import unicodedata\n",
    "def normalize_NFKD(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "# Remove accents from the PROIEL files\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-PROIEL\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-PROIEL/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/\" + \"{0}_NFKD.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKD(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKD(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        \n",
    "\n",
    "\n",
    "import unicodedata\n",
    "def normalize_NFKC(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "# Remove accents from the PROIEL files\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-PROIEL\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-PROIEL/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/\" + \"{0}_NKFC.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKC(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKC(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Universal dependencies tools validate.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert conllu to spacy UD_Ancient_Greek and UD_Ancient_Greek-PROIEL\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_train= DocBin().from_disk('../corpus/train/lemma_train/lemma_train.spacy')\n",
    "lemma_train_docs = list(lemma_train.get_docs(nlp.vocab))\n",
    "\n",
    "PROIEL_NFKD= DocBin().from_disk('../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/grc_proiel-ud-train_NFKD.spacy')\n",
    "PROIEL_NFKD_docs = list(PROIEL_NFKD.get_docs(nlp.vocab))\n",
    "\n",
    "PROIEL_NFKC= DocBin().from_disk('../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/grc_proiel-ud-train_NKFC.spacy')\n",
    "PROIEL_NFKC_docs = list(PROIEL_NFKC.get_docs(nlp.vocab))\n",
    "\n",
    "Perseus_NFKD= DocBin().from_disk('../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/grc_perseus-ud-train_NFKD.spacy')\n",
    "Perseus_NFKD_docs = list(Perseus_NFKD.get_docs(nlp.vocab))\n",
    "\n",
    "Perseus_NFKC= DocBin().from_disk('../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/grc_perseus-ud-train_NFKC.spacy')\n",
    "Perseus_NFKC_docs = list(Perseus_NFKC.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through sentences in lemma_train. If a sentence is in any of the other files, print the sentence and the file it is in\n",
    "for doc in lemma_train_docs:\n",
    "    if doc not in PROIEL_NFKD_docs:\n",
    "        print(\"PROIEL_NFKD\")\n",
    "        print(doc)\n",
    "    if doc in PROIEL_NFKC_docs:\n",
    "        print(\"PROIEL_NFKC\")\n",
    "        print(doc)\n",
    "    if doc in Perseus_NFKD_docs:\n",
    "        print(\"Perseus_NFKD\")\n",
    "        print(doc)\n",
    "    if doc in Perseus_NFKC_docs:\n",
    "        print(\"Perseus_NFKC\")\n",
    "        print(doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check sentence in lemma_train contain \"XLV.\" if so, then print sentence\n",
    "for doc in lemma_train_docs:\n",
    "    if \"XLV.\" in doc.text:\n",
    "        print(doc.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
