{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import conllu\n",
    "#from conllu import parse\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"grc_proiel_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Creating blank nlp object for language 'grc'\u001b[0m\n",
      "[2023-05-03 10:33:32,020] [INFO] Reading vectors from ../assets/grc_floret_cbow_nn2_xn10_b200k_dim300.floret\n",
      "200000it [00:07, 28266.74it/s]\n",
      "[2023-05-03 10:33:39,098] [INFO] Loaded vectors from ../assets/grc_floret_cbow_nn2_xn10_b200k_dim300.floret\n",
      "\u001b[38;5;2m✔ Successfully converted 200000 vectors\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
      "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
      "/root/Projects/Atlomy/git/greCy_ATLOMY/vectors/large\n"
     ]
    }
   ],
   "source": [
    "# if using vectors\n",
    "!python -m spacy init vectors grc ../assets/grc_floret_cbow_nn2_xn10_b200k_dim300.floret ../vectors/large --mode floret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: ../training/senter/large\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../training/senter/large\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-05-03 11:03:36,280] [INFO] Set up nlp object from config\n",
      "[2023-05-03 11:03:36,288] [INFO] Pipeline: ['senter']\n",
      "[2023-05-03 11:03:36,290] [INFO] Created vocabulary\n",
      "[2023-05-03 11:03:36,685] [INFO] Added vectors: ../vectors/large\n",
      "[2023-05-03 11:03:37,079] [INFO] Finished initializing nlp object\n",
      "[2023-05-03 11:03:39,483] [INFO] Initialized pipeline components: ['senter']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['senter']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS SENTER  SENTS_F  SCORE \n",
      "---  ------  -----------  -------  ------\n",
      "  0       0        73.00     0.20    0.00\n",
      "  0     200      2920.38    93.06    0.93\n",
      "  0     400       389.75    93.99    0.94\n",
      "  0     600       281.08    95.59    0.96\n",
      "  0     800       177.70    96.01    0.96\n",
      "  0    1000       220.88    97.20    0.97\n",
      "  1    1200       251.47    96.27    0.96\n",
      "  1    1400       302.10    97.06    0.97\n",
      "  1    1600       345.58    96.96    0.97\n",
      "  2    1800       380.08    96.87    0.97\n",
      "  2    2000       470.41    97.37    0.97\n",
      "  3    2200       491.41    98.01    0.98\n",
      "  4    2400       603.38    96.73    0.97\n",
      "  5    2600       641.61    96.73    0.97\n",
      "  6    2800       595.79    97.00    0.97\n",
      "  7    3000       548.48    97.28    0.97\n",
      "  8    3200       467.64    96.78    0.97\n",
      "  9    3400       441.20    96.59    0.97\n",
      " 10    3600       426.27    96.54    0.97\n",
      " 12    3800       384.77    97.09    0.97\n",
      " 13    4000       383.25    97.45    0.97\n",
      " 14    4200       379.28    97.04    0.97\n",
      " 15    4400       333.28    97.77    0.98\n",
      " 16    4600       357.88    97.28    0.97\n",
      " 17    4800       332.93    97.45    0.97\n",
      " 18    5000       336.05    97.64    0.98\n",
      " 19    5200       322.30    97.82    0.98\n",
      " 20    5400       323.60    97.64    0.98\n",
      " 22    5600       325.39    97.64    0.98\n",
      " 23    5800       282.61    97.59    0.98\n",
      " 24    6000       336.42    97.82    0.98\n",
      " 25    6200       307.07    97.81    0.98\n",
      " 26    6400       277.62    97.86    0.98\n",
      " 27    6600       292.31    97.55    0.98\n",
      " 28    6800       305.11    97.32    0.97\n",
      " 29    7000       307.50    97.55    0.98\n",
      " 31    7200       280.26    97.55    0.98\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../training/senter/large/model-last\n"
     ]
    }
   ],
   "source": [
    "# if making senter\n",
    "!python -m spacy train ../configs/senter_vec.cfg --output ../training/senter/large --gpu-id 0 --paths.train ../corpus/train/grc_perseus-ud-train.spacy --paths.dev ../corpus/dev/grc_perseus-ud-dev.spacy --paths.vectors ../vectors/large --nlp.lang=grc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: ../training/lemmatizer/large\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-05-03 11:10:53,265] [INFO] Set up nlp object from config\n",
      "[2023-05-03 11:10:53,272] [INFO] Pipeline: ['lemmatizer']\n",
      "[2023-05-03 11:10:53,275] [INFO] Created vocabulary\n",
      "[2023-05-03 11:10:53,669] [INFO] Added vectors: ../vectors/large\n",
      "[2023-05-03 11:10:54,064] [INFO] Finished initializing nlp object\n",
      "[2023-05-03 11:11:07,586] [INFO] Initialized pipeline components: ['lemmatizer']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['lemmatizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS LEMMA...  LEMMA_ACC  SCORE \n",
      "---  ------  -------------  ---------  ------\n",
      "  0       0         141.00      25.77    0.26\n",
      "  0     200       30759.78      60.75    0.61\n",
      "  0     400       24842.97      70.83    0.71\n",
      "  0     600       16802.02      74.58    0.75\n",
      "  0     800       16110.00      77.47    0.77\n",
      "  0    1000       15897.04      78.95    0.79\n",
      "  0    1200       17209.46      80.81    0.81\n",
      "  0    1400       17122.92      82.17    0.82\n",
      "  0    1600       17226.29      83.68    0.84\n",
      "  0    1800       18399.53      84.49    0.84\n",
      "  0    2000       20841.12      85.54    0.86\n",
      "  0    2200       18135.25      86.15    0.86\n",
      "  0    2400       21816.25      87.17    0.87\n",
      "  1    2600       23689.42      87.84    0.88\n",
      "  1    2800       21986.85      88.57    0.89\n",
      "  1    3000       28863.86      89.28    0.89\n",
      "  1    3200       25136.46      89.58    0.90\n",
      "  1    3400       24663.45      89.97    0.90\n",
      "  1    3600       21900.54      90.11    0.90\n",
      "  2    3800       18520.66      90.35    0.90\n",
      "  2    4000       19520.21      90.61    0.91\n",
      "  2    4200       19511.09      90.85    0.91\n",
      "  2    4400       19868.68      91.01    0.91\n",
      "  2    4600       18714.45      91.14    0.91\n",
      "  3    4800       13542.58      91.04    0.91\n",
      "  3    5000       14715.03      90.41    0.90\n",
      "  3    5200       14024.77      91.43    0.91\n",
      "  3    5400       14409.05      91.42    0.91\n",
      "  3    5600       14867.05      91.62    0.92\n",
      "  4    5800       11227.54      91.43    0.91\n",
      "  4    6000       11275.17      91.52    0.92\n",
      "  4    6200       12087.04      91.52    0.92\n",
      "  4    6400       12218.37      91.56    0.92\n",
      "  4    6600       12248.00      90.98    0.91\n",
      "  5    6800       10163.33      91.69    0.92\n",
      "  5    7000        9477.41      91.64    0.92\n",
      "  5    7200       10489.32      91.74    0.92\n",
      "  5    7400       10525.39      91.80    0.92\n",
      "  5    7600       10758.91      91.84    0.92\n",
      "  6    7800        8678.86      91.83    0.92\n",
      "  6    8000        8572.27      91.92    0.92\n",
      "  6    8200       10047.53      91.82    0.92\n",
      "  6    8400        9439.62      91.94    0.92\n",
      "  7    8600       10194.24      91.92    0.92\n",
      "  7    8800        7166.61      91.98    0.92\n",
      "  7    9000        8692.72      91.97    0.92\n",
      "  7    9200        9738.97      91.44    0.91\n",
      "  7    9400        8096.05      92.05    0.92\n",
      "  8    9600        8811.43      92.06    0.92\n",
      "  8    9800        7380.95      92.00    0.92\n",
      "  8   10000        7372.79      91.84    0.92\n",
      "  8   10200        7517.02      91.98    0.92\n",
      "  8   10400        7958.65      92.05    0.92\n",
      "  9   10600        9475.53      92.07    0.92\n",
      "  9   10800        6595.90      92.16    0.92\n",
      "  9   11000        7533.98      92.14    0.92\n",
      "  9   11200        7217.16      92.16    0.92\n",
      "  9   11400        7755.37      92.06    0.92\n",
      " 10   11600        7904.40      92.08    0.92\n",
      " 10   11800        6733.99      92.16    0.92\n",
      " 10   12000        6429.15      92.07    0.92\n",
      " 10   12200        7277.17      92.09    0.92\n",
      " 10   12400        7189.16      92.19    0.92\n",
      " 11   12600        7290.74      92.18    0.92\n",
      " 11   12800        5701.21      92.16    0.92\n",
      " 11   13000        6740.56      92.22    0.92\n",
      " 11   13200        7389.87      92.14    0.92\n",
      " 11   13400        7015.96      92.25    0.92\n",
      " 12   13600        6388.52      92.22    0.92\n",
      " 12   13800        5695.25      92.17    0.92\n",
      " 12   14000        6344.17      92.17    0.92\n",
      " 12   14200        6586.91      92.20    0.92\n",
      " 12   14400        6943.35      92.21    0.92\n",
      " 13   14600        7188.91      91.45    0.91\n",
      " 13   14800        5374.41      92.20    0.92\n",
      " 13   15000        5996.98      92.24    0.92\n",
      " 13   15200        6049.47      92.30    0.92\n",
      " 13   15400        5908.52      92.31    0.92\n",
      " 14   15600        6397.61      92.25    0.92\n",
      " 14   15800        5966.48      92.29    0.92\n",
      " 14   16000        6004.54      92.23    0.92\n",
      " 14   16200        6050.86      92.39    0.92\n",
      " 14   16400        6345.48      92.42    0.92\n",
      " 15   16600        5348.58      92.31    0.92\n",
      " 15   16800        5319.04      92.28    0.92\n",
      " 15   17000        5226.01      92.29    0.92\n",
      " 15   17200        5772.00      92.29    0.92\n",
      " 15   17400        6835.11      92.34    0.92\n",
      " 16   17600        5479.85      92.41    0.92\n",
      " 16   17800        4672.63      92.36    0.92\n",
      " 16   18000        5220.78      92.41    0.92\n",
      " 16   18200        5819.22      92.31    0.92\n",
      " 16   18400        5543.76      92.39    0.92\n",
      " 17   18600        6223.61      92.38    0.92\n",
      " 17   18800        5379.97      92.37    0.92\n",
      " 17   19000        4636.57      92.40    0.92\n",
      " 17   19200        5534.05      92.45    0.92\n",
      " 17   19400        5381.72      92.48    0.92\n",
      " 18   19600        5388.21      92.39    0.92\n",
      " 18   19800        5341.06      92.37    0.92\n",
      " 18   20000        5487.80      92.39    0.92\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../training/lemmatizer/large/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ../configs/lemmatizer_vec.cfg --output ../training/lemmatizer/large --gpu-id 0 --paths.train ../corpus/train/lemma_train --paths.dev ../corpus/dev/lemma_dev --paths.vectors ../vectors/large --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory:\n",
      "../training/transformer/proiel/assembled\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "../training/transformer/proiel/assembled\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-05-03 12:32:35,402] [INFO] Set up nlp object from config\n",
      "[2023-05-03 12:32:35,411] [INFO] Pipeline: ['transformer', 'morphologizer', 'tagger', 'parser', 'senter', 'lemmatizer', 'attribute_ruler']\n",
      "[2023-05-03 12:32:35,416] [INFO] Created vocabulary\n",
      "[2023-05-03 12:32:35,770] [INFO] Added vectors: ../vectors/large\n",
      "[2023-05-03 12:32:36,019] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at Jacobo/aristoBERTo were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-05-03 12:32:47,709] [INFO] Initialized pipeline components: ['transformer', 'morphologizer', 'tagger', 'parser', 'attribute_ruler']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'morphologizer', 'tagger', 'parser',\n",
      "'senter', 'lemmatizer', 'attribute_ruler']\u001b[0m\n",
      "\u001b[38;5;4mℹ Frozen components: ['lemmatizer', 'senter']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS MORPH...  LOSS TAGGER  LOSS PARSER  POS_ACC  MORPH_ACC  TAG_ACC  DEP_UAS  DEP_LAS  LEMMA_ACC  SCORE \n",
      "---  ------  -------------  -------------  -----------  -----------  -------  ---------  -------  -------  -------  ---------  ------\n",
      "  0       0        2898.38        1369.67      1369.59      2168.33     4.53       2.28     0.00    15.63     5.17      95.58    0.59\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ../configs/transformer.cfg --output ../training/transformer/proiel/assembled --gpu-id 0 --paths.train ../corpus/train/grc_proiel-ud-train.spacy --paths.dev ../corpus/dev/grc_proiel-ud-dev.spacy --nlp.lang=grc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove accents definition\n",
    "import unicodedata\n",
    "def clean_text(text: str) -> str:\n",
    "    #Cleans the given text by stripping accents and lowercasing.\n",
    "    try:\n",
    "        non_accent_characters = [\n",
    "        char for char in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(char) != 'Mn'\n",
    "        or char == '̓'  # Greek coronis\n",
    "        ]\n",
    "    # str.lower() works for unicode characters\n",
    "        return ''.join(non_accent_characters).lower()\n",
    "    except TypeError:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xenophon_i_train.conllu\n",
      "herodotus_train.conllu\n",
      "galen_dev.conllu\n",
      "aeschylus_i_train.conllu\n",
      "plato_iii_train.conllu\n",
      "plato_iii_dev.conllu\n",
      "xenophon_ii_train.conllu\n",
      "galen_train.conllu\n",
      "aeschylus_ii_train.conllu\n",
      "euripides_i_train.conllu\n",
      "aeschylus_i_dev.conllu\n",
      "xenophon_ii_dev_spaced.conllu\n",
      "plato_ii_train_spaced.conllu\n",
      "Empty form:  > 11 PUNCT TokenList<ΘΕΟ., τὰ, ποῖα, δή, ;, ΣΩ., \"\", #, Σωκράτης, \"\", , ΣΩ., σοφιστήν, ,, πολιτικόν, ,, φιλόσοφον, ., ΘΕΟ., τί, δὲ, μάλιστα, καὶ, τὸ, ποῖόν, τι, περὶ, αὐτῶν, διαπορηθεὶς, ἐρέσθαι, διενοήθης, ;, ΣΩ., τόδε, ·, πότερον, ἓν, πάντα, ταῦτα, ἐνόμιζον, ἢ, δύο, ,, ἢ, καθάπερ, τὰ, ὀνόματα, τρία, ,, τρία, καὶ, τὰ, γένη, διαιρούμενοι, καθʼ, ἓν, ὄνομα, γένος, ἑκάστῳ, προσῆπτον, ;, ΘΕΟ., ἀλλʼ, οὐδείς, ,, ὡς, ἐγᾦμαι, ,, φθόνος, αὐτῷ, διελθεῖν, αὐτά, ·, ἢ, πῶς, ,, ὦ, ξένε, ,, λέγωμεν, ;, ΞΕ., #, Ξένος, \"\", , ΞΕ., οὕτως, ,, ὦ, Θεόδωρε, ., φθόνος, μὲν, γὰρ, οὐδεὶς, οὐδὲ, χαλεπὸν, εἰπεῖν, ὅτι, γε, τρίʼ, ἡγοῦντο, ·, καθʼ, ἕκαστον, μὴν, διορίσασθαι, σαφῶς, τί, ποτʼ, ἔστιν, ,, οὐ, σμικρὸν, οὐδὲ, ῥᾴδιον, ἔργον, ., ΘΕΟ., καὶ, μὲν, δὴ, κατὰ, τύχην, γε, ,, ὦ, Σώκρατες, ,, λόγων, ἐπελάβου, παραπλησίων, ὧν, καὶ, πρὶν, ἡμᾶς, δεῦρʼ, ἐλθεῖν, διερωτῶντες, αὐτὸν, ἐτυγχάνομεν, ,, ὁ, δὲ, ταὐτὰ, ἅπερ, πρὸς, σὲ, νῦν, καὶ, τότε, ἐσκήπτετο, πρὸς, ἡμᾶς, ·, ἐπεὶ, διακηκοέναι, γέ, φησιν, ἱκανῶς, καὶ, οὐκ, ἀμνημονεῖν, ., ΣΩ., μὴ, τοίνυν, ,, ὦ, ξένε, ,, ἡμῶν, τήν, γε, πρώτην, αἰτησάντων, χάριν, ἀπαρνηθεὶς, γένῃ, ,, τοσόνδε, δʼ, ἡμῖν, φράζε, ., πότερον, εἴωθας, ἥδιον, αὐτὸς, ἐπὶ, σαυτοῦ, μακρῷ, λόγῳ, διεξιέναι, λέγων, τοῦτο, ὃ, ἂν, ἐνδείξασθαί, τῳ, βουληθῇς, ,, ἢ, διʼ, ἐρωτήσεων, ,, οἷόν, ποτε, καὶ, Παρμενίδῃ, χρωμένῳ, καὶ, διεξιόντι, λόγους, παγκάλους, παρεγενόμην, ἐγὼ, νέος, ὤν, ,, ἐκείνου, μάλα, δὴ, τότε, ὄντος, πρεσβύτου, ;, ΞΕ., τῷ, μέν, ,, ὦ, Σώκρατες, ,, ἀλύπως, τε, καὶ, εὐηνίως, προσδιαλεγομένῳ, ῥᾷον, οὕτω, ,, τὸ, πρὸς, ἄλλον, ·, εἰ, δὲ, μή, ,, τὸ, καθʼ, αὑτόν, ., ΣΩ., ἔξεστι, τοίνυν, τῶν, παρόντων, ὃν, ἂν, βουληθῇς, ἐκλέξασθαι, ,, πάντες, γὰρ, ὑπακούσονταί, σοι, πρᾴως, ·, συμβούλῳ, μὴν, ἐμοὶ, χρώμενος, τῶν, νέων, τινὰ, αἱρήσῃ, ,, Θεαίτητον, τόνδε, ,, ἢ, καὶ, τῶν, ἄλλων, εἴ, τίς, σοι, κατὰ, νοῦν, ., ΞΕ., ὦ, Σώκρατες, ,, αἰδώς, τίς, μʼ, ἔχει, τὸ, νῦν, πρῶτον, συγγενόμενον, ὑμῖν, μὴ, κατὰ, σμικρὸν, ἔπος, πρὸς, ἔπος, ποιεῖσθαι, τὴν, συνουσίαν, ,, ἀλλʼ, ἐκτείναντα, ἀπομηκύνειν, λόγον, συχνὸν, κατʼ, ἐμαυτόν, ,, εἴτε, καὶ, πρὸς, ἕτερον, ,, οἷον, ἐπίδειξιν, ποιούμενον, ·, τῷ, γὰρ, ὄντι, τὸ, νῦν, ῥηθὲν, οὐχ, ὅσον, ὧδε, ἐρωτηθὲν, ἐλπίσειεν, ἂν, αὐτὸ, εἶναί, τις, ,, ἀλλὰ, τυγχάνει, λόγου, παμμήκους, ὄν, ., τὸ, δὲ, αὖ, σοὶ, μὴ, χαρίζεσθαι, καὶ, τοῖσδε, ,, ἄλλως, τε, καὶ, σοῦ, λέξαντος, ὡς, εἶπες, ,, ἄξενόν, τι, καταφαίνεταί, μοι, καὶ, ἄγριον, ., ἐπεὶ, Θεαίτητόν, γε, τὸν, προσδιαλεγόμενον, εἶναι, δέχομαι, παντάπασιν, ἐξ, ὧν, αὐτός, τε, πρότερον, διείλεγμαι, καὶ, σὺ, τὰ, νῦν, μοι, διακελεύῃ, .>\n",
      "Empty form:  > 86 PUNCT TokenList<ΘΕΟ., τὰ, ποῖα, δή, ;, ΣΩ., \"\", #, Σωκράτης, \"\", >, ΣΩ., σοφιστήν, ,, πολιτικόν, ,, φιλόσοφον, ., ΘΕΟ., τί, δὲ, μάλιστα, καὶ, τὸ, ποῖόν, τι, περὶ, αὐτῶν, διαπορηθεὶς, ἐρέσθαι, διενοήθης, ;, ΣΩ., τόδε, ·, πότερον, ἓν, πάντα, ταῦτα, ἐνόμιζον, ἢ, δύο, ,, ἢ, καθάπερ, τὰ, ὀνόματα, τρία, ,, τρία, καὶ, τὰ, γένη, διαιρούμενοι, καθʼ, ἓν, ὄνομα, γένος, ἑκάστῳ, προσῆπτον, ;, ΘΕΟ., ἀλλʼ, οὐδείς, ,, ὡς, ἐγᾦμαι, ,, φθόνος, αὐτῷ, διελθεῖν, αὐτά, ·, ἢ, πῶς, ,, ὦ, ξένε, ,, λέγωμεν, ;, ΞΕ., #, Ξένος, \"\", , ΞΕ., οὕτως, ,, ὦ, Θεόδωρε, ., φθόνος, μὲν, γὰρ, οὐδεὶς, οὐδὲ, χαλεπὸν, εἰπεῖν, ὅτι, γε, τρίʼ, ἡγοῦντο, ·, καθʼ, ἕκαστον, μὴν, διορίσασθαι, σαφῶς, τί, ποτʼ, ἔστιν, ,, οὐ, σμικρὸν, οὐδὲ, ῥᾴδιον, ἔργον, ., ΘΕΟ., καὶ, μὲν, δὴ, κατὰ, τύχην, γε, ,, ὦ, Σώκρατες, ,, λόγων, ἐπελάβου, παραπλησίων, ὧν, καὶ, πρὶν, ἡμᾶς, δεῦρʼ, ἐλθεῖν, διερωτῶντες, αὐτὸν, ἐτυγχάνομεν, ,, ὁ, δὲ, ταὐτὰ, ἅπερ, πρὸς, σὲ, νῦν, καὶ, τότε, ἐσκήπτετο, πρὸς, ἡμᾶς, ·, ἐπεὶ, διακηκοέναι, γέ, φησιν, ἱκανῶς, καὶ, οὐκ, ἀμνημονεῖν, ., ΣΩ., μὴ, τοίνυν, ,, ὦ, ξένε, ,, ἡμῶν, τήν, γε, πρώτην, αἰτησάντων, χάριν, ἀπαρνηθεὶς, γένῃ, ,, τοσόνδε, δʼ, ἡμῖν, φράζε, ., πότερον, εἴωθας, ἥδιον, αὐτὸς, ἐπὶ, σαυτοῦ, μακρῷ, λόγῳ, διεξιέναι, λέγων, τοῦτο, ὃ, ἂν, ἐνδείξασθαί, τῳ, βουληθῇς, ,, ἢ, διʼ, ἐρωτήσεων, ,, οἷόν, ποτε, καὶ, Παρμενίδῃ, χρωμένῳ, καὶ, διεξιόντι, λόγους, παγκάλους, παρεγενόμην, ἐγὼ, νέος, ὤν, ,, ἐκείνου, μάλα, δὴ, τότε, ὄντος, πρεσβύτου, ;, ΞΕ., τῷ, μέν, ,, ὦ, Σώκρατες, ,, ἀλύπως, τε, καὶ, εὐηνίως, προσδιαλεγομένῳ, ῥᾷον, οὕτω, ,, τὸ, πρὸς, ἄλλον, ·, εἰ, δὲ, μή, ,, τὸ, καθʼ, αὑτόν, ., ΣΩ., ἔξεστι, τοίνυν, τῶν, παρόντων, ὃν, ἂν, βουληθῇς, ἐκλέξασθαι, ,, πάντες, γὰρ, ὑπακούσονταί, σοι, πρᾴως, ·, συμβούλῳ, μὴν, ἐμοὶ, χρώμενος, τῶν, νέων, τινὰ, αἱρήσῃ, ,, Θεαίτητον, τόνδε, ,, ἢ, καὶ, τῶν, ἄλλων, εἴ, τίς, σοι, κατὰ, νοῦν, ., ΞΕ., ὦ, Σώκρατες, ,, αἰδώς, τίς, μʼ, ἔχει, τὸ, νῦν, πρῶτον, συγγενόμενον, ὑμῖν, μὴ, κατὰ, σμικρὸν, ἔπος, πρὸς, ἔπος, ποιεῖσθαι, τὴν, συνουσίαν, ,, ἀλλʼ, ἐκτείναντα, ἀπομηκύνειν, λόγον, συχνὸν, κατʼ, ἐμαυτόν, ,, εἴτε, καὶ, πρὸς, ἕτερον, ,, οἷον, ἐπίδειξιν, ποιούμενον, ·, τῷ, γὰρ, ὄντι, τὸ, νῦν, ῥηθὲν, οὐχ, ὅσον, ὧδε, ἐρωτηθὲν, ἐλπίσειεν, ἂν, αὐτὸ, εἶναί, τις, ,, ἀλλὰ, τυγχάνει, λόγου, παμμήκους, ὄν, ., τὸ, δὲ, αὖ, σοὶ, μὴ, χαρίζεσθαι, καὶ, τοῖσδε, ,, ἄλλως, τε, καὶ, σοῦ, λέξαντος, ὡς, εἶπες, ,, ἄξενόν, τι, καταφαίνεταί, μοι, καὶ, ἄγριον, ., ἐπεὶ, Θεαίτητόν, γε, τὸν, προσδιαλεγόμενον, εἶναι, δέχομαι, παντάπασιν, ἐξ, ὧν, αὐτός, τε, πρότερον, διείλεγμαι, καὶ, σὺ, τὰ, νῦν, μοι, διακελεύῃ, .>\n",
      "Empty form:  > 130 PUNCT TokenList<ΞΕ., ἔχει, τοίνυν, καὶ, μάλα, συχνήν, ., ὁ, μὲν, γὰρ, εὐήθης, αὐτῶν, ἐστιν, ,, οἰόμενος, εἰδέναι, ταῦτα, ἃ, δοξάζει, ·, τὸ, δὲ, θατέρου, σχῆμα, διὰ, τὴν, ἐν, τοῖς, λόγοις, κυλίνδησιν, ἔχει, πολλὴν, ὑποψίαν, καὶ, φόβον, ὡς, ἀγνοεῖ, ταῦτα, ἃ, πρὸς, τοὺς, ἄλλους, ὡς, εἰδὼς, ἐσχημάτισται, ., ΘΕΑΙ., πάνυ, μὲν, οὖν, ἔστιν, ἑκατέρου, γένος, ὧν, εἴρηκας, ., ΞΕ., οὐκοῦν, τὸν, μὲν, ἁπλοῦν, μιμητήν, τινα, ,, τὸν, δὲ, εἰρωνικὸν, μιμητὴν, θήσομεν, ;, ΘΕΑΙ., εἰκὸς, γοῦν, ., ΞΕ., τούτου, δʼ, αὖ, τὸ, γένος, ἓν, ἢ, δύο, φῶμεν, ;, ΘΕΑΙ., ὅρα, σύ, ., ΞΕ., σκοπῶ, ,, καί, μοι, διττὼ, καταφαίνεσθόν, τινε, ·, τὸν, μὲν, δημοσίᾳ, τε, καὶ, μακροῖς, λόγοις, πρὸς, πλήθη, δυνατὸν, εἰρωνεύεσθαι, καθορῶ, ,, τὸν, δὲ, ἰδίᾳ, τε, καὶ, βραχέσι, λόγοις, ἀναγκάζοντα, τὸν, προσδιαλεγόμενον, ἐναντιολογεῖν, αὐτὸν, αὑτῷ, ., ΘΕΑΙ., #, Θεαίτητος, \"\", , ΘΕΑΙ., λέγεις, ὀρθότατα, ., ΞΕ., τίνα, οὖν, ἀποφαινώμεθα, τὸν, μακρολογώτερον, εἶναι, ;, πότερα, πολιτικὸν, ἢ, δημολογικόν, ;, ΘΕΑΙ., δημολογικόν, ., ΞΕ., τί, δὲ, τὸν, ἕτερον, ἐροῦμεν, ;, σοφὸν, ἢ, σοφιστικόν, ;, ΘΕΑΙ., τὸ, μέν, που, σοφὸν, ἀδύνατον, ,, ἐπείπερ, οὐκ, εἰδότα, αὐτὸν, ἔθεμεν, ·, μιμητὴς, δʼ, ὢν, τοῦ, σοφοῦ, δῆλον, ὅτι, παρωνύμιον, αὐτοῦ, τι, λήψεται, ,, καὶ, σχεδὸν, ἤδη, μεμάθηκα, ὅτι, τοῦτον, δεῖ, προσειπεῖν, ἀληθῶς, αὐτὸν, ἐκεῖνον, τὸν, παντάπασιν, ὄντως, σοφιστήν, ., ΞΕ., οὐκοῦν, συνδήσομεν, αὐτοῦ, ,, καθάπερ, ἔμπροσθεν, ,, τοὔνομα, συμπλέξαντες, ἀπὸ, τελευτῆς, ἐπʼ, ἀρχήν, ;, ΘΕΑΙ., πάνυ, μὲν, οὖν, ., ΞΕ., τὸ, δὴ, τῆς, ἐναντιοποιολογικῆς, εἰρωνικοῦ, μέρους, τῆς, δοξαστικῆς, μιμητικόν, ,, τοῦ, φανταστικοῦ, γένους, ἀπὸ, τῆς, εἰδωλοποιικῆς, οὐ, θεῖον, ἀλλʼ, ἀνθρωπικὸν, τῆς, ποιήσεως, ἀφωρισμένον, ἐν, λόγοις, τὸ, θαυματοποιικὸν, μόριον, ,, ταύτης, τῆς, γενεᾶς, τε, καὶ, αἵματος, ὃς, ἂν, φῇ, τὸν, ὄντως, σοφιστὴν, εἶναι, ,, τἀληθέστατα, ,, ὡς, ἔοικεν, ,, ἐρεῖ, ., ΘΕΑΙ., παντάπασι, μὲν, οὖν, .>\n",
      "herodotus_dev.conllu\n",
      "plato_ii_dev_spaced.conllu\n",
      "aeschylus_ii_dev.conllu\n",
      "euripides_i_dev.conllu\n",
      "xenophon_i_dev.conllu\n"
     ]
    }
   ],
   "source": [
    "# Normalization to NFKD\n",
    "import unicodedata\n",
    "def normalize(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "# # load files from folder\n",
    "for file in os.listdir(\"../assets/Lemmatization_training_files\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/Lemmatization_training_files/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/Lemmatization_training_files/lemma_train_NFKD/\" + \"{0}_NFKD.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    if token[\"form\"] == '':\n",
    "                        print ('Empty form:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"form\"] = token[\"lemma\"]\n",
    "                    #print(clean_text(sentence))\n",
    "                    if token[\"lemma\"] == '':\n",
    "                        print ('Empty lemma:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"lemma\"] = token[\"form\"]\n",
    "                    if token[\"id\"] == '':\n",
    "                        print ('Empty id:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"id\"] = 'UNK'\n",
    "                    if token[\"upos\"] == '':\n",
    "                        print ('Empty upos:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"upos\"] = 'UNK'\n",
    "                    token[\"form\"] = normalize(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (49 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/aeschylus_i_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (185 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/aeschylus_i_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (33 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/aeschylus_ii_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (136 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/aeschylus_ii_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (31 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/euripides_i_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (115 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/euripides_i_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/galen_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (9 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/galen_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (80 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/herodotus_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (355 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/herodotus_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/plato_ii_dev_spaced_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (22 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/plato_ii_train_spaced_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (28 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/plato_iii_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (109 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/plato_iii_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (44 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/xenophon_i_dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (196 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/xenophon_i_train_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (31 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/xenophon_ii_dev_spaced_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (118 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKD/xenophon_ii_train_NFKD.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# convert conllu to spacy Lemmatization_training_files with NFKD normalization\n",
    "!python -m spacy convert ../assets/Lemmatization_training_files/lemma_train_NFKD/ ../assets/Lemmatization_training_files/lemma_train_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xenophon_i_train.conllu\n",
      "herodotus_train.conllu\n",
      "galen_dev.conllu\n",
      "aeschylus_i_train.conllu\n",
      "plato_iii_train.conllu\n",
      "plato_iii_dev.conllu\n",
      "xenophon_ii_train.conllu\n",
      "galen_train.conllu\n",
      "aeschylus_ii_train.conllu\n",
      "euripides_i_train.conllu\n",
      "aeschylus_i_dev.conllu\n",
      "xenophon_ii_dev_spaced.conllu\n",
      "plato_ii_train_spaced.conllu\n",
      "Empty form:  > 11 PUNCT TokenList<ΘΕΟ., τὰ, ποῖα, δή, ;, ΣΩ., \"\", #, Σωκράτης, \"\", , ΣΩ., σοφιστήν, ,, πολιτικόν, ,, φιλόσοφον, ., ΘΕΟ., τί, δὲ, μάλιστα, καὶ, τὸ, ποῖόν, τι, περὶ, αὐτῶν, διαπορηθεὶς, ἐρέσθαι, διενοήθης, ;, ΣΩ., τόδε, ·, πότερον, ἓν, πάντα, ταῦτα, ἐνόμιζον, ἢ, δύο, ,, ἢ, καθάπερ, τὰ, ὀνόματα, τρία, ,, τρία, καὶ, τὰ, γένη, διαιρούμενοι, καθʼ, ἓν, ὄνομα, γένος, ἑκάστῳ, προσῆπτον, ;, ΘΕΟ., ἀλλʼ, οὐδείς, ,, ὡς, ἐγᾦμαι, ,, φθόνος, αὐτῷ, διελθεῖν, αὐτά, ·, ἢ, πῶς, ,, ὦ, ξένε, ,, λέγωμεν, ;, ΞΕ., #, Ξένος, \"\", , ΞΕ., οὕτως, ,, ὦ, Θεόδωρε, ., φθόνος, μὲν, γὰρ, οὐδεὶς, οὐδὲ, χαλεπὸν, εἰπεῖν, ὅτι, γε, τρίʼ, ἡγοῦντο, ·, καθʼ, ἕκαστον, μὴν, διορίσασθαι, σαφῶς, τί, ποτʼ, ἔστιν, ,, οὐ, σμικρὸν, οὐδὲ, ῥᾴδιον, ἔργον, ., ΘΕΟ., καὶ, μὲν, δὴ, κατὰ, τύχην, γε, ,, ὦ, Σώκρατες, ,, λόγων, ἐπελάβου, παραπλησίων, ὧν, καὶ, πρὶν, ἡμᾶς, δεῦρʼ, ἐλθεῖν, διερωτῶντες, αὐτὸν, ἐτυγχάνομεν, ,, ὁ, δὲ, ταὐτὰ, ἅπερ, πρὸς, σὲ, νῦν, καὶ, τότε, ἐσκήπτετο, πρὸς, ἡμᾶς, ·, ἐπεὶ, διακηκοέναι, γέ, φησιν, ἱκανῶς, καὶ, οὐκ, ἀμνημονεῖν, ., ΣΩ., μὴ, τοίνυν, ,, ὦ, ξένε, ,, ἡμῶν, τήν, γε, πρώτην, αἰτησάντων, χάριν, ἀπαρνηθεὶς, γένῃ, ,, τοσόνδε, δʼ, ἡμῖν, φράζε, ., πότερον, εἴωθας, ἥδιον, αὐτὸς, ἐπὶ, σαυτοῦ, μακρῷ, λόγῳ, διεξιέναι, λέγων, τοῦτο, ὃ, ἂν, ἐνδείξασθαί, τῳ, βουληθῇς, ,, ἢ, διʼ, ἐρωτήσεων, ,, οἷόν, ποτε, καὶ, Παρμενίδῃ, χρωμένῳ, καὶ, διεξιόντι, λόγους, παγκάλους, παρεγενόμην, ἐγὼ, νέος, ὤν, ,, ἐκείνου, μάλα, δὴ, τότε, ὄντος, πρεσβύτου, ;, ΞΕ., τῷ, μέν, ,, ὦ, Σώκρατες, ,, ἀλύπως, τε, καὶ, εὐηνίως, προσδιαλεγομένῳ, ῥᾷον, οὕτω, ,, τὸ, πρὸς, ἄλλον, ·, εἰ, δὲ, μή, ,, τὸ, καθʼ, αὑτόν, ., ΣΩ., ἔξεστι, τοίνυν, τῶν, παρόντων, ὃν, ἂν, βουληθῇς, ἐκλέξασθαι, ,, πάντες, γὰρ, ὑπακούσονταί, σοι, πρᾴως, ·, συμβούλῳ, μὴν, ἐμοὶ, χρώμενος, τῶν, νέων, τινὰ, αἱρήσῃ, ,, Θεαίτητον, τόνδε, ,, ἢ, καὶ, τῶν, ἄλλων, εἴ, τίς, σοι, κατὰ, νοῦν, ., ΞΕ., ὦ, Σώκρατες, ,, αἰδώς, τίς, μʼ, ἔχει, τὸ, νῦν, πρῶτον, συγγενόμενον, ὑμῖν, μὴ, κατὰ, σμικρὸν, ἔπος, πρὸς, ἔπος, ποιεῖσθαι, τὴν, συνουσίαν, ,, ἀλλʼ, ἐκτείναντα, ἀπομηκύνειν, λόγον, συχνὸν, κατʼ, ἐμαυτόν, ,, εἴτε, καὶ, πρὸς, ἕτερον, ,, οἷον, ἐπίδειξιν, ποιούμενον, ·, τῷ, γὰρ, ὄντι, τὸ, νῦν, ῥηθὲν, οὐχ, ὅσον, ὧδε, ἐρωτηθὲν, ἐλπίσειεν, ἂν, αὐτὸ, εἶναί, τις, ,, ἀλλὰ, τυγχάνει, λόγου, παμμήκους, ὄν, ., τὸ, δὲ, αὖ, σοὶ, μὴ, χαρίζεσθαι, καὶ, τοῖσδε, ,, ἄλλως, τε, καὶ, σοῦ, λέξαντος, ὡς, εἶπες, ,, ἄξενόν, τι, καταφαίνεταί, μοι, καὶ, ἄγριον, ., ἐπεὶ, Θεαίτητόν, γε, τὸν, προσδιαλεγόμενον, εἶναι, δέχομαι, παντάπασιν, ἐξ, ὧν, αὐτός, τε, πρότερον, διείλεγμαι, καὶ, σὺ, τὰ, νῦν, μοι, διακελεύῃ, .>\n",
      "Empty form:  > 86 PUNCT TokenList<ΘΕΟ., τὰ, ποῖα, δή, ;, ΣΩ., \"\", #, Σωκράτης, \"\", >, ΣΩ., σοφιστήν, ,, πολιτικόν, ,, φιλόσοφον, ., ΘΕΟ., τί, δὲ, μάλιστα, καὶ, τὸ, ποῖόν, τι, περὶ, αὐτῶν, διαπορηθεὶς, ἐρέσθαι, διενοήθης, ;, ΣΩ., τόδε, ·, πότερον, ἓν, πάντα, ταῦτα, ἐνόμιζον, ἢ, δύο, ,, ἢ, καθάπερ, τὰ, ὀνόματα, τρία, ,, τρία, καὶ, τὰ, γένη, διαιρούμενοι, καθʼ, ἓν, ὄνομα, γένος, ἑκάστῳ, προσῆπτον, ;, ΘΕΟ., ἀλλʼ, οὐδείς, ,, ὡς, ἐγᾦμαι, ,, φθόνος, αὐτῷ, διελθεῖν, αὐτά, ·, ἢ, πῶς, ,, ὦ, ξένε, ,, λέγωμεν, ;, ΞΕ., #, Ξένος, \"\", , ΞΕ., οὕτως, ,, ὦ, Θεόδωρε, ., φθόνος, μὲν, γὰρ, οὐδεὶς, οὐδὲ, χαλεπὸν, εἰπεῖν, ὅτι, γε, τρίʼ, ἡγοῦντο, ·, καθʼ, ἕκαστον, μὴν, διορίσασθαι, σαφῶς, τί, ποτʼ, ἔστιν, ,, οὐ, σμικρὸν, οὐδὲ, ῥᾴδιον, ἔργον, ., ΘΕΟ., καὶ, μὲν, δὴ, κατὰ, τύχην, γε, ,, ὦ, Σώκρατες, ,, λόγων, ἐπελάβου, παραπλησίων, ὧν, καὶ, πρὶν, ἡμᾶς, δεῦρʼ, ἐλθεῖν, διερωτῶντες, αὐτὸν, ἐτυγχάνομεν, ,, ὁ, δὲ, ταὐτὰ, ἅπερ, πρὸς, σὲ, νῦν, καὶ, τότε, ἐσκήπτετο, πρὸς, ἡμᾶς, ·, ἐπεὶ, διακηκοέναι, γέ, φησιν, ἱκανῶς, καὶ, οὐκ, ἀμνημονεῖν, ., ΣΩ., μὴ, τοίνυν, ,, ὦ, ξένε, ,, ἡμῶν, τήν, γε, πρώτην, αἰτησάντων, χάριν, ἀπαρνηθεὶς, γένῃ, ,, τοσόνδε, δʼ, ἡμῖν, φράζε, ., πότερον, εἴωθας, ἥδιον, αὐτὸς, ἐπὶ, σαυτοῦ, μακρῷ, λόγῳ, διεξιέναι, λέγων, τοῦτο, ὃ, ἂν, ἐνδείξασθαί, τῳ, βουληθῇς, ,, ἢ, διʼ, ἐρωτήσεων, ,, οἷόν, ποτε, καὶ, Παρμενίδῃ, χρωμένῳ, καὶ, διεξιόντι, λόγους, παγκάλους, παρεγενόμην, ἐγὼ, νέος, ὤν, ,, ἐκείνου, μάλα, δὴ, τότε, ὄντος, πρεσβύτου, ;, ΞΕ., τῷ, μέν, ,, ὦ, Σώκρατες, ,, ἀλύπως, τε, καὶ, εὐηνίως, προσδιαλεγομένῳ, ῥᾷον, οὕτω, ,, τὸ, πρὸς, ἄλλον, ·, εἰ, δὲ, μή, ,, τὸ, καθʼ, αὑτόν, ., ΣΩ., ἔξεστι, τοίνυν, τῶν, παρόντων, ὃν, ἂν, βουληθῇς, ἐκλέξασθαι, ,, πάντες, γὰρ, ὑπακούσονταί, σοι, πρᾴως, ·, συμβούλῳ, μὴν, ἐμοὶ, χρώμενος, τῶν, νέων, τινὰ, αἱρήσῃ, ,, Θεαίτητον, τόνδε, ,, ἢ, καὶ, τῶν, ἄλλων, εἴ, τίς, σοι, κατὰ, νοῦν, ., ΞΕ., ὦ, Σώκρατες, ,, αἰδώς, τίς, μʼ, ἔχει, τὸ, νῦν, πρῶτον, συγγενόμενον, ὑμῖν, μὴ, κατὰ, σμικρὸν, ἔπος, πρὸς, ἔπος, ποιεῖσθαι, τὴν, συνουσίαν, ,, ἀλλʼ, ἐκτείναντα, ἀπομηκύνειν, λόγον, συχνὸν, κατʼ, ἐμαυτόν, ,, εἴτε, καὶ, πρὸς, ἕτερον, ,, οἷον, ἐπίδειξιν, ποιούμενον, ·, τῷ, γὰρ, ὄντι, τὸ, νῦν, ῥηθὲν, οὐχ, ὅσον, ὧδε, ἐρωτηθὲν, ἐλπίσειεν, ἂν, αὐτὸ, εἶναί, τις, ,, ἀλλὰ, τυγχάνει, λόγου, παμμήκους, ὄν, ., τὸ, δὲ, αὖ, σοὶ, μὴ, χαρίζεσθαι, καὶ, τοῖσδε, ,, ἄλλως, τε, καὶ, σοῦ, λέξαντος, ὡς, εἶπες, ,, ἄξενόν, τι, καταφαίνεταί, μοι, καὶ, ἄγριον, ., ἐπεὶ, Θεαίτητόν, γε, τὸν, προσδιαλεγόμενον, εἶναι, δέχομαι, παντάπασιν, ἐξ, ὧν, αὐτός, τε, πρότερον, διείλεγμαι, καὶ, σὺ, τὰ, νῦν, μοι, διακελεύῃ, .>\n",
      "Empty form:  > 130 PUNCT TokenList<ΞΕ., ἔχει, τοίνυν, καὶ, μάλα, συχνήν, ., ὁ, μὲν, γὰρ, εὐήθης, αὐτῶν, ἐστιν, ,, οἰόμενος, εἰδέναι, ταῦτα, ἃ, δοξάζει, ·, τὸ, δὲ, θατέρου, σχῆμα, διὰ, τὴν, ἐν, τοῖς, λόγοις, κυλίνδησιν, ἔχει, πολλὴν, ὑποψίαν, καὶ, φόβον, ὡς, ἀγνοεῖ, ταῦτα, ἃ, πρὸς, τοὺς, ἄλλους, ὡς, εἰδὼς, ἐσχημάτισται, ., ΘΕΑΙ., πάνυ, μὲν, οὖν, ἔστιν, ἑκατέρου, γένος, ὧν, εἴρηκας, ., ΞΕ., οὐκοῦν, τὸν, μὲν, ἁπλοῦν, μιμητήν, τινα, ,, τὸν, δὲ, εἰρωνικὸν, μιμητὴν, θήσομεν, ;, ΘΕΑΙ., εἰκὸς, γοῦν, ., ΞΕ., τούτου, δʼ, αὖ, τὸ, γένος, ἓν, ἢ, δύο, φῶμεν, ;, ΘΕΑΙ., ὅρα, σύ, ., ΞΕ., σκοπῶ, ,, καί, μοι, διττὼ, καταφαίνεσθόν, τινε, ·, τὸν, μὲν, δημοσίᾳ, τε, καὶ, μακροῖς, λόγοις, πρὸς, πλήθη, δυνατὸν, εἰρωνεύεσθαι, καθορῶ, ,, τὸν, δὲ, ἰδίᾳ, τε, καὶ, βραχέσι, λόγοις, ἀναγκάζοντα, τὸν, προσδιαλεγόμενον, ἐναντιολογεῖν, αὐτὸν, αὑτῷ, ., ΘΕΑΙ., #, Θεαίτητος, \"\", , ΘΕΑΙ., λέγεις, ὀρθότατα, ., ΞΕ., τίνα, οὖν, ἀποφαινώμεθα, τὸν, μακρολογώτερον, εἶναι, ;, πότερα, πολιτικὸν, ἢ, δημολογικόν, ;, ΘΕΑΙ., δημολογικόν, ., ΞΕ., τί, δὲ, τὸν, ἕτερον, ἐροῦμεν, ;, σοφὸν, ἢ, σοφιστικόν, ;, ΘΕΑΙ., τὸ, μέν, που, σοφὸν, ἀδύνατον, ,, ἐπείπερ, οὐκ, εἰδότα, αὐτὸν, ἔθεμεν, ·, μιμητὴς, δʼ, ὢν, τοῦ, σοφοῦ, δῆλον, ὅτι, παρωνύμιον, αὐτοῦ, τι, λήψεται, ,, καὶ, σχεδὸν, ἤδη, μεμάθηκα, ὅτι, τοῦτον, δεῖ, προσειπεῖν, ἀληθῶς, αὐτὸν, ἐκεῖνον, τὸν, παντάπασιν, ὄντως, σοφιστήν, ., ΞΕ., οὐκοῦν, συνδήσομεν, αὐτοῦ, ,, καθάπερ, ἔμπροσθεν, ,, τοὔνομα, συμπλέξαντες, ἀπὸ, τελευτῆς, ἐπʼ, ἀρχήν, ;, ΘΕΑΙ., πάνυ, μὲν, οὖν, ., ΞΕ., τὸ, δὴ, τῆς, ἐναντιοποιολογικῆς, εἰρωνικοῦ, μέρους, τῆς, δοξαστικῆς, μιμητικόν, ,, τοῦ, φανταστικοῦ, γένους, ἀπὸ, τῆς, εἰδωλοποιικῆς, οὐ, θεῖον, ἀλλʼ, ἀνθρωπικὸν, τῆς, ποιήσεως, ἀφωρισμένον, ἐν, λόγοις, τὸ, θαυματοποιικὸν, μόριον, ,, ταύτης, τῆς, γενεᾶς, τε, καὶ, αἵματος, ὃς, ἂν, φῇ, τὸν, ὄντως, σοφιστὴν, εἶναι, ,, τἀληθέστατα, ,, ὡς, ἔοικεν, ,, ἐρεῖ, ., ΘΕΑΙ., παντάπασι, μὲν, οὖν, .>\n",
      "herodotus_dev.conllu\n",
      "plato_ii_dev_spaced.conllu\n",
      "aeschylus_ii_dev.conllu\n",
      "euripides_i_dev.conllu\n",
      "xenophon_i_dev.conllu\n"
     ]
    }
   ],
   "source": [
    "# Normalization to NFKC\n",
    "import unicodedata\n",
    "def normalize(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "# # load files from folder\n",
    "for file in os.listdir(\"../assets/Lemmatization_training_files\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/Lemmatization_training_files/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/Lemmatization_training_files/lemma_train_NFKC/\" + \"{0}_NFKC.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    if token[\"form\"] == '':\n",
    "                        print ('Empty form:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"form\"] = token[\"lemma\"]\n",
    "                    #print(clean_text(sentence))\n",
    "                    if token[\"lemma\"] == '':\n",
    "                        print ('Empty lemma:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"lemma\"] = token[\"form\"]\n",
    "                    if token[\"id\"] == '':\n",
    "                        print ('Empty id:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"id\"] = 'UNK'\n",
    "                    if token[\"upos\"] == '':\n",
    "                        print ('Empty upos:', token[\"form\"], token[\"lemma\"], token[\"id\"], token[\"upos\"], sentence)\n",
    "                        token[\"upos\"] = 'UNK'\n",
    "                    token[\"form\"] = normalize(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (49 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/aeschylus_i_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (185 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/aeschylus_i_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (33 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/aeschylus_ii_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (136 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/aeschylus_ii_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (31 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/euripides_i_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (115 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/euripides_i_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/galen_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (9 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/galen_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (80 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/herodotus_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (355 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/herodotus_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/plato_ii_dev_spaced_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (22 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/plato_ii_train_spaced_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (28 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/plato_iii_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (109 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/plato_iii_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (44 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/xenophon_i_dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (196 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/xenophon_i_train_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (31 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/xenophon_ii_dev_spaced_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (118 documents):\n",
      "../assets/Lemmatization_training_files/lemma_train_NFKC/xenophon_ii_train_NFKC.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# convert conllu to spacy Lemmatization_training_files with NFKC normalization\n",
    "!python -m spacy convert ../assets/Lemmatization_training_files/lemma_train_NFKC/ ../assets/Lemmatization_training_files/lemma_train_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grc_perseus-ud-train.conllu\n",
      "grc_perseus-ud-test.conllu\n",
      "grc_perseus-ud-dev.conllu\n",
      "grc_perseus-ud-train.conllu\n",
      "grc_perseus-ud-test.conllu\n",
      "grc_perseus-ud-dev.conllu\n"
     ]
    }
   ],
   "source": [
    "#Normalize and convert Perseus dataset\n",
    "\n",
    "import unicodedata\n",
    "def normalize_NFKD(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-Perseus\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-Perseus/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/\" + \"{0}_NFKD.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKD(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKD(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        \n",
    "import unicodedata\n",
    "def normalize_NFKC(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-Perseus\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-Perseus/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/\" + \"{0}_NFKC.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKC(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKC(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grc_proiel-ud-train.conllu\n",
      "grc_proiel-ud-test.conllu\n",
      "grc_proiel-ud-dev.conllu\n",
      "grc_proiel-ud-train.conllu\n",
      "grc_proiel-ud-test.conllu\n",
      "grc_proiel-ud-dev.conllu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Normalize and convert PROEIL dataset\n",
    "\n",
    "import unicodedata\n",
    "def normalize_NFKD(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "# Remove accents from the PROIEL files\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-PROIEL\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-PROIEL/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/\" + \"{0}_NFKD.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKD(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKD(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        \n",
    "\n",
    "\n",
    "import unicodedata\n",
    "def normalize_NFKC(text):\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "# Remove accents from the PROIEL files\n",
    "# load files from folder\n",
    "for file in os.listdir(\"../assets/UD_Ancient_Greek-PROIEL\"):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        print(file)\n",
    "        data = open(\"../assets/UD_Ancient_Greek-PROIEL/\" + file, \"r\", encoding=\"utf-8\")\n",
    "        annotations = data.read()\n",
    "        sentences = conllu.parse(annotations)\n",
    "        with open(\"../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/\" + \"{0}_NKFC.conllu\".format(file[:-7]), \"w\", encoding=\"utf-8\") as data:\n",
    "            for sentence in sentences:\n",
    "                for token in sentence:\n",
    "                    token[\"form\"] = normalize_NFKC(token[\"form\"])\n",
    "                    token[\"lemma\"] = normalize_NFKC(token[\"lemma\"])\n",
    "                data.write(sentence.serialize())\n",
    "        data.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Universal dependencies tools validate.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (114 documents):\n",
      "../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/grc_perseus-ud-dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (131 documents):\n",
      "../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/grc_perseus-ud-test_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1148 documents):\n",
      "../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/grc_perseus-ud-train_NFKD.spacy\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (114 documents):\n",
      "../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/grc_perseus-ud-dev_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (131 documents):\n",
      "../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/grc_perseus-ud-test_NFKC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1148 documents):\n",
      "../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/grc_perseus-ud-train_NFKC.spacy\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (102 documents):\n",
      "../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/grc_proiel-ud-dev_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (105 documents):\n",
      "../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/grc_proiel-ud-test_NFKD.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1502 documents):\n",
      "../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/grc_proiel-ud-train_NFKD.spacy\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (102 documents):\n",
      "../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/grc_proiel-ud-dev_NKFC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (105 documents):\n",
      "../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/grc_proiel-ud-test_NKFC.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1502 documents):\n",
      "../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/grc_proiel-ud-train_NKFC.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# convert conllu to spacy UD_Ancient_Greek and UD_Ancient_Greek-PROIEL\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ ../assets/UD_Ancient_Greek-Perseus/UD_Ancient_Greek-Perseus_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKD/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "!python -m spacy convert ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ ../assets/UD_Ancient_Greek-PROIEL/UD_Ancient_Greek-PROIEL_NFKC/ -c conllu -m --n-sents 10 --merge-subtokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(\"̓\")\n",
    "# remove whitespace\n",
    "testr = str(\" ̓ \").strip()\n",
    "print (testr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"/root/Projects/Atlomy/git/greCy_ATLOMY/training/Lemmatize_transformer_morefiles/lemmatizer/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy files\n",
    "from spacy.tokens import DocBin, Doc\n",
    "\n",
    "test_data= DocBin().from_disk(\"../corpus/train/lemma_train/aeschylus_train.spacy\")\n",
    "test_docbin_docs = list(test_data.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docbin_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy files\n",
    "from spacy.tokens import DocBin, Doc\n",
    "\n",
    "test_data= DocBin().from_disk(\"../assets/Lemmatization_training_files/lemma_train_clean/aeschylus_i_train.spacy\")\n",
    "test_docbin_docs = list(test_data.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docbin_docs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
